@article{wood2018design,
  title={Design exposition with literate visualization},
  author={Wood, Jo and Kachkaev, Alexander and Dykes, Jason},
  journal={IEEE transactions on visualization and computer graphics},
  volume={25},
  number={1},
  pages={759--768},
  year={2018},
  publisher={IEEE}
}

@inproceedings{dunne2012graphtrail,
  title={GraphTrail: Analyzing large multivariate, heterogeneous networks while supporting exploration history},
  author={Dunne, Cody and Henry Riche, Nathalie and Lee, Bongshin and Metoyer, Ronald and Robertson, George},
  booktitle={Proceedings of the SIGCHI conference on human factors in computing systems},
  pages={1663--1672},
  year={2012}
}

@article{siddiqui2016effortless,
  title={Effortless data exploration with zenvisage: an expressive and interactive visual analytics system},
  author={Siddiqui, Tarique and Kim, Albert and Lee, John and Karahalios, Karrie and Parameswaran, Aditya},
  journal={arXiv preprint arXiv:1604.03583},
  year={2016}
}

@inproceedings{wattenberg2001sketching,
  title={Sketching a graph to query a time-series database},
  author={Wattenberg, Martin},
  booktitle={CHI'01 Extended Abstracts on Human factors in Computing Systems},
  pages={381--382},
  year={2001}
}

@inproceedings {ottley2019curious,
booktitle = {EuroVis 2019 - Short Papers},
editor = {Johansson, Jimmy and Sadlo, Filip and Marai, G. Elisabeta},
title = {{The Curious Case of Combining Text and Visualization}},
author = {Ottley, Alvitta and Kaszowska, Aleksandra and Crouser, R. Jordan and Peck, Evan M.},
year = {2019},
publisher = {The Eurographics Association},
ISBN = {978-3-03868-090-1},
DOI = {10.2312/evs.20191181}
}

@article{anderson2011user,
  title={A user study of visualization effectiveness using EEG and cognitive load},
  author={Anderson, Erik W and Potter, Kristin C and Matzen, Laura E and Shepherd, Jason F and Preston, Gilbert A and Silva, Cl{\'a}udio T},
  journal={Computer graphics forum},
  volume={30},
  number={3},
  pages={791--800},
  year={2011},
  organization={Wiley Online Library}
}

@inproceedings{peck2013using,
  title={Using fNIRS brain sensing to evaluate information visualization interfaces},
  author={Peck, Evan M M and Yuksel, Beste F and Ottley, Alvitta and Jacob, Robert JK and Chang, Remco},
  booktitle={Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  pages={473--482},
  year={2013}
}

@article{heer2008graphical,
  title={Graphical histories for visualization: Supporting analysis, communication, and evaluation},
  author={Heer, Jeffrey and Mackinlay, Jock and Stolte, Chris and Agrawala, Maneesh},
  journal={IEEE transactions on visualization and computer graphics},
  volume={14},
  number={6},
  pages={1189--1196},
  year={2008},
  publisher={IEEE}
}

@inproceedings{shrinivasan2008supporting,
  title={Supporting the analytical reasoning process in information visualization},
  author={Shrinivasan, Yedendra Babu and van Wijk, Jarke J},
  booktitle={Proceedings of the SIGCHI conference on human factors in computing systems},
  pages={1237--1246},
  year={2008}
}

@inproceedings{brown2018modelspace,
  title={Modelspace: Visualizing the trails of data models in visual analytics systems},
  author={Brown, Eli T and Yarlagadda, Sriram and Cook, Kristin A and Chang, Remco and Endert, Alex},
  booktitle={IEEE Visualization Workshop on Machine Learning from User Interactions for Visualization and Analytics},
  year={2019}
}

@inproceedings{endert2011observation,
  title={Observation-level interaction with statistical models for visual analytics},
  author={Endert, Alex and Han, Chao and Maiti, Dipayan and House, Leanna and North, Chris},
  booktitle={2011 IEEE conference on visual analytics science and technology (VAST)},
  pages={121--130},
  year={2011},
  organization={IEEE}
}

@misc{ancona2012sensemaking,
  title={SenseMaking: Framing and Acting in the Unknown in The Handbook of Teaching Leadership: Knowing, Doing and Being. Scott, S., Nohria, N., Khurana, R},
  author={Ancona, B},
  year={2012},
  publisher={Sage Publishing, California}
}

@article{satyanarayan2015reactive,
  title={Reactive vega: A streaming dataflow architecture for declarative interactive visualization},
  author={Satyanarayan, Arvind and Russell, Ryan and Hoffswell, Jane and Heer, Jeffrey},
  journal={IEEE transactions on visualization and computer graphics},
  volume={22},
  number={1},
  pages={659--668},
  year={2015},
  publisher={IEEE}
}

@article{satyanarayan2014lyra,
  title={Lyra: An interactive visualization design environment},
  author={Satyanarayan, Arvind and Heer, Jeffrey},
  journal={Computer Graphics Forum},
  volume={33},
  number={3},
  pages={351--360},
  year={2014},
  organization={Wiley Online Library}
}

@article{satyanarayan2016vega,
  title={Vega-lite: A grammar of interactive graphics},
  author={Satyanarayan, Arvind and Moritz, Dominik and Wongsuphasawat, Kanit and Heer, Jeffrey},
  journal={IEEE transactions on visualization and computer graphics},
  volume={23},
  number={1},
  pages={341--350},
  year={2016},
  publisher={IEEE}
}

@inproceedings{setlur2016eviza,
  title={Eviza: A natural language interface for visual analysis},
  author={Setlur, Vidya and Battersby, Sarah E and Tory, Melanie and Gossweiler, Rich and Chang, Angel X},
  booktitle={Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
  pages={365--377},
  year={2016}
}

@inproceedings{garg2008model,
  title={Model-driven visual analytics},
  author={Garg, Supriya and Nam, Julia Eunju and Ramakrishnan, IV and Mueller, Klaus},
  booktitle={2008 IEEE Symposium on Visual Analytics Science and Technology},
  pages={19--26},
  year={2008},
  organization={IEEE}
}

@inproceedings{xiao2006enhancing,
  title={Enhancing visual analysis of network traffic using a knowledge representation},
  author={Xiao, Ling and Gerth, John and Hanrahan, Pat},
  booktitle={2006 IEEE Symposium On Visual Analytics Science And Technology},
  pages={107--114},
  year={2006},
  organization={IEEE}
}

@inproceedings{williamson1992dynamic,
  title={The Dynamic HomeFinder: Evaluating dynamic queries in a real-estate information exploration system},
  author={Williamson, Christopher and Shneiderman, Ben},
  booktitle={Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={338--346},
  year={1992}
}

@article{gulwani2012spreadsheet,
  title={Spreadsheet data manipulation using examples},
  author={Gulwani, Sumit and Harris, William R and Singh, Rishabh},
  journal={Communications of the ACM},
  volume={55},
  number={8},
  pages={97--105},
  year={2012},
  publisher={ACM New York, NY, USA}
}

@article{gulwani2011automating,
  title={Automating string processing in spreadsheets using input-output examples},
  author={Gulwani, Sumit},
  journal={ACM Sigplan Notices},
  volume={46},
  number={1},
  pages={317--330},
  year={2011},
  publisher={ACM New York, NY, USA}
}

@inproceedings{pirolli2005sensemaking,
  title={The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis},
  author={Pirolli, Peter and Card, Stuart},
  booktitle={Proceedings of international conference on intelligence analysis},
  volume={5},
  pages={2--4},
  year={2005},
  organization={McLean, VA, USA}
}

@article{yi2007toward,
    author={J. S. {Yi} and Y. a. {Kang} and J. {Stasko}},
    journal={IEEE Transactions on Visualization and Computer Graphics},
    title={Toward a Deeper Understanding of the Role of Interaction in Information Visualization},
    year={2007},
    volume={13},
    number={6},
    pages={1224-1231},
    keywords={data visualisation;human computer interaction;information visualization;Infovis community;Infovis interaction techniques;Infovis systems;taxonomy;Visual analytics;Taxonomy;Data visualization;Research and development;Filters;Computer graphics;Rendering (computer graphics);Computer displays;Human computer interaction;Conference proceedings;Information visualization;interaction;interaction techniques;taxonomy;visual analytics},
    doi={10.1109/TVCG.2007.70515},
    ISSN={2160-9306},
    month={Nov},
}


@inproceedings{muller_how_2019,
	address = {Glasgow, Scotland Uk},
	title = {How {Data} {Science} {Workers} {Work} with {Data}: {Discovery}, {Capture}, {Curation}, {Design}, {Creation}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {How {Data} {Science} {Workers} {Work} with {Data}},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300356},
	doi = {10.1145/3290605.3300356},
	abstract = {With the rise of big data, there has been an increasing need for practitioners in this space and an increasing opportunity for researchers to understand their workflows and design new tools to improve it. Data science is often described as data-driven, comprising unambiguous data and proceeding through regularized steps of analysis. However, this view focuses more on abstract processes, pipelines, and workflows, and less on how data science workers engage with the data. In this paper, we build on the work of other CSCW and HCI researchers in describing the ways that scientists, scholars, engineers, and others work with their data, through analyses of interviews with 21 data science professionals. We set five approaches to data along a dimension of interventions: Data as given; as captured; as curated; as designed; and as created. Data science workers develop an intuitive sense of their data and processes, and actively shape their data. We propose new ways to apply these interventions analytically, to make sense of the complex activities around data practices.},
	language = {en},
	urldate = {2019-11-26},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems} - {CHI} '19},
	publisher = {ACM Press},
	author = {Muller, Michael and Lange, Ingrid and Wang, Dakuo and Piorkowski, David and Tsay, Jason and Liao, Q. Vera and Dugan, Casey and Erickson, Thomas},
	year = {2019},
	pages = {1--15},
	file = {Muller et al. - 2019 - How Data Science Workers Work with Data Discovery.pdf:C\:\\Users\\conny\\Zotero\\storage\\5CX3EH3K\\Muller et al. - 2019 - How Data Science Workers Work with Data Discovery.pdf:application/pdf}
}

@inproceedings{lee_dynamic_2019,
	address = {Glasgow, Scotland Uk},
	title = {Dynamic {Network} {Plaid}: {A} {Tool} for the {Analysis} of {Dynamic} {Networks}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {Dynamic {Network} {Plaid}},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300360},
	doi = {10.1145/3290605.3300360},
	abstract = {Network data that changes over time can be very useful for studying a wide range of important phenomena, from how social network connections change to epidemiology. However, it is challenging to analyze, especially if it has many actors, connections or if the covered timespan is large with rapidly changing links (e.g., months of changes with changes at second resolution). In these analyses one would often like to compare many periods of time to others, without having to look at the full timeline. To support this kind of analysis we designed and implemented a technique and system to visualize this dynamic data. The Dynamic Network Plaid (DNP) is designed for large displays and based on user-generated interactive timeslicing on the dynamic graph attributes and on linked provenance-preserving representations. We present the technique, interface and the design/evaluation with a group of public health researchers investigating non-suicidal self-harm picture sharing in Instagram.},
	language = {en},
	urldate = {2019-11-26},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Lee, Alexandra and Archambault, Daniel and Nacenta, Miguel},
	year = {2019},
	pages = {1--14},
	file = {Lee et al. - 2019 - Dynamic Network Plaid A Tool for the Analysis of .pdf:C\:\\Users\\conny\\Zotero\\storage\\FQM7BLBD\\Lee et al. - 2019 - Dynamic Network Plaid A Tool for the Analysis of .pdf:application/pdf}
}

@inproceedings{koesten_collaborative_2019,
	address = {Glasgow, Scotland Uk},
	title = {Collaborative {Practices} with {Structured} {Data}: {Do} {Tools} {Support} {What} {Users} {Need}?},
	isbn = {978-1-4503-5970-2},
	shorttitle = {Collaborative {Practices} with {Structured} {Data}},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300330},
	doi = {10.1145/3290605.3300330},
	abstract = {Collaborative work with data is increasingly common and spans a broad range of activities - from creating or analysing data in a team, to sharing it with others, to reusing someone else’s data in a new context. In this paper, we explore collaboration practices around structured data and how they are supported by current technology. We present the results of an interview study with twenty data practitioners, from which we derive four high-level user needs for tool support. We compare them against the capabilities of twenty systems that are commonly associated with data activities, including data publishing software, wikis, web-based collaboration tools, and online community platforms. Our findings suggest that data-centric collaborative work would benefit from: structured documentation of data and its lifecycle; advanced affordances for conversations among collaborators; better change control; and custom data access. The findings help us formalise practices around data teamwork, and build a better understanding how people’s motivations and barriers when working with structured data.},
	language = {en},
	urldate = {2019-11-26},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Koesten, Laura and Kacprzak, Emilia and Tennison, Jeni and Simperl, Elena},
	year = {2019},
	pages = {1--14},
	file = {Koesten et al. - 2019 - Collaborative Practices with Structured Data Do T.pdf:C\:\\Users\\conny\\Zotero\\storage\\W3IBQHPC\\Koesten et al. - 2019 - Collaborative Practices with Structured Data Do T.pdf:application/pdf}
}

@inproceedings{boukhelifa_exploratory_2019,
	address = {Glasgow, Scotland Uk},
	title = {An {Exploratory} {Study} on {Visual} {Exploration} of {Model} {Simulations} by {Multiple} {Types} of {Experts}},
	isbn = {978-1-4503-5970-2},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300874},
	doi = {10.1145/3290605.3300874},
	language = {en},
	urldate = {2019-11-26},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Boukhelifa, Nadia and Bezerianos, Anastasia and Trelea, Ioan Cristian and Perrot, Nathalie Méjean and Lutton, Evelyne},
	year = {2019},
	keywords = {ENCODING - Model, HOW - Classification, Type of Work: Empirical Study, WHEN - Retrospective Analysis, HOW - Classification, WHY - User Behaviour, User Characteristics, User Modelling, Type of Work: Empirical Study, ENCODING - Model},
	pages = {1--14},
	file = {Boukhelifa et al. - 2019 - An Exploratory Study on Visual Exploration of Mode.pdf:C\:\\Users\\conny\\Zotero\\storage\\BE3FTSM3\\Boukhelifa et al. - 2019 - An Exploratory Study on Visual Exploration of Mode.pdf:application/pdf}
}

@inproceedings{correll_ethical_2019,
	address = {Glasgow, Scotland Uk},
	title = {Ethical {Dimensions} of {Visualization} {Research}},
	isbn = {978-1-4503-5970-2},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300418},
	doi = {10.1145/3290605.3300418},
	abstract = {Visualizations have a potentially enormous influence on how data are used to make decisions across all areas of human endeavor. However, it is not clear how this power connects to ethical duties: what obligations do we have when it comes to visualizations and visual analytics systems, beyond our duties as scientists and engineers? Drawing on historical and contemporary examples, I address the moral components of the design and use of visualizations, identify some ongoing areas of visualization research with ethical dilemmas, and propose a set of additional moral obligations that we have as designers, builders, and researchers of visualizations.},
	language = {en},
	urldate = {2019-11-26},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Correll, Michael},
	year = {2019},
	pages = {1--13},
	file = {Correll - 2019 - Ethical Dimensions of Visualization Research.pdf:C\:\\Users\\conny\\Zotero\\storage\\MGHFPR8J\\Correll - 2019 - Ethical Dimensions of Visualization Research.pdf:application/pdf}
}

@article{camisetty_enhancing_2018,
	title = {Enhancing {Web}-based {Analytics} {Applications} through {Provenance}},
	url = {https://ieeexplore.ieee.org/document/8500765},
	doi = {10.1109/TVCG.2018.2865039},
	abstract = {Visual analytics systems continue to integrate new technologies and leverage modern environments for exploration and collaboration, making tools and techniques available to a wide audience through web browsers. Many of these systems have been developed with rich interactions, offering users the opportunity to examine details and explore hypotheses that have not been directly encoded by a designer. Understanding is enhanced when users can replay and revisit the steps in the sensemaking process, and in collaborative settings, it is especially important to be able to review not only the current state but also what decisions were made along the way. Unfortunately, many web-based systems lack the ability to capture such reasoning, and the path to a result is transient, forgotten when a user moves to a new view. This paper explores the requirements to augment existing client-side web applications with support for capturing, reviewing, sharing, and reusing steps in the reasoning process. Furthermore, it considers situations where decisions are made with streaming data, and the insights gained from revisiting those choices when more data is available. It presents a proof of concept, the Shareable Interactive Manipulation Provenance framework (SIMProv.js), that addresses these requirements in a modern, client-side JavaScript library, and describes how it can be integrated with existing frameworks.},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Camisetty, Akhilesh and Chandurkar, Chaitanya and Sun, Maoyuan and Koop, David},
	month = oct,
	year = {2018},
	keywords = {Type of Work: Tool/Software, HOW: javascript library, Maybe related. Javascript library for capturing (storing, and using) provenance for web-based applications, WHY: visualize history, reuse, sharing}
}

@inproceedings{hu_viznet:_2019,
	address = {Glasgow, Scotland Uk},
	title = {{VizNet}: {Towards} {A} {Large}-{Scale} {Visualization} {Learning} and {Benchmarking} {Repository}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {{VizNet}},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300892},
	doi = {10.1145/3290605.3300892},
	language = {en},
	urldate = {2019-11-26},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Hu, Kevin and Demiralp, Çağatay and Gaikwad, Snehalkumar 'Neil' S. and Hulsebos, Madelon and Bakker, Michiel A. and Zgraggen, Emanuel and Hidalgo, César and Kraska, Tim and Li, Guoliang and Satyanarayan, Arvind},
	year = {2019},
	keywords = {WHEN - Retrospective Analysis, WHY - Adaptive Systems / GuidancENCODING - Image, HOW - Probabilistic Models / Prediction, Type of Work: Technique \& Algorithm, ENCODING - ImagWHEN - Retrospective Analysis, WHY - Adaptive Systems / Guidance},
	pages = {1--12},
	file = {Hu et al. - 2019 - VizNet Towards A Large-Scale Visualization Learni.pdf:C\:\\Users\\conny\\Zotero\\storage\\78L95B5B\\Hu et al. - 2019 - VizNet Towards A Large-Scale Visualization Learni.pdf:application/pdf}
}

@inproceedings{cho_anchoring_2017,
	title = {The {Anchoring} {Effect} in {Decision}-{Making} with {Visual} {Analytics}},
	url = {https://ieeexplore.ieee.org/document/8585665},
	doi = {10.1109/VAST.2017.8585665},
	abstract = {Anchoring effect is the tendency to focus too heavily on one piece of information when making decisions. In this paper, we present a novel, systematic study and resulting analyses that investigate the effects of anchoring effect on human decision-making using visual analytic systems. Visual analytics interfaces typically contain multiple views that present various aspects of information such as spatial, temporal, and categorical. These views are designed to present complex, heterogeneous data in accessible forms that aid decision-making. However, human decision-making is often hindered by the use of heuristics, or cognitive biases, such as anchoring effect. Anchoring effect can be triggered by the order in which information is presented or the magnitude of information presented. Through carefully designed laboratory experiments, we present evidence of anchoring effect in analysis with visual analytics interfaces when users are primed by representation of different pieces of information. We also describe detailed analyses of users' interaction logs which reveal the impact of anchoring bias on the visual representation preferred and paths of analysis. We discuss implications for future research to possibly detect and alleviate anchoring bias.},
	author = {Cho, Isaac and Wesslen, Ryan and Karduni, Alireza and Santhanam, Sashank and Shaikh, Samira and Dou, Wenwen},
	month = oct,
	year = {2017},
	keywords = {Type of Work: User Study, WHEN - Real-Time Applications, HOW - Pattern Analysis, HOW - Classification Models, HOW: classification (lots of different techniques), WHY: use user interactions to detect occurance of anchoring bias, WHY - User Behavior / User Characteristics / User Modelling, ENCODING - Sequence},
	booktitle = {Proceedings of IEEE Conference on Visual Analytics Science and Technology}
}

@article{dabek_grammar-based_2016,
	title = {A {Grammar}-based {Approach} for {Modeling} {User} {Interactions} and {Generating} {Suggestions} {During} the {Data} {Exploration} {Process}},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/27514057},
	doi = {10.1109/TVCG.2016.2598471},
	abstract = {Despite the recent popularity of visual analytics focusing on big data, little is known about how to support users that use visualization techniques to explore multi-dimensional datasets and accomplish specific tasks. Our lack of models that can assist end-users during the data exploration process has made it challenging to learn from the user's interactive and analytical process. The ability to model how a user interacts with a specific visualization technique and what difficulties they face are paramount in supporting individuals with discovering new patterns within their complex datasets. This paper introduces the notion of visualization systems understanding and modeling user interactions with the intent of guiding a user through a task thereby enhancing visual data exploration. The challenges faced and the necessary future steps to take are discussed; and to provide a working example, a grammar-based model is presented that can learn from user interactions, determine the common patterns among a number of subjects using a K-Reversible algorithm, build a set of rules, and apply those rules in the form of suggestions to new users with the goal of guiding them along their visual analytic process. A formal evaluation study with 300 subjects was performed showing that our grammar-based model is effective at capturing the interactive process followed by users and that further research in this area has the potential to positively impact how users interact with a visualization system.},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Dabek, Filip and Caban, Jesus J},
	month = jan,
	year = {2016},
	keywords = {WHEN - Real-Time Applications, HOW - Pattern Analysis, WHY - Adaptive Systems / Guidance, Type of Work: Technique \& Algorithm, HOW: grammar based, WHY: modeling user interactions, WHY: recommendation during data exploration, WHY - User Behavior / User Characteristics / User Modelling, ENCODING - Grammar},
	file = {Dabek and Caban - 2016 - A Grammar-based Approach for Modeling User Interac.pdf:C\:\\Users\\conny\\Zotero\\storage\\52H6NAK4\\Dabek and Caban - 2016 - A Grammar-based Approach for Modeling User Interac.pdf:application/pdf}
}

@article{guo_case_2015,
	title = {A {Case} {Study} {Using} {Visualization} {Interaction} {Logs} and {Insight} {Metrics} to {Understand} {How} {Analysts} {Arrive} at {Insights}},
	url = {https://ieeexplore.ieee.org/abstract/document/7192662},
	doi = {10.1109/TVCG.2015.2467613},
	abstract = {We present results from an experiment aimed at using logs of interactions with a visual analytics application to better understand how interactions lead to insight generation. We performed an insight-based user study of a visual analytics application and ran post hoc quantitative analyses of participants' measured insight metrics and interaction logs. The quantitative analyses identified features of interaction that were correlated with insight characteristics, and we confirmed these findings using a qualitative analysis of video captured during the user study. Results of the experiment include design guidelines for the visual analytics application aimed at supporting insight generation. Furthermore, we demonstrated an analysis method using interaction logs that identified which interaction patterns led to insights, going beyond insight-based evaluations that only quantify insight characteristics. We also discuss choices and pitfalls encountered when applying this analysis method, such as the benefits and costs of applying an abstraction framework to application-specific actions before further analysis. Our method can be applied to evaluations of other visualization tools to inform the design of insight-promoting interactions and to better understand analyst behaviors.},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Guo, Hua and Gomez, Steven R. and Ziemkiewicz, Caroline and Laidlaw, David H.},
	month = aug,
	year = {2015},
	keywords = {Type of Work: User Study, WHEN - Real-Time Applications, HOW - Pattern Analysis, Type of Work: Technique, HOW: classification, WHY: understanding analysis process, WHY - User Behavior / User Characteristics / User Modelling, ENCODING - Sequence}
}

@article{ragan_characterizing_2015,
	title = {Characterizing {Provenance} in {Visualization} and {Data} {Analysis}: {An} {Organizational} {Framework} of {Provenance} {Types} and {Purposes}},
	url = {https://ieeexplore.ieee.org/document/7192714},
	doi = {10.1109/TVCG.2015.2467551},
	abstract = {While the primary goal of visual analytics research is to improve the quality of insights and findings, a substantial amount of research in provenance has focused on the history of changes and advances throughout the analysis process. The term, provenance, has been used in a variety of ways to describe different types of records and histories related to visualization. The existing body of provenance research has grown to a point where the consolidation of design knowledge requires cross-referencing a variety of projects and studies spanning multiple domain areas. We present an organizational framework of the different types of provenance information and purposes for why they are desired in the field of visual analytics. Our organization is intended to serve as a framework to help researchers specify types of provenance and coordinate design knowledge across projects. We also discuss the relationships between these factors and the methods used to capture provenance information. In addition, our organization can be used to guide the selection of evaluation methodology and the comparison of study outcomes in provenance research.},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Ragan, Eric D. and Endert, Alex and Sanyal, Jibonananda and Chen, Jian},
	month = aug,
	year = {2015},
	keywords = {WHEN - Real-Time Applications, HOW - Pattern Analysis, Type of Work: Survey, WHY - Adaptive Systems / Guidance, HOW - Classification Models, WHEN - Retrospective Analyses, WHY - Report Generation / Storytelling, WHY - Model Steering / Active Learning, WHY - User Behavior / User Characteristics / User Modelling, ENCODING - Sequence}
}

@article{walker_extensible_2013,
	title = {An {Extensible} {Framework} for {Provenance} in {Human} {Terrain} {Visual} {Analytics}},
	url = {An Extensible Framework for Provenance in Human Terrain Visual Analytics},
	doi = {10.1109/TVCG.2013.132},
	abstract = {We describe and demonstrate an extensible framework that supports data exploration and provenance in the context of Human Terrain Analysis (HTA). Working closely with defence analysts we extract requirements and a list of features that characterise data analysed at the end of the HTA chain. From these, we select an appropriate non-classified data source with analogous features, and model it as a set of facets. We develop ProveML, an XML-based extension of the Open Provenance Model, using these facets and augment it with the structures necessary to record the provenance of data, analytical process and interpretations. Through an iterative process, we develop and refine a prototype system for Human Terrain Visual Analytics (HTVA), and demonstrate means of storing, browsing and recalling analytical provenance and process through analytic bookmarks in ProveML. We show how these bookmarks can be combined to form narratives that link back to the live data. Throughout the process, we demonstrate that through structured workshops, rapid prototyping and structured communication with intelligence analysts we are able to establish requirements, and design schema, techniques and tools that meet the requirements of the intelligence community. We use the needs and reactions of defence analysts in defining and steering the methods to validate the framework.},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Walker, Rick and Slingsby, Aidan and Dykes, Jason and Xu, Kai and Wood, Jo},
	year = {2013},
	keywords = {HOW - Program Synthesis, WHEN - Real-Time Applications, Type of Work: Tool/Software, WHY - Report Generation / Storytelling, HOW: program synthesis (using a markup language), WHY: recall analytical provenance, WHY: storytelling / summarization of provenance, WHY: visual analytics for "human terrain" analysis, ENCODING - Grammar}
}

@article{endert_semantic_2012,
	title = {Semantic {Interaction} for {Sensemaking}: {Inferring} {Analytical} {Reasoning} for {Model} {Steering}},
	url = {https://www.cc.gatech.edu/~aendert3/resources/Endert_TVCG2012_.pdf},
	doi = {10.1109/TVCG.2012.260},
	abstract = {Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users' analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user's reasoning and intuition.},
	journal = {EEE Transactions on Visualization and Computer Graphics},
	author = {Endert, Alex and Fiaux, Patrick and North, Chris},
	month = dec,
	year = {2012},
	keywords = {WHEN - Real-Time Applications, Type of Work: Technique, WHY: model steering, HOW: learning ML model (sematic interaction), HOW - Classification Models, WHY - Model Steering / Active Learning, user reasoning, ENCODING - Signal (temporal?)}
}

@inproceedings{green_alida:_2010,
	title = {{ALIDA}: {Using} machine learning for intent discernment in visual analytics interfaces},
	url = {https://ieeexplore.ieee.org/document/5650854},
	doi = {10.1109/VAST.2010.5650854},
	abstract = {In this paper, we introduce ALIDA, an Active Learning Intent Discerning Agent for visual analytics interfaces. As users interact with and explore data in a visual analytics environment they are each developing their own unique analytic process. The goal of ALIDA is to observe and record the human-computer interactions and utilize these observations as a means of supporting user exploration; ALIDA does this by using interaction to make decision about user interest. As such, ALIDA is designed to track the decision history (interactions) of a user. This history is then utilized to enhance the user's decision-making process by allowing the user to return to previously visited search states, as well as providing suggestions of other search states that may be of interest based on past exploration modalities. The agent passes these suggestions (or decisions) back to an interactive visualization prototype, and these suggestions are used to guide the user, either by suggesting searches or changes to the visualization view. Current work has tested ALIDA under the exploration of homonyms for users wishing to explore word linkages within a dictionary. Ongoing work includes using ALIDA to guide users in transfer function design for volume rendering within scientific gateways.},
	publisher = {IEEE},
	author = {Green, Tera Marie and Maciejewski, Ross and DiPaola, Steve},
	month = oct,
	year = {2010}
}

@inproceedings{steinparz_visualization_2019,
	title = {Visualization of {Rubik}'s {Cube} {Solution} {Algorithms}},
	isbn = {978-3-03868-087-1},
	url = {https://diglib.eg.org:443/xmlui/handle/10.2312/eurova20191119},
	abstract = {Rubik's Cube is among the world's most famous puzzle toys. Despite its relatively simple principle, it requires dedicated, carefully planned algorithms to be solved. In this paper, we present an approach to visualize how different solution algorithms navigate through the high-dimensional space of Rubik's Cube states. We use t-distributed stochastic neighbor embedding (t-SNE) to project feature vector representations of cube states to two dimensions. t-SNE preserves the similarity of cube states and leads to clusters of intermediate states and bundles of cube solution pathways in the projection. Our prototype implementation allows interactive exploration of differences between algorithms, showing detailed state information on demand.},
	language = {en},
	urldate = {2019-11-23},
	publisher = {The Eurographics Association},
	author = {Steinparz, Christian Alexander and Hinterreiter, Andreas and Stitz, Holger and Streit, Marc},
	year = {2019},
	doi = {10.2312/eurova.20191119},
	keywords = {hybrid approaches, pattern analysis, Real-time or post-hoc quantification, WHY - Evaluation of Tools and Systems, HOW - Pattern Analysis, WHY - Report Generation and Storytelling, Type of Work: Tool/Software, WHY - Real-time or post-hoc quantification and re-application, HOW - Clustering},
	file = {Steinparz et al. - 2019 - Visualization of Rubik's Cube Solution Algorithms.pdf:C\:\\Users\\conny\\Zotero\\storage\\TUKRHWU2\\Steinparz et al. - 2019 - Visualization of Rubik's Cube Solution Algorithms.pdf:application/pdf}
}

@article{cavallo_track_2018,
	title = {Track {Xplorer}: {A} {System} for {Visual} {Analysis} of {Sensor}-based {Motor} {Activity} {Predictions}},
	volume = {37},
	copyright = {© 2018 The Author(s) Computer Graphics Forum © 2018 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {Track {Xplorer}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13424},
	doi = {10.1111/cgf.13424},
	abstract = {With the rapid commoditization of wearable sensors, detecting human movements from sensor datasets has become increasingly common over a wide range of applications. To detect activities, data scientists iteratively experiment with different classifiers before deciding which model to deploy. Effective reasoning about and comparison of alternative classifiers are crucial in successful model development. This is, however, inherently difficult in developing classifiers for sensor data, where the intricacy of long temporal sequences, high prediction frequency, and imprecise labeling make standard evaluation methods relatively ineffective and even misleading. We introduce Track Xplorer, an interactive visualization system to query, analyze, and compare the predictions of sensor-data classifiers. Track Xplorer enables users to interactively explore and compare the results of different classifiers, and assess their accuracy with respect to the ground-truth labels and video. Through integration with a version control system, Track Xplorer supports tracking of models and their parameters without additional workload on model developers. Track Xplorer also contributes an extensible algebra over track representations to filter, compose, and compare classification outputs, enabling users to reason effectively about classifier performance. We apply Track Xplorer in a collaborative project to develop classifiers to detect movements from multisensor data gathered from Parkinson's disease patients. We demonstrate how Track Xplorer helps identify early on possible systemic data errors, effectively track and compare the results of different classifiers, and reason about and pinpoint the causes of misclassifications.},
	language = {en},
	number = {3},
	urldate = {2019-11-24},
	journal = {Computer Graphics Forum},
	author = {Cavallo, Marco and Demiralp, Çağatay},
	year = {2018},
	keywords = {retrospective analysis, ML model selection, tbd},
	pages = {339--349},
	file = {Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\UVQ4E53X\\Cavallo and Demiralp - 2018 - Track Xplorer A System for Visual Analysis of Sen.pdf:application/pdf}
}

@article{ceneda_review_2019,
	title = {A {Review} of {Guidance} {Approaches} in {Visual} {Data} {Analysis}: {A} {Multifocal} {Perspective}},
	volume = {38},
	copyright = {© 2019 The Author(s) Computer Graphics Forum © 2019 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {A {Review} of {Guidance} {Approaches} in {Visual} {Data} {Analysis}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13730},
	doi = {10.1111/cgf.13730},
	abstract = {Visual data analysis can be envisioned as a collaboration of the user and the computational system with the aim of completing a given task. Pursuing an effective system-user integration, in which the system actively helps the user to reach his/her analysis goal has been focus of visualization research for quite some time. However, this problem is still largely unsolved. As a result, users might be overwhelmed by powerful but complex visual analysis systems which also limits their ability to produce insightful results. In this context, guidance is a promising step towards enabling an effective mixed-initiative collaboration to promote the visual analysis. However, the way how guidance should be put into practice is still to be unravelled. Thus, we conducted a comprehensive literature research and provide an overview of how guidance is tackled by different approaches in visual analysis systems. We distinguish between guidance that is provided by the system to support the user, and guidance that is provided by the user to support the system. By identifying open problems, we highlight promising research directions and point to missing factors that are needed to enable the envisioned human-computer collaboration, and thus, promote a more effective visual data analysis.},
	language = {en},
	number = {3},
	urldate = {2019-11-23},
	journal = {Computer Graphics Forum},
	author = {Ceneda, Davide and Gschwandtner, Theresia and Miksch, Silvia},
	year = {2019},
	keywords = {WHEN - Real-Time Applications, WHY - Model Steering, Type of Work: Survey, WHY - Adaptive Systems / Guidance, HOW - Probabilistic Models / Prediction, HOW - Classification Models, WHEN - Retrospective Analyses, WHY - Report Generation / Storytelling},
	pages = {861--879},
	file = {Ceneda et al. - 2019 - A Review of Guidance Approaches in Visual Data Ana.pdf:C\:\\Users\\conny\\Zotero\\storage\\REC2LR7E\\Ceneda et al. - 2019 - A Review of Guidance Approaches in Visual Data Ana.pdf:application/pdf}
}

@article{mathisen_insideinsights:_2019,
	title = {{InsideInsights}: {Integrating} {Data}-{Driven} {Reporting} in {Collaborative} {Visual} {Analytics}},
	volume = {38},
	copyright = {© 2019 The Author(s) Computer Graphics Forum © 2019 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {{InsideInsights}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13717},
	doi = {10.1111/cgf.13717},
	abstract = {Analyzing complex data is a non-linear process that alternates between identifying discrete facts and developing overall assessments and conclusions. In addition, data analysis rarely occurs in solitude; multiple collaborators can be engaged in the same analysis, or intermediate results can be reported to stakeholders. However, current data-driven communication tools are detached from the analysis process and promote linear stories that forego the hierarchical and branching nature of data analysis, which leads to either too much or too little detail in the final report. We propose a conceptual design for integrated data-driven reporting that allows for iterative structuring of insights into hierarchies linked to analytic provenance and chosen analysis views. The hierarchies become dynamic and interactive reports where collaborators can review and modify the analysis at a desired level of detail. Our web-based InsideInsights system provides interaction techniques to annotate states of analytic components, structure annotations, and link them to appropriate presentation views. We demonstrate the generality and usefulness of our system with two use cases and a qualitative expert review.},
	language = {en},
	number = {3},
	urldate = {2019-11-23},
	journal = {Computer Graphics Forum},
	author = {Mathisen, A. and Horak, T. and Klokmose, C. N. and Grønbæk, K. and Elmqvist, N.},
	year = {2019},
	keywords = {Type of Work: Empirical Study, WHEN - Retrospective Analyses, HOW - Other, WHY - Report Generation / Storytelling, Type of Work: Theory \& Model},
	pages = {649--661},
	file = {Mathisen et al. - 2019 - InsideInsights Integrating Data-Driven Reporting .pdf:C\:\\Users\\conny\\Zotero\\storage\\9DEX97EB\\Mathisen et al. - 2019 - InsideInsights Integrating Data-Driven Reporting .pdf:application/pdf}
}

@article{battle_characterizing_2019,
	title = {Characterizing {Exploratory} {Visual} {Analysis}: {A} {Literature} {Review} and {Evaluation} of {Analytic} {Provenance} in {Tableau}},
	volume = {38},
	copyright = {© 2019 The Author(s) Computer Graphics Forum © 2019 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {Characterizing {Exploratory} {Visual} {Analysis}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13678},
	doi = {10.1111/cgf.13678},
	abstract = {Supporting exploratory visual analysis (EVA) is a central goal of visualization research, and yet our understanding of the process is arguably vague and piecemeal. We contribute a consistent definition of EVA through review of the relevant literature, and an empirical evaluation of existing assumptions regarding how analysts perform EVA using Tableau, a popular visual analysis tool. We present the results of a study where 27 Tableau users answered various analysis questions across 3 datasets. We measure task performance, identify recurring patterns across participants' analyses, and assess variance from task specificity and dataset. We find striking differences between existing assumptions and the collected data. Participants successfully completed a variety of tasks, with over 80\% accuracy across focused tasks with measurably correct answers. The observed cadence of analyses is surprisingly slow compared to popular assumptions from the database community. We find significant overlap in analyses across participants, showing that EVA behaviors can be predictable. Furthermore, we find few structural differences between behavior graphs for open-ended and more focused exploration tasks.},
	language = {en},
	number = {3},
	urldate = {2019-11-23},
	journal = {Computer Graphics Forum},
	author = {Battle, Leilani and Heer, Jeffrey},
	year = {2019},
	keywords = {HOW - Pattern Analysis, Type of Work: Technique, Type of Work: Survey, HOW - Probabilistic Models / Prediction, WHEN - Retrospective Analyses, Type of Work: Theory \& ModelHOW - Probabilistic Models / Prediction, Type of Work: Survey, Type of Work: Technique, Type of Work: Theory \& Model, WHEN - Retrospective Analyses, WHY - User Behavior / User Characteristics / User Modelling},
	pages = {145--159},
	file = {Battle and Heer - 2019 - Characterizing Exploratory Visual Analysis A Lite.pdf:C\:\\Users\\conny\\Zotero\\storage\\T5MA766N\\Battle and Heer - 2019 - Characterizing Exploratory Visual Analysis A Lite.pdf:application/pdf}
}

@article{liu_coreflow:_2017,
	title = {{CoreFlow}: {Extracting} and {Visualizing} {Branching} {Patterns} from {Event} {Sequences}},
	volume = {36},
	issn = {01677055},
	shorttitle = {{CoreFlow}},
	url = {http://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=123910133&site=ehost-live&authtype=sso&custid=s5409946},
	doi = {10.1111/cgf.13208},
	abstract = {Event sequence datasets with high event cardinality and long sequences are difficult to visualize and analyze. In particular, it is hard to generate a high level visual summary of paths and volume of flow. Existing approaches of mining and visualizing frequent sequential patterns look promising, but have limitations in terms of scalability, interpretability and utility. We propose CoreFlow, a technique that automatically extracts and visualizes branching patterns in event sequences. CoreFlow constructs a tree by recursively applying a three-step procedure: rank events, divide sequences into groups, and trim sequences by the chosen event. The resulting tree contains key events as nodes, and links represent aggregated flows between key events. Based on CoreFlow, we have developed an interactive system for event sequence analysis. Our approach can compute branching patterns for millions of events in a few seconds, with improved interpretability of extracted patterns compared to previous work. We also present case studies of using the system in three different domains and discuss success and failure cases of applying CoreFlow to real-world analytic problems. These case studies call forth future research on metrics and models to evaluate the quality of visual summaries of event sequences.},
	number = {3},
	urldate = {2019-12-01},
	journal = {Computer Graphics Forum},
	author = {Liu, Zhicheng and Kerr, Bernard and Dontcheva, Mira and Grover, Justin and Hoffman, Matthew and Wilson, Alan},
	month = jun,
	year = {2017},
	keywords = {WHY - Evaluation of Tools and Systems, HOW - Pattern Analysis, Type of Work: Empirical Study, HOW - Classification Models, WHEN - Retrospective Analyses, Type of Work: Technique \& Algorithm, WHY - User Behavior / User Characteristics / User Modelling},
	pages = {527--538},
	file = {EBSCO Full Text:C\:\\Users\\conny\\Zotero\\storage\\YIX4KNIU\\Liu et al. - 2017 - CoreFlow Extracting and Visualizing Branching Pat.pdf:application/pdf}
}

@article{kijmongkolchai_empirically_2017,
	title = {Empirically {Measuring} {Soft} {Knowledge} in {Visualization}},
	volume = {36},
	issn = {01677055},
	url = {http://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=123910099&site=ehost-live&authtype=sso&custid=s5409946},
	doi = {10.1111/cgf.13169},
	abstract = {In this paper, we present an empirical study designed to evaluate the hypothesis that humans' soft knowledge can enhance the cost-benefit ratio of a visualization process by reducing the potential distortion. In particular, we focused on the impact of three classes of soft knowledge: (i) knowledge about application contexts, (ii) knowledge about the patterns to be observed (i.e., in relation to visualization task), and (iii) knowledge about statistical measures. We mapped these classes into three control variables, and used real-world time series data to construct stimuli. The results of the study confirmed the positive contribution of each class of knowledge towards the reduction of the potential distortion, while the knowledge about the patterns prevents distortion more effectively than the other two classes.},
	number = {3},
	urldate = {2019-12-01},
	journal = {Computer Graphics Forum},
	author = {Kijmongkolchai, Natchaya and Abdul‐Rahman, Alfie and Chen, Min},
	month = jun,
	year = {2017},
	pages = {73--85},
	file = {EBSCO Full Text:C\:\\Users\\conny\\Zotero\\storage\\GXNK4LL3\\Kijmongkolchai et al. - 2017 - Empirically Measuring Soft Knowledge in Visualizat.pdf:application/pdf}
}

@article{lukasczyk_nested_2017,
	title = {Nested {Tracking} {Graphs}},
	volume = {36},
	issn = {01677055},
	url = {http://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=123910104&site=ehost-live&authtype=sso&custid=s5409946},
	doi = {10.1111/cgf.13164},
	abstract = {Tracking graphs are a well established tool in topological analysis to visualize the evolution of components and their properties over time, i.e., when components appear, disappear, merge, and split. However, tracking graphs are limited to a single level threshold and the graphs may vary substantially even under small changes to the threshold. To examine the evolution of features for varying levels, users have to compare multiple tracking graphs without a direct visual link between them. We propose a novel, interactive, nested graph visualization based on the fact that the tracked superlevel set components for different levels are related to each other through their nesting hierarchy. This approach allows us to set multiple tracking graphs in context to each other and enables users to effectively follow the evolution of components for different levels simultaneously. We demonstrate the effectiveness of our approach on datasets from finite pointset methods, computational fluid dynamics, and cosmology simulations.},
	number = {3},
	urldate = {2019-12-01},
	journal = {Computer Graphics Forum},
	author = {Lukasczyk, Jonas and Weber, Gunther and Maciejewski, Ross and Garth, Christoph and Leitte, Heike},
	month = jun,
	year = {2017},
	keywords = {NOT SURE},
	pages = {12--22},
	file = {EBSCO Full Text:C\:\\Users\\conny\\Zotero\\storage\\ETCHB2WJ\\Lukasczyk et al. - 2017 - Nested Tracking Graphs.pdf:application/pdf}
}

@article{porteous_applying_2010,
	title = {Applying {Planning} to {Interactive} {Storytelling}: {Narrative} {Control} {Using} {State} {Constraints}},
	volume = {1},
	issn = {2157-6904},
	shorttitle = {Applying {Planning} to {Interactive} {Storytelling}},
	url = {http://doi.acm.org/10.1145/1869397.1869399},
	doi = {10.1145/1869397.1869399},
	abstract = {We have seen ten years of the application of AI planning to the problem of narrative generation in Interactive Storytelling (IS). In that time planning has emerged as the dominant technology and has featured in a number of prototype systems. Nevertheless key issues remain, such as how best to control the shape of the narrative that is generated (e.g., by using narrative control knowledge, i.e., knowledge about narrative features that enhance user experience) and also how best to provide support for real-time interactive performance in order to scale up to more realistic sized systems. Recent progress in planning technology has opened up new avenues for IS and we have developed a novel approach to narrative generation that builds on this. Our approach is to specify narrative control knowledge for a given story world using state trajectory constraints and then to treat these state constraints as landmarks and to use them to decompose narrative generation in order to address scalability issues and the goal of real-time performance in larger story domains. This approach to narrative generation is fully implemented in an interactive narrative based on the “Merchant of Venice.” The contribution of the work lies both in our novel use of state constraints to specify narrative control knowledge for interactive storytelling and also our development of an approach to narrative generation that exploits such constraints. In the article we show how the use of state constraints can provide a unified perspective on important problems faced in IS.},
	number = {2},
	urldate = {2019-12-02},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Porteous, Julie and Cavazza, Marc and Charles, Fred},
	month = dec,
	year = {2010},
	keywords = {HOW - Program Synthesis, WHEN - Real-Time Applications, WHY - User Behaviour, User Characteristics, User Modelling, Type of Work: Technique \& Algorithm, WHY - Report Generation / Storytelling},
	pages = {10:1--10:21},
	file = {Porteous et al. - 2010 - Applying Planning to Interactive Storytelling Nar.pdf:C\:\\Users\\conny\\Zotero\\storage\\LJDBJKBH\\Porteous et al. - 2010 - Applying Planning to Interactive Storytelling Nar.pdf:application/pdf}
}

@inproceedings{kunkel_3d_2017,
	address = {Limassol, Cyprus},
	title = {A {3D} {Item} {Space} {Visualization} for {Presenting} and {Manipulating} {User} {Preferences} in {Collaborative} {Filtering}},
	isbn = {978-1-4503-4348-0},
	url = {http://dl.acm.org/citation.cfm?doid=3025171.3025189},
	doi = {10.1145/3025171.3025189},
	abstract = {While conventional Recommender Systems perform well in automatically generating personalized suggestions, it is often difﬁcult for users to understand why certain items are recommended and which parts of the item space are covered by the recommendations. Also, the available means to inﬂuence the process of generating results are usually very limited. To alleviate these problems, we suggest a 3D map-based visualization of the entire item space in which we position and present sample items along with recommendations. The map is produced by mapping latent factors obtained from Collaborative Filtering data onto a 2D surface through Multidimensional Scaling. Then, areas that contain items relevant with respect to the current user’s preferences are shown as elevations on the map, areas of low interest as valleys. In addition to the presentation of his or her preferences, the user may interactively manipulate the underlying proﬁle by raising or lowering parts of the landscape, also at cold-start. Each change may lead to an immediate update of the recommendations. Using a demonstrator, we conducted a user study that, among others, yielded promising results regarding the usefulness of our approach.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Intelligent} {User} {Interfaces} - {IUI} '17},
	publisher = {ACM Press},
	author = {Kunkel, Johannes and Loepp, Benedikt and Ziegler, Jürgen},
	year = {2017},
	keywords = {WHEN - Real-Time Applications, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, Type of Work: Technique \& Algorithm, WHY - Model Steering / Active Learning},
	pages = {3--15},
	file = {Kunkel et al. - 2017 - A 3D Item Space Visualization for Presenting and M.pdf:C\:\\Users\\conny\\Zotero\\storage\\HQE5ATNG\\Kunkel et al. - 2017 - A 3D Item Space Visualization for Presenting and M.pdf:application/pdf}
}

@INPROCEEDINGS{provenance-layer, author={D. {Gotz} and M. X. {Zhou}}, booktitle={2008 IEEE Symposium on Visual Analytics Science and Technology}, title={Characterizing users’ visual analytic activity for insight provenance}, year={2008}, volume={}, number={}, pages={123-130},}

@inproceedings{gotz_behavior-driven_2009,
	address = {New York, NY, USA},
	series = {{IUI} '09},
	title = {Behavior-driven {Visualization} {Recommendation}},
	isbn = {978-1-60558-168-2},
	url = {http://doi.acm.org/10.1145/1502650.1502695},
	doi = {10.1145/1502650.1502695},
	abstract = {We present a novel approach to visualization recommendation that monitors user behavior for implicit signals of user intent to provide more effective recommendation. This is in contrast to previous approaches which are either insensitive to user intent or require explicit, user specified task information. Our approach, called Behavior-Driven Visualization Recommendation (BDVR), consists of two distinct phases: (1) pattern detection, and (2) visualization recommendation. In the first phase, user behavior is analyzed dynamically to find semantically meaningful interaction patterns using a library of pattern definitions developed through observations of real-world visual analytic activity. In the second phase, our BDVR algorithm uses the detected patterns to infer a user's intended visual task. It then automatically suggests alternative visualizations that support the inferred visual task more directly than the user's current visualization. We present the details of BDVR and describe its implementation within our lab's prototype visual analysis system. We also present study results that demonstrate that our approach shortens task completion time and reduces error rates when compared to behavior-agnostic recommendation.},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {ACM},
	author = {Gotz, David and Wen, Zhen},
	year = {2009},
	note = {event-place: Sanibel Island, Florida, USA},
	keywords = {WHEN - Real-Time Applications, HOW - Pattern Analysis, Technique, Others: Pattern Analysis, time-series analysis, search, WHY - User Behaviour, User Characteristics, User Modelling, WHY - Adaptive Systems / Guidance, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study},
	pages = {315--324},
	file = {p315-gotz.pdf:C\:\\Users\\conny\\Zotero\\storage\\9GFRS2WE\\p315-gotz.pdf:application/pdf}
}

@inproceedings{sherkat_interactive_2018,
	address = {New York, NY, USA},
	series = {{IUI} '18},
	title = {Interactive {Document} {Clustering} {Revisited}: {A} {Visual} {Analytics} {Approach}},
	isbn = {978-1-4503-4945-1},
	shorttitle = {Interactive {Document} {Clustering} {Revisited}},
	url = {http://doi.acm.org/10.1145/3172944.3172964},
	doi = {10.1145/3172944.3172964},
	abstract = {Document clustering is an efficient way to get insight into large text collections. Due to the personalized nature of document clustering, even the best fully automatic algorithms cannot create clusters that accurately reflect the user»s perspectives. To incorporate the user»s perspective in the clustering process and, at the same time, effectively visualize document collections to enhance user's sense-making of data, we propose a novel visual analytics system for interactive document clustering. We built our system on top of clustering algorithms that can adapt to user's feedback. First, the initial clustering is created based on the user-defined number of clusters and the selected clustering algorithm. Second, the clustering result is visualized to the user. A collection of coordinated visualization modules and document projection is designed to guide the user towards a better insight into the document collection and clusters. The user changes clusters and key-terms iteratively as a feedback to the clustering algorithm until the result is satisfactory. In key-term based interaction, the user assigns a set of key-terms to each target cluster to guide the clustering algorithm. A set of quantitative experiments, a use case, and a user study have been conducted to show the advantages of the approach for document analytics based on clustering.},
	urldate = {2019-12-09},
	booktitle = {23rd {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {ACM},
	author = {Sherkat, Ehsan and Nourashrafeddin, Seyednaser and Milios, Evangelos E. and Minghim, Rosane},
	year = {2018},
	note = {event-place: Tokyo, Japan},
	keywords = {WHEN - Real-Time Applications, Others: Pattern Analysis, time-series analysis, search, Type of Work: Empirical Study, HOW - Classification Models, Type of Work: Technique \& Algorithm, WHY - Model Steering / Active Learning},
	pages = {281--292},
	file = {Sherkat et al. - 2018 - Interactive Document Clustering Revisited A Visua.pdf:C\:\\Users\\conny\\Zotero\\storage\\BEA62GBC\\Sherkat et al. - 2018 - Interactive Document Clustering Revisited A Visua.pdf:application/pdf}
}

@inproceedings{khan_flux_2019,
	address = {Marina del Ray, California},
	title = {Flux capacitors for {JavaScript} deloreans: approximate caching for physics-based data interaction},
	isbn = {978-1-4503-6272-6},
	shorttitle = {Flux capacitors for {JavaScript} deloreans},
	url = {http://dl.acm.org/citation.cfm?doid=3301275.3302291},
	doi = {10.1145/3301275.3302291},
	abstract = {Interactive visualizations have become an e ective and pervasive mode of allowing users to explore the data in a visual, uid, and immersive manner. While modern web, mobile, touch, and gesturedriven next-generation interfaces such as Leap Motion allow for highly interactive experiences, they pose unique and unprecedented workloads to the underlying data platform. Usually, these visualizations do not need precise results for most queries generated during an interaction, and the users require the intermediate results as feedback only to guide them towards their goal query. We present a middleware component - Flux Capacitor, that insulates the backend from bursty and query-intensive workloads. Flux Capacitor uses prefetching and caching strategies devised by exploiting the inherent physics-metaphor of UI widgets such as friction and inertia in range sliders, and typical characteristics of user-interaction. This enables low interaction response times while intelligently trading o accuracy.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Intelligent} {User} {Interfaces} - {IUI} '19},
	publisher = {ACM Press},
	author = {Khan, Meraj Ahmed and Nandi, Arnab},
	year = {2019},
	keywords = {WHEN - Real-Time Applications, WHY - Prefetching, WHY - User Behaviour, User Characteristics, User Modelling, Type of Work: Empirical Study, Type of Work: Technique \& Algorithm, HOW - Other, WHY - Explain ML Model / Debug Algorithm / Query Plan},
	pages = {177--185},
	file = {Khan and Nandi - 2019 - Flux capacitors for JavaScript deloreans approxim.pdf:C\:\\Users\\conny\\Zotero\\storage\\BC7UD4MC\\Khan and Nandi - 2019 - Flux capacitors for JavaScript deloreans approxim.pdf:application/pdf}
}

@inproceedings{wegba_interactive_2018,
	address = {New York, NY, USA},
	series = {{IUI} '18},
	title = {Interactive {Storytelling} for {Movie} {Recommendation} {Through} {Latent} {Semantic} {Analysis}},
	isbn = {978-1-4503-4945-1},
	url = {http://doi.acm.org/10.1145/3172944.3172979},
	doi = {10.1145/3172944.3172979},
	abstract = {Recommendation is essential to many online services; however current systems often provide limited interaction and visualization mechanisms, affecting the user satisfaction of recommendation. This paper presents an interactive recommendation approach for the general public without any knowledge of recommendation or visualization algorithms. Our approach emphasizes interactivity, explicit user input, and semantic information convey with the following two components. First, we propose a Latent Semantic Model that captures the statistical features of semantic concepts on 2D domains and abstracts user preferences for personal recommendation, so that high-dimensional spectral space from the rating records can be understood and interacted with directly. Second, we propose an interactive recommendation approach through a storytelling mechanism for promoting the communication between the user and the recommendation system. We demonstrate and evaluate our approach with a real dataset. Our approach can also be extended to other applications including various online recommendation systems.},
	urldate = {2019-12-09},
	booktitle = {23rd {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {ACM},
	author = {Wegba, Kodzo and Lu, Aidong and Li, Yuemeng and Wang, Wencheng},
	year = {2018},
	note = {event-place: Tokyo, Japan},
	keywords = {WHEN - Real-Time Applications, WHY - Report Generation and Storytelling, WHY - Adaptive Systems / Guidance, Type of Work: Empirical Study, HOW - Classification Models, Type of Work: Technique \& Algorithm, HOW - Other},
	pages = {521--533},
	file = {Wegba et al. - 2018 - Interactive Storytelling for Movie Recommendation .pdf:C\:\\Users\\conny\\Zotero\\storage\\D9LIYPLC\\Wegba et al. - 2018 - Interactive Storytelling for Movie Recommendation .pdf:application/pdf}
}

@inproceedings{gotz_adaptive_2016,
	address = {Sonoma, California, USA},
	title = {Adaptive {Contextualization}: {Combating} {Bias} {During} {High}-{Dimensional} {Visualization} and {Data} {Selection}},
	isbn = {978-1-4503-4137-0},
	shorttitle = {Adaptive {Contextualization}},
	url = {http://dl.acm.org/citation.cfm?doid=2856767.2856779},
	doi = {10.1145/2856767.2856779},
	abstract = {Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. In this paper, we present Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is speciﬁcally designed to combat the invisible introduction of selection bias. Our approach (1) monitors and models a users visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. We also share results from a user study which demonstrate the effectiveness of our technique.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Intelligent} {User} {Interfaces} - {IUI} '16},
	publisher = {ACM Press},
	author = {Gotz, David and Sun, Shun and Cao, Nan},
	year = {2016},
	keywords = {WHEN - Real-Time Applications, Hellinger distance metric, WHY - User Behaviour, User Characteristics, User Modelling, WHY - Adaptive Systems / Guidance, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, Type of Work: Technique \& Algorithm},
	pages = {85--95},
	file = {Gotz et al. - 2016 - Adaptive Contextualization Combating Bias During .pdf:C\:\\Users\\conny\\Zotero\\storage\\3WAGYHJR\\Gotz et al. - 2016 - Adaptive Contextualization Combating Bias During .pdf:application/pdf}
}

@inproceedings{setlur_inferencing_2019,
	address = {Marina del Ray, California},
	title = {Inferencing underspecified natural language utterances in visual analysis},
	isbn = {978-1-4503-6272-6},
	url = {http://dl.acm.org/citation.cfm?doid=3301275.3302270},
	doi = {10.1145/3301275.3302270},
	abstract = {Handling ambiguity and underspecification of users’ utterances is challenging, particularly for natural language interfaces that help with visual analytical tasks. Constraints in the underlying analytical platform and the users’ expectations of high precision and recall require thoughtful inferencing to help generate useful responses. In this paper, we introduce a system to resolve partial utterances based on syntactic and semantic constraints of the underlying analytical expressions. We extend inferencing based on best practices in information visualization to generate useful visualization responses. We employ heuristics to help constrain the solution space of possible inferences, and apply ranking logic to the interpretations based on relevancy. We evaluate the quality of inferred interpretations based on relevancy and analytical usefulness.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Intelligent} {User} {Interfaces} - {IUI} '19},
	publisher = {ACM Press},
	author = {Setlur, Vidya and Tory, Melanie and Djalali, Alex},
	year = {2019},
	keywords = {WHEN - Real-Time Applications, WHY - User Behaviour, User Characteristics, User Modelling, WHY - Adaptive Systems / Guidance, Type of Work: Empirical Study, Type of Work: Technique \& Algorithm, HOW - Other},
	pages = {40--51},
	file = {Setlur et al. - 2019 - Inferencing underspecified natural language uttera.pdf:C\:\\Users\\conny\\Zotero\\storage\\HXBMWC9S\\Setlur et al. - 2019 - Inferencing underspecified natural language uttera.pdf:application/pdf}
}

@inproceedings{verbert_visualizing_2013,
	address = {Santa Monica, California, USA},
	title = {Visualizing recommendations to support exploration, transparency and controllability},
	isbn = {978-1-4503-1965-2},
	url = {http://dl.acm.org/citation.cfm?doid=2449396.2449442},
	doi = {10.1145/2449396.2449442},
	abstract = {Research on recommender systems has traditionally focused on the development of algorithms to improve accuracy of recommendations. So far, little research has been done to enable user interaction with such systems as a basis to support exploration and control by end users. In this paper, we present our research on the use of information visualization techniques to interact with recommender systems. We investigated how information visualization can improve user understanding of the typically black-box rationale behind recommendations in order to increase their perceived relevance and meaning and to support exploration and user involvement in the recommendation process. Our study has been performed using TalkExplorer, an interactive visualization tool developed for attendees of academic conferences. The results of user studies performed at two conferences allowed us to obtain interesting insights to enhance user interfaces that integrate recommendation technology. More specifically, effectiveness and probability of item selection both increase when users are able to explore and interrelate multiple entities – i.e. items bookmarked by users, recommendations and tags.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2013 international conference on {Intelligent} user interfaces - {IUI} '13},
	publisher = {ACM Press},
	author = {Verbert, Katrien and Parra, Denis and Brusilovsky, Peter and Duval, Erik},
	year = {2013},
	keywords = {recommendation},
	pages = {351},
	annote = {not sure if this is relevant. The abstract hints at using interaction to improve recommendations.},
	file = {Verbert et al. - 2013 - Visualizing recommendations to support exploration.pdf:C\:\\Users\\conny\\Zotero\\storage\\4NH3PDTQ\\Verbert et al. - 2013 - Visualizing recommendations to support exploration.pdf:application/pdf}
}

@inproceedings{lee_avoiding_2019,
	address = {Marina del Ray, California},
	title = {Avoiding drill-down fallacies with \textit{{VisPilot}}: assisted exploration of data subsets},
	isbn = {978-1-4503-6272-6},
	shorttitle = {Avoiding drill-down fallacies with \textit{{VisPilot}}},
	url = {http://dl.acm.org/citation.cfm?doid=3301275.3302307},
	doi = {10.1145/3301275.3302307},
	abstract = {As datasets continue to grow in size and complexity, exploring multidimensional datasets remain challenging for analysts. A common operation during this exploration is drill-down—understanding the behavior of data subsets by progressively adding filters. While widely used, in the absence of careful attention towards confounding factors, drill-downs could lead to inductive fallacies. Specifically, an analyst may end up being “deceived” into thinking that a deviation in trend is attributable to a local change, when in fact it is a more general phenomenon; we term this the drill-down fallacy. One way to avoid falling prey to drill-down fallacies is to exhaustively explore all potential drill-down paths, which quickly becomes infeasible on complex datasets with many attributes. We present VisPilot, an accelerated visual data exploration tool that guides analysts through the key insights in a dataset, while avoiding drilldown fallacies. Our user study results show that VisPilot helps analysts discover interesting visualizations, understand attribute importance, and predict unseen visualizations better than other multidimensional data analysis baselines.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Intelligent} {User} {Interfaces} - {IUI} '19},
	publisher = {ACM Press},
	author = {Lee, Doris Jung-Lin and Dev, Himel and Hu, Huizi and Elmeleegy, Hazem and Parameswaran, Aditya},
	year = {2019},
	keywords = {WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, WHY - Adaptive Systems / Guidance, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, Type of Work: Technique \& Algorithm, WHY - Report Generation / Storytelling},
	pages = {186--196},
	file = {Lee et al. - 2019 - Avoiding drill-down fallacies with iVisPiloti.pdf:C\:\\Users\\conny\\Zotero\\storage\\DP5KU58Y\\Lee et al. - 2019 - Avoiding drill-down fallacies with iVisPiloti.pdf:application/pdf}
}

@article{micallef_interactive_2017,
	title = {Interactive {Elicitation} of {Knowledge} on {Feature} {Relevance} {Improves} {Predictions} in {Small} {Data} {Sets}},
	url = {http://arxiv.org/abs/1612.02487},
	abstract = {Providing accurate predictions is challenging for machine learning algorithms when the number of features is larger than the number of samples in the data. Prior knowledge can improve machine learning models by indicating relevant variables and parameter values. Yet, this prior knowledge is often tacit and only available from domain experts. We present a novel approach that uses interactive visualization to elicit the tacit prior knowledge and uses it to improve the accuracy of prediction models. The main component of our approach is a user model that models the domain expert's knowledge of the relevance of different features for a prediction task. In particular, based on the expert's earlier input, the user model guides the selection of the features on which to elicit user's knowledge next. The results of a controlled user study show that the user model significantly improves prior knowledge elicitation and prediction accuracy, when predicting the relative citation counts of scientific documents in a specific domain.},
	urldate = {2019-12-09},
	journal = {arXiv:1612.02487 [cs, stat]},
	author = {Micallef, Luana and Sundin, Iiris and Marttinen, Pekka and Ammad-ud-din, Muhammad and Peltola, Tomi and Soare, Marta and Jacucci, Giulio and Kaski, Samuel},
	month = jan,
	year = {2017},
	note = {arXiv: 1612.02487},
	keywords = {WHEN - Real-Time Applications, WHY - User Behaviour, User Characteristics, User Modelling, WHY - Adaptive Systems / Guidance, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, Type of Work: Technique \& Algorithm, WHY - Model Steering / Active Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\conny\\Zotero\\storage\\89GUL3KF\\Micallef et al. - 2017 - Interactive Elicitation of Knowledge on Feature Re.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\conny\\Zotero\\storage\\DUS6R5FC\\1612.html:text/html}
}

@inproceedings{toker_pupillometry_2017,
	address = {New York, NY, USA},
	series = {{IUI} '17},
	title = {Pupillometry and {Head} {Distance} to the {Screen} to {Predict} {Skill} {Acquisition} {During} {Information} {Visualization} {Tasks}},
	isbn = {978-1-4503-4348-0},
	url = {http://doi.acm.org/10.1145/3025171.3025187},
	doi = {10.1145/3025171.3025187},
	abstract = {In this paper we investigate using a variety of behavioral measures collectible with an eye tracker to predict a user's skill acquisition phase while performing various information visualization tasks with bar graphs. Our long term goal is to use this information in real-time to create user-adaptive visualizations that can provide personalized support to facilitate visualization processing based on the user's predicted skill level. We show that leveraging two additional content-independent data sources, namely information on a user's pupil dilation and head distance to the screen, yields a significant improvement for predictive accuracies of skill acquisition compared to predictions made using content-dependent information related to user eye gaze attention patterns, as was done in previous work. We show that including features from both pupil dilation and head distance to the screen improve the ability to predict users' skill acquisition state, beating both the baseline and a model using only content-dependent gaze information.},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the {22Nd} {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {ACM},
	author = {Toker, Dereck and Lallé, Sébastien and Conati, Cristina},
	year = {2017},
	note = {event-place: Limassol, Cyprus},
	keywords = {Eye-Tracking, WHEN - Retrospective Analysis, Cognitive Abilities, WHY - User Behaviour, User Characteristics, User Modelling, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, HOW - Classification Models},
	pages = {221--231},
	file = {Toker et al. - 2017 - Pupillometry and Head Distance to the Screen to Pr.pdf:C\:\\Users\\conny\\Zotero\\storage\\RQ65JW8W\\Toker et al. - 2017 - Pupillometry and Head Distance to the Screen to Pr.pdf:application/pdf}
}

@inproceedings{toker_towards_2014,
	address = {Haifa, Israel},
	title = {Towards facilitating user skill acquisition: identifying untrained visualization users through eye tracking},
	isbn = {978-1-4503-2184-6},
	shorttitle = {Towards facilitating user skill acquisition},
	url = {http://dl.acm.org/citation.cfm?doid=2557500.2557524},
	doi = {10.1145/2557500.2557524},
	abstract = {A key challenge for information visualization designers lies in developing systems that best support users in terms of their individual abilities, needs, and preferences. However, most visualizations require users to first gather a certain set of skills before they can efficiently process the displayed information. This paper presents a first step towards designing visualizations that provide personalized support in order to ease the so-called ‘learning curve’ during a user’s skill acquisition phase. We present prediction models, trained on users’ gaze data, that can identify if users are still in the skill acquisition phase or if they have gained the necessary abilities. The paper first reveals that users exhibit the learning curve even during the usage of simple information visualizations, and then shows that we can generate reasonably accurate predictions about a user’s skill acquisition using solely their eye gaze behavior.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 19th international conference on {Intelligent} {User} {Interfaces} - {IUI} '14},
	publisher = {ACM Press},
	author = {Toker, Dereck and Steichen, Ben and Gingerich, Matthew and Conati, Cristina and Carenini, Giuseppe},
	year = {2014},
	keywords = {Eye-Tracking, Congitive Abilities, WHY - User Behaviour, User Characteristics, User Modelling, WHY - Adaptive Systems / Guidance, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, HOW - Classification Models, WHEN - Retrospective Analyses},
	pages = {105--114},
	file = {Toker et al. - 2014 - Towards facilitating user skill acquisition ident.pdf:C\:\\Users\\conny\\Zotero\\storage\\THMFWDA5\\Toker et al. - 2014 - Towards facilitating user skill acquisition ident.pdf:application/pdf}
}

@article{shrinivasan_supporting_2010,
	title = {Supporting {Exploratory} {Analysis} with the {Select} \& {Slice} {Table}},
	volume = {29},
	issn = {01677055},
	url = {http://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=52903459&site=ehost-live&authtype=sso&custid=s5409946},
	doi = {10.1111/j.1467-8659.2009.01696.x},
	abstract = {In interactive visualization, selection techniques such as dynamic queries and brushing are used to specify and extract items of interest. In other words, users define areas of interest in data space that often have a clear semantic meaning. We call such areas Semantic Zones, and argue that support for their manipulation and reasoning with them is highly useful during exploratory analysis. An important use case is the use of these zones across different subsets of the data, for instance to study the population of semantic zones over time. To support this, we present the Select \& Slice Table. Semantic zones are arranged along one axis of the table, and data subsets are arranged along the other axis of the table. Each cell contains a set of items of interest from a data subset that matches the selection specifications of a zone. Items in cells can be visualized in various ways, as a count, as an aggregation of a measure, or as a separate visualization, such that the table gives an overview of the relationship between zones and data subsets. Furthermore, users can reuse zones, combine zones, and compare and trace items of interest across different semantic zones and data subsets. We present two case studies to illustrate the support offered by the Select \& Slice table during exploratory analysis of multivariate data.},
	number = {3},
	urldate = {2019-12-06},
	journal = {Computer Graphics Forum},
	author = {Shrinivasan, Yedendra B. and van Wijk, Jarke. J.},
	month = jun,
	year = {2010},
	keywords = {Real-time applications, not sure, real-time quantification, Type of Work: Case Study},
	pages = {803--812},
	file = {EBSCO Full Text:C\:\\Users\\conny\\Zotero\\storage\\CG6JIWX4\\Shrinivasan and van Wijk - 2010 - Supporting Exploratory Analysis with the Select & .pdf:application/pdf}
}

@article{javed_explates:_2013,
	title = {{ExPlates}: {Spatializing} {Interactive} {Analysis} to {Scaffold} {Visual} {Exploration}},
	volume = {32},
	issn = {01677055},
	shorttitle = {{ExPlates}},
	url = {http://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=88800219&site=ehost-live&authtype=sso&custid=s5409946},
	doi = {10.1111/cgf.12131},
	abstract = {Visual exploration involves using visual representations to investigate data where the goals of the process are unclear and poorly defined. However, this often places unduly high cognitive load on the user, particularly in terms of keeping track of multiple investigative branches, remembering earlier results, and correlating between different views. We propose a new methodology for automatically spatializing the individual steps in visual exploration onto a large visual canvas, allowing users to easily recall, reflect, and assess their progress. We also present a web-based implementation of our methodology called E xP latesJS where users can manipulate multidimensional data in their browsers, automatically building visual queries as they explore the data.},
	number = {3pt4},
	urldate = {2019-12-05},
	journal = {Computer Graphics Forum},
	author = {Javed, W. and Elmqvist, N.},
	month = dec,
	year = {2013},
	keywords = {WHEN - Real-Time Applications, Type of Work: Case Study, WHY - Report Generation and Storytelling, NOT SURE},
	pages = {441--450},
	file = {EBSCO Full Text:C\:\\Users\\conny\\Zotero\\storage\\FZGC6K4E\\Javed and Elmqvist - 2013 - ExPlates Spatializing Interactive Analysis to Sca.pdf:application/pdf}
}

@article{nafari_augmenting_2013,
	title = {Augmenting {Visualization} with {Natural} {Language} {Translation} of {Interaction}: {A} {Usability} {Study}},
	volume = {32},
	issn = {01677055},
	shorttitle = {Augmenting {Visualization} with {Natural} {Language} {Translation} of {Interaction}},
	url = {http://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=88800211&site=ehost-live&authtype=sso&custid=s5409946},
	doi = {10.1111/cgf.12126},
	abstract = {As visualization tools get more complicated, users often find it increasingly difficult to learn interaction sequences, recall past queries, and interpret visual states. We examine a query-to-question (Q2Q) supporting system that takes advantage of natural language generation (NLG) techniques to automatically translate and display query interactions as natural language questions. We focus on a symmetric pattern of multiple coordinated views, cross-filtered views, that involves only nominal/categorical data. We describe a study of the effects of pairing a visualization with a Q2Q interface on several aspects of usability. Q2Q produces considerable improvements in learnability, efficiency, and memorability of visualization in terms of speed and the length of interaction sequences that users follow, along with a modest decrease in error ratio. From a visual language perspective, we analyze how Q2Q speeds up users' comprehension of interaction, particularly when a visualization representation has deficiencies in illustrating hidden items or relationships.},
	number = {3pt4},
	urldate = {2019-12-05},
	journal = {Computer Graphics Forum},
	author = {Nafari, M. and Weaver, C.},
	month = dec,
	year = {2013},
	keywords = {WHEN - Retrospective Analysis, Type of Work: User Study, WHY - Evaluation of Tools and Systems, HOW - Classification, NOT SURE},
	pages = {391--400},
	file = {EBSCO Full Text:C\:\\Users\\conny\\Zotero\\storage\\B43673JK\\Nafari and Weaver - 2013 - Augmenting Visualization with Natural Language Tra.pdf:application/pdf}
}

@article{boukhelifa_evolutionary_2013,
	title = {Evolutionary {Visual} {Exploration}: {Evaluation} {With} {Expert} {Users}},
	volume = {32},
	issn = {01677055},
	shorttitle = {Evolutionary {Visual} {Exploration}},
	url = {http://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=88800174&site=ehost-live&authtype=sso&custid=s5409946},
	doi = {10.1111/cgf.12090},
	abstract = {We present an Evolutionary Visual Exploration (EVE) system that combines visual analytics with stochastic optimisation to aid the exploration of multidimensional datasets characterised by a large number of possible views or projections. Starting from dimensions whose values are automatically calculated by a PCA, an interactive evolutionary algorithm progressively builds (or evolves) non-trivial viewpoints in the form of linear and non-linear dimension combinations, to help users discover new interesting views and relationships in their data. The criteria for evolving new dimensions is not known a priori and are partially specified by the user via an interactive interface: (i) The user selects views with meaningful or interesting visual patterns and provides a satisfaction score. (ii) The system calibrates a fitness function (optimised by the evolutionary algorithm) to take into account the user input, and then calculates new views. Our method leverages automatic tools to detect interesting visual features and human interpretation to derive meaning, validate the findings and guide the exploration without having to grasp advanced statistical concepts. To validate our method, we built a prototype tool (EvoGraphDice) as an extension of an existing scatterplot matrix inspection tool, and conducted an observational study with five domain experts. Our results show that EvoGraphDice can help users quantify qualitative hypotheses and try out different scenarios to dynamically transform their data. Importantly, it allowed our experts to think laterally, better formulate their research questions and build new hypotheses for further investigation.},
	number = {3pt1},
	urldate = {2019-12-05},
	journal = {Computer Graphics Forum},
	author = {Boukhelifa, N. and Cancino, W. and Bezerianos, A. and Lutton, E.},
	month = jun,
	year = {2013},
	keywords = {WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, WHY - Adaptive Systems and Recommender Systems, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, WHY - Real-time or post-hoc Quantification and Re-Application, WHY - Model Steering / Active Learning, WHY - User Behavior / User Characteristics / User Modelling, Type of Work: System},
	pages = {31--40},
	file = {EBSCO Full Text:C\:\\Users\\conny\\Zotero\\storage\\29TQLZNQ\\Boukhelifa et al. - 2013 - Evolutionary Visual Exploration Evaluation With E.pdf:application/pdf}
}

@article{moritz_perfopticon:_2015,
	title = {Perfopticon: {Visual} {Query} {Analysis} for {Distributed} {Databases}},
	volume = {34},
	issn = {01677055},
	shorttitle = {Perfopticon},
	url = {http://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=108442264&site=ehost-live&authtype=sso&custid=s5409946},
	doi = {10.1111/cgf.12619},
	abstract = {Distributed database performance is often unpredictable due to issues such as system complexity, network congestion, or imbalanced data distribution. These issues are difficult for users to assess in part due to the opaque mapping between declaratively specified queries and actual physical execution plans. Database developers currently must expend significant time and effort scanning log files to isolate and debug the root causes of performance issues. In response, we present Perfopticon, an interactive query profiling tool that enables rapid insight into common problems such as performance bottlenecks and data skew. Perfopticon combines interactive visualizations of (1) query plans, (2) overall query execution, (3) data flow among servers, and (4) execution traces. These views coordinate multiple levels of abstraction to enable detection, isolation, and understanding of performance issues. We evaluate our design choices through engagements with system developers, scientists, and students. We demonstrate that Perfopticon enables performance debugging for real-world tasks.},
	number = {3},
	urldate = {2019-12-05},
	journal = {Computer Graphics Forum},
	author = {Moritz, Dominik and Halperin, Daniel and Howe, Bill and Heer, Jeffrey},
	month = jun,
	year = {2015},
	keywords = {WHY - Evaluation of Tools and Systems, HOW - Pattern Analysis, Type of Work: Empirical Study, WHEN - Retrospective Analyses, Type of Work: Technique \& Algorithm, WHY - Explain ML Model / Debug Algorithm / Query Plan},
	pages = {71--80},
	file = {EBSCO Full Text:C\:\\Users\\conny\\Zotero\\storage\\IJMQELBR\\Moritz et al. - 2015 - Perfopticon Visual Query Analysis for Distributed.pdf:application/pdf}
}

@article{scheepens_rationale_2015,
	title = {Rationale {Visualization} for {Safety} and {Security}},
	volume = {34},
	issn = {01677055},
	url = {http://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=108442262&site=ehost-live&authtype=sso&custid=s5409946},
	doi = {10.1111/cgf.12631},
	abstract = {In safety and security domains where objects of interest (OOI), such as people, vessels, or transactions, are continuously monitored, automated reasoning is required due to their sheer number and volume of information. We present a method to visually explain the rationale of a reasoning engine that raises an alarm if a certain situation is reached. Based both on evidence from heterogeneous and possibly unreliable sources, and on a domain specific reasoning structure, this engine concludes with a certain probability that, e.g., the OOI is suspected of smuggling. To support decision making, we visualize the rationale, an abstraction of the complicated reasoning structure. The evidence is displayed in a color-coded matrix that easily reveals if and where observations contradict. In it, domain and operational experts can quickly understand and find complicated patterns and relate them to real-world situations. Also, two groups of these experts evaluate our system through maritime use cases based on real data.},
	number = {3},
	urldate = {2019-12-05},
	journal = {Computer Graphics Forum},
	author = {Scheepens, Roeland and Michels, Steffen and van de Wetering, Huub and van Wijk, Jarke J.},
	month = jun,
	year = {2015},
	keywords = {WHY - Evaluation of Tools and Systems, WHY - Adaptive Systems / Guidance, Type of Work: Empirical Study, WHEN - Retrospective Analyses, HOW - Other, WHY - Explain ML Model / Debug Algorithm / Query Plan, WHY - Report Generation / Storytelling, WHY - Real-time or post-hoc Quantification and Re-Application},
	pages = {191--200},
	file = {EBSCO Full Text:C\:\\Users\\conny\\Zotero\\storage\\9YT9WMVS\\Scheepens et al. - 2015 - Rationale Visualization for Safety and Security.pdf:application/pdf}
}

@article{dextrasromagnino_segmentifier:_2019,
	title = {Segmentifier: {Interactive} {Refinement} of {Clickstream} {Data}},
	volume = {38},
	copyright = {© 2019 The Author(s) Computer Graphics Forum © 2019 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {Segmentifier},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13715},
	doi = {10.1111/cgf.13715},
	abstract = {Clickstream data has the potential to provide insights into e-commerce consumer behavior, but previous techniques fall short of handling the scale and complexity of real-world datasets because they require relatively clean and small input. We present Segmentifier, a novel visual analytics interface that supports an iterative process of refining collections of action sequences into meaningful segments. We present task and data abstractions for clickstream data analysis, leading to a high-level model built around an iterative view-refine-record loop with outcomes of conclude with an answer, export segment for further analysis in downstream tools, or abandon the question for a more fruitful analysis path. Segmentifier supports fast and fluid refinement of segments through tightly coupled visual encoding and interaction with a rich set of views that show evocative derived attributes for segments, sequences, and actions in addition to underlying raw sequences. These views support fast and fluid refinement of segments through filtering and partitioning attribute ranges. Interactive visual queries on custom action sequences are aggregated according to a three-level hierarchy. Segmentifier features a detailed glyph-based visual history of the automatically recorded analysis process showing the provenance of each segment as an analysis path of attribute constraints. We demonstrate the effectiveness of our approach through a usage scenario with real-world data and a case study documenting the insights gained by a corporate e-commerce analyst.},
	language = {en},
	number = {3},
	urldate = {2019-12-04},
	journal = {Computer Graphics Forum},
	author = {Dextras‐Romagnino, K. and Munzner, T.},
	year = {2019},
	keywords = {WHY - Evaluation of Tools and Systems, HOW - Pattern Analysis, ?classification, WHY - Prefetching, Type of Work: Empirical Study, WHEN - Retrospective Analyses, Type of Work: Theory \& Model, WHY - User Behavior / User Characteristics / User Modelling},
	pages = {623--634},
	file = {Dextras‐Romagnino and Munzner - 2019 - Segmentifier Interactive Refinement of Clickstrea.pdf:C\:\\Users\\conny\\Zotero\\storage\\XQDF47CG\\Dextras‐Romagnino and Munzner - 2019 - Segmentifier Interactive Refinement of Clickstrea.pdf:application/pdf;Dextras‐Romagnino and Munzner - 2019 - Segmentifier Interactive Refinement of Clickstrea.pdf:C\:\\Users\\conny\\Zotero\\storage\\UWCTLKVY\\Dextras‐Romagnino and Munzner - 2019 - Segmentifier Interactive Refinement of Clickstrea.pdf:application/pdf}
}

@article{gratzl_visual_2016,
	title = {From {Visual} {Exploration} to {Storytelling} and {Back} {Again}},
	volume = {35},
	issn = {01677055},
	url = {http://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=116877233&site=ehost-live&authtype=sso&custid=s5409946},
	doi = {10.1111/cgf.12925},
	abstract = {The primary goal of visual data exploration tools is to enable the discovery of new insights. To justify and reproduce insights, the discovery process needs to be documented and communicated. A common approach to documenting and presenting findings is to capture visualizations as images or videos. Images, however, are insufficient for telling the story of a visual discovery, as they lack full provenance information and context. Videos are difficult to produce and edit, particularly due to the non-linear nature of the exploratory process. Most importantly, however, neither approach provides the opportunity to return to any point in the exploration in order to review the state of the visualization in detail or to conduct additional analyses. In this paper we present CLUE (Capture, Label, Understand, Explain), a model that tightly integrates data exploration and presentation of discoveries. Based on provenance data captured during the exploration process, users can extract key steps, add annotations, and author "Vistories", visual stories based on the history of the exploration. These Vistories can be shared for others to view, but also to retrace and extend the original analysis. We discuss how the CLUE approach can be integrated into visualization tools and provide a prototype implementation. Finally, we demonstrate the general applicability of the model in two usage scenarios: a Gapminder-inspired visualization to explore public health data and an example from molecular biology that illustrates how Vistories could be used in scientific journals.},
	number = {3},
	urldate = {2019-12-04},
	journal = {Computer Graphics Forum},
	author = {Gratzl, S. and Lex, A. and Gehlenborg, N. and Cosgrove, N. and Streit, M.},
	month = jun,
	year = {2016},
	keywords = {WHEN - Retrospective Analyses, HOW - Other, WHY - Report Generation / Storytelling, Type of Work: System},
	pages = {491--500},
	file = {EBSCO Full Text:C\:\\Users\\conny\\Zotero\\storage\\2KCS7FHF\\Gratzl et al. - 2016 - From Visual Exploration to Storytelling and Back A.pdf:application/pdf}
}

@article{stitz_avocado:_2016,
	title = {{AVOCADO}: {Visualization} of {Workflow}-{Derived} {Data} {Provenance} for {Reproducible} {Biomedical} {Research}},
	volume = {35},
	issn = {01677055},
	shorttitle = {{AVOCADO}},
	url = {http://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=116877232&site=ehost-live&authtype=sso&custid=s5409946},
	doi = {10.1111/cgf.12924},
	abstract = {A major challenge in data-driven biomedical research lies in the collection and representation of data provenance information to ensure that findings are reproducibile. In order to communicate and reproduce multi-step analysis workflows executed on datasets that contain data for dozens or hundreds of samples, it is crucial to be able to visualize the provenance graph at different levels of aggregation. Most existing approaches are based on node-link diagrams, which do not scale to the complexity of typical data provenance graphs. In our proposed approach, we reduce the complexity of the graph using hierarchical and motif-based aggregation. Based on user action and graph attributes, a modular degree-of-interest (DoI) function is applied to expand parts of the graph that are relevant to the user. This interest-driven adaptive approach to provenance visualization allows users to review and communicate complex multi-step analyses, which can be based on hundreds of files that are processed by numerous workflows. We have integrated our approach into an analysis platform that captures extensive data provenance information, and demonstrate its effectiveness by means of a biomedical usage scenario.},
	number = {3},
	urldate = {2019-12-04},
	journal = {Computer Graphics Forum},
	author = {Stitz, H. and Luger, S. and Streit, M. and Gehlenborg, N.},
	month = jun,
	year = {2016},
	keywords = {WHY - Evaluation of Tools and Systems, WHY - Adaptive Systems / Guidance, WHEN - Retrospective Analyses, Type of Work: Technique \& Algorithm, HOW - Other, WHY - Real-time or post-hoc Quantification and Re-Application},
	pages = {481--490},
	file = {EBSCO Full Text:C\:\\Users\\conny\\Zotero\\storage\\QBQWLLEG\\Stitz et al. - 2016 - AVOCADO Visualization of Workflow-Derived Data Pr.pdf:application/pdf}
}

@article{hoffswell_visual_2016,
	title = {Visual {Debugging} {Techniques} for {Reactive} {Data} {Visualization}},
	volume = {35},
	issn = {01677055},
	url = {http://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=116877253&site=ehost-live&authtype=sso&custid=s5409946},
	doi = {10.1111/cgf.12903},
	abstract = {Interaction is critical to effective visualization, but can be difficult to author and debug due to dependencies among input events, program state, and visual output. Recent advances leverage reactive semantics to support declarative design and avoid the "spaghetti code" of imperative event handlers. While reactive programming improves many aspects of development, textual specifications still fail to convey the complex runtime dynamics. In response, we contribute a set of visual debugging techniques to reveal the runtime behavior of reactive visualizations. A timeline view records input events and dynamic variable updates, allowing designers to replay and inspect the propagation of values step-by-step. On-demand annotations overlay the output visualization to expose relevant state and scale mappings in-situ. Dynamic tables visualize how backing datasets change over time. To evaluate the effectiveness of these techniques, we study how first-time Vega users debug interactions in faulty, unfamiliar specifications; with no prior knowledge, participants were able to accurately trace errors through the specification.},
	number = {3},
	urldate = {2019-12-04},
	journal = {Computer Graphics Forum},
	author = {Hoffswell, Jane and Satyanarayan, Arvind and Heer, Jeffrey},
	month = jun,
	year = {2016},
	keywords = {WHEN - Retrospective Analysis, Type of Work: User Study, HOW - OTHERS, WHY - Evaluation of Tools and Systems, Type of Work: Model/Framework/Theory, HOW - Pattern Analysis, Type of Work: Technique, NOT SURE},
	pages = {271--280},
	file = {EBSCO Full Text:C\:\\Users\\conny\\Zotero\\storage\\QXQJGA7J\\Hoffswell et al. - 2016 - Visual Debugging Techniques for Reactive Data Visu.pdf:application/pdf}
}

@article{frohler_gemse:_2016,
	title = {{GEMSe}: {Visualization}-{Guided} {Exploration} of {Multi}-channel {Segmentation} {Algorithms}},
	volume = {35},
	issn = {01677055},
	shorttitle = {{GEMSe}},
	url = {http://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=116877247&site=ehost-live&authtype=sso&custid=s5409946},
	doi = {10.1111/cgf.12895},
	abstract = {We present GEMSe, an interactive tool for exploring and analyzing the parameter space of multi-channel segmentation algorithms. Our targeted user group are domain experts who are not necessarily segmentation specialists. GEMSe allows the exploration of the space of possible parameter combinations for a segmentation framework and its ensemble of results. Users start with sampling the parameter space and computing the corresponding segmentations. A hierarchically clustered image tree provides an overview of variations in the resulting space of label images. Details are provided through exemplary images from the selected cluster and histograms visualizing the parameters and the derived output in the selected cluster. The correlation between parameters and derived output as well as the effect of parameter changes can be explored through interactive filtering and scatter plots. We evaluate the usefulness of GEMSe through expert reviews and case studies based on three different kinds of datasets: A synthetic dataset emulating the combination of 3D X-ray computed tomography with data from K-Edge spectroscopy, a three-channel scan of a rock crystal acquired by a Talbot-Lau grating interferometer X-ray computed tomography device, as well as a hyperspectral image.},
	number = {3},
	urldate = {2019-12-04},
	journal = {Computer Graphics Forum},
	author = {Fröhler, B. and Möller, T. and Heinzl, C.},
	month = jun,
	year = {2016},
	keywords = {WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, WHY - Adaptive Systems / Guidance, Type of Work: Empirical Study, HOW - Classification Models, Type of Work: System},
	pages = {191--200},
	file = {EBSCO Full Text:C\:\\Users\\conny\\Zotero\\storage\\GTSESQE4\\Fröhler et al. - 2016 - GEMSe Visualization-Guided Exploration of Multi-c.pdf:application/pdf}
}

@article{loorak_changecatcher:_2018,
	title = {{ChangeCatcher}: {Increasing} {Inter}‐author {Awareness} for {Visualization} {Development}},
	volume = {37},
	issn = {01677055},
	shorttitle = {{ChangeCatcher}},
	url = {http://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=130628132&site=ehost-live&authtype=sso&custid=s5409946},
	doi = {10.1111/cgf.13400},
	abstract = {Abstract: We introduce an approach for explicitly revealing changes between versions of a visualization workbook to support version comparison tasks. Visualization authors may need to understand version changes for a variety of reasons, analogous to document editing. An author who has been away for a while may need to catch up on the changes made by their co‐author, or a person responsible for formatting compliance may need to check formatting changes that occurred since the last time they reviewed the work. We introduce ChangeCatcher, a prototype tool to help people find and understand changes in a visualization workbook, specifically, a Tableau workbook. Our design is based on interviews we conducted with experts to investigate user needs and practices around version comparison. ChangeCatcher provides an overview of changes across six categories, and employs a multi‐level details‐on‐demand approach to progressively reveal details. Our qualitative study showed that ChangeCatcher's methods for explicitly revealing and categorizing version changes were helpful in version comparison tasks.},
	number = {3},
	urldate = {2019-12-04},
	journal = {Computer Graphics Forum},
	author = {Loorak, M. and Tory, M. and Carpendale, S.},
	month = jun,
	year = {2018},
	keywords = {collaboration, WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, WHY - Real-time or post-hoc quantification and re-application, Type of Work: Empirical Study, HOW - Classification Models, WHEN - Retrospective Analyses, WHY - User Behavior / User Characteristics / User Modelling},
	pages = {51--62},
	file = {EBSCO Full Text:C\:\\Users\\conny\\Zotero\\storage\\XDPXEXEW\\Loorak et al. - 2018 - ChangeCatcher Increasing Inter‐author Awareness f.pdf:application/pdf}
}

@article{xu_chart_2018,
	title = {Chart {Constellations}: {Effective} {Chart} {Summarization} for {Collaborative} and {Multi}‐{User} {Analyses}},
	volume = {37},
	issn = {01677055},
	shorttitle = {Chart {Constellations}},
	url = {http://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=130628134&site=ehost-live&authtype=sso&custid=s5409946},
	doi = {10.1111/cgf.13402},
	abstract = {Abstract: Many data problems in the real world are complex and require multiple analysts working together to uncover embedded insights by creating chart‐driven data stories. How, as a subsequent analysis step, do we interpret and learn from these collections of charts? We present Chart Constellations, a system to interactively support a single analyst in the review and analysis of data stories created by other collaborative analysts. Instead of iterating through the individual charts for each data story, the analyst can project, cluster, filter, and connect results from all users in a meta‐visualization approach. Constellations supports deriving summary insights about prior investigations and supports the exploration of new, unexplored regions in the dataset. To evaluate our system, we conduct a user study comparing it against data science notebooks. Results suggest that Constellations promotes the discovery of both broad and high‐level insights, including theme and trend analysis, subjective evaluation, and hypothesis generation.},
	number = {3},
	urldate = {2019-12-04},
	journal = {Computer Graphics Forum},
	author = {Xu, Shenyu and Bryan, Chris and Li, Jianping Kelvin and Zhao, Jian and Ma, Kwan‐Liu},
	month = jun,
	year = {2018},
	keywords = {WHY - Evaluation of Tools and Systems, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, HOW - Classification Models, WHEN - Retrospective Analyses, WHY - Report Generation / Storytelling, WHY - Real-time or post-hoc Quantification and Re-Application},
	pages = {75--86},
	file = {EBSCO Full Text:C\:\\Users\\conny\\Zotero\\storage\\W6JDLIY6\\Xu et al. - 2018 - Chart Constellations Effective Chart Summarizatio.pdf:application/pdf}
}

@inproceedings{kittley-davies_evaluating_2019,
	address = {Glasgow, Scotland Uk},
	title = {Evaluating the {Effect} of {Feedback} from {Different} {Computer} {Vision} {Processing} {Stages}: {A} {Comparative} {Lab} {Study}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {Evaluating the {Effect} of {Feedback} from {Different} {Computer} {Vision} {Processing} {Stages}},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300273},
	doi = {10.1145/3290605.3300273},
	abstract = {Computer vision and pattern recognition are increasingly being employed by smartphone and tablet applications targeted at lay-users. An open design challenge is to make such systems intelligible without requiring users to become technical experts. This paper reports a lab study examining the role of visual feedback. Our findings indicate that the stage of processing from which feedback is derived plays an important role in users’ ability to develop coherent and correct understandings of a system’s operation. Participants in our study showed a tendency to misunderstand the meaning being conveyed by the feedback, relating it to processing outcomes and higher level concepts, when in reality the feedback represented low level features. Drawing on the experimental results and the qualitative data collected, we discuss the challenges of designing interactions around pattern matching algorithms.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Kittley-Davies, Jacob and Alqaraawi, Ahmed and Yang, Rayoung and Costanza, Enrico and Rogers, Alex and Stein, Sebastian},
	year = {2019},
	keywords = {Visual Feedback, Qualitative, Machine learning algorithm},
	pages = {1--12},
	file = {Kittley-Davies et al. - 2019 - Evaluating the Effect of Feedback from Different C.pdf:C\:\\Users\\conny\\Zotero\\storage\\8JU5KVL9\\Kittley-Davies et al. - 2019 - Evaluating the Effect of Feedback from Different C.pdf:application/pdf}
}

@inproceedings{kong_understanding_2019,
	address = {Glasgow, Scotland Uk},
	title = {Understanding {Visual} {Cues} in {Visualizations} {Accompanied} by {Audio} {Narrations}},
	isbn = {978-1-4503-5970-2},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300280},
	doi = {10.1145/3290605.3300280},
	abstract = {It is often assumed that visual cues, which highlight specific parts of a visualization to guide the audience’s attention, facilitate visualization storytelling and presentation. This assumption has not been systematically studied. We present an in-lab experiment and a Mechanical Turk study to examine the effects of integral and separable visual cues on the recall and comprehension of visualizations that are accompanied by audio narration. Eye-tracking data in the in-lab experiment confirm that cues helped the viewers focus on relevant parts of the visualization faster. We found that in general, visual cues did not have a significant effect on learning outcomes, but for specific cue techniques (e.g. glow) or specific chart types (e.g heatmap), cues significantly improved comprehension. Based on these results, we discuss how presenters might select visual cues depending on the role of the cues and the visualization type.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Kong, Ha-Kyung and Zhu, Wenjie and Liu, Zhicheng and Karahalios, Karrie},
	year = {2019},
	keywords = {Storytelling, Visual Cues, Comprehension of Visualization, Eye-Tracking, Recall},
	pages = {1--13},
	file = {Kong et al. - 2019 - Understanding Visual Cues in Visualizations Accomp.pdf:C\:\\Users\\conny\\Zotero\\storage\\8CQE5RUX\\Kong et al. - 2019 - Understanding Visual Cues in Visualizations Accomp.pdf:application/pdf}
}

@inproceedings{piazentin_ono_historytracker:_2019,
	address = {Glasgow, Scotland Uk},
	title = {{HistoryTracker}: {Minimizing} {Human} {Interactions} in {Baseball} {Game} {Annotation}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {{HistoryTracker}},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300293},
	doi = {10.1145/3290605.3300293},
	abstract = {The sport data tracking systems available today are based on specialized hardware (high-definition cameras, speed radars, RFID) to detect and track targets on the field. While effective, implementing and maintaining these systems pose a number of challenges, including high cost and need for close human monitoring. On the other hand, the sports analytics community has been exploring human computation and crowdsourcing in order to produce tracking data that is trustworthy, cheaper and more accessible. However, state-of-theart methods require a large number of users to perform the annotation, or put too much burden into a single user. We propose HistoryTracker, a methodology that facilitates the creation of tracking data for baseball games by warm-starting the annotation process using a vast collection of historical data. We show that HistoryTracker helps users to produce tracking data in a fast and reliable way.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Piazentin Ono, Jorge and Gjoka, Arvi and Salamon, Justin and Dietrich, Carlos and Silva, Claudio T.},
	year = {2019},
	keywords = {Sport Data Tracking, Annotation},
	pages = {1--12},
	file = {Piazentin Ono et al. - 2019 - HistoryTracker Minimizing Human Interactions in B.pdf:C\:\\Users\\conny\\Zotero\\storage\\YMVIGGI5\\Piazentin Ono et al. - 2019 - HistoryTracker Minimizing Human Interactions in B.pdf:application/pdf}
}

@inproceedings{choi_concept-driven_2019,
	address = {Glasgow, Scotland Uk},
	title = {Concept-{Driven} {Visual} {Analytics}: an {Exploratory} {Study} of {Model}- and {Hypothesis}-{Based} {Reasoning} with {Visualizations}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {Concept-{Driven} {Visual} {Analytics}},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300298},
	doi = {10.1145/3290605.3300298},
	abstract = {Visualization tools facilitate exploratory data analysis, but fall short at supporting hypothesis-based reasoning. We conducted an exploratory study to investigate how visualizations might support a concept-driven analysis style, where users can optionally share their hypotheses and conceptual models in natural language, and receive customized plots depicting the fit of their models to the data. We report on how participants leveraged these unique affordances for visual analysis. We found that a majority of participants articulated meaningful models and predictions, utilizing them as entry points to sensemaking. We contribute an abstract typology representing the types of models participants held and externalized as data expectations. Our findings suggest ways for rearchitecting visual analytics tools to better support hypothesis- and model-based reasoning, in addition to their traditional role in exploratory analysis. We discuss the design implications and reflect on the potential benefits and challenges involved.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Choi, In Kwon and Childers, Taylor and Raveendranath, Nirmal Kumar and Mishra, Swati and Harris, Kyle and Reda, Khairi},
	year = {2019},
	keywords = {Visual Analytics, Mental model, Sensemaking, Model-based reasoning, hypothesis-based reasoning, Exploratory analysis},
	pages = {1--14},
	file = {Choi et al. - 2019 - Concept-Driven Visual Analytics an Exploratory St.pdf:C\:\\Users\\conny\\Zotero\\storage\\9QHSTERN\\Choi et al. - 2019 - Concept-Driven Visual Analytics an Exploratory St.pdf:application/pdf}
}

@inproceedings{kim_dataselfie:_2019,
	address = {Glasgow, Scotland Uk},
	title = {{DataSelfie}: {Empowering} {People} to {Design} {Personalized} {Visuals} to {Represent} {Their} {Data}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {{DataSelfie}},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300309},
	doi = {10.1145/3290605.3300309},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Kim, Nam Wook and Im, Hyejin and Henry Riche, Nathalie and Wang, Alicia and Gajos, Krzysztof and Pfister, Hanspeter},
	year = {2019},
	keywords = {Visual Vocabulary, Personal Visualization, Tracking},
	pages = {1--12},
	file = {Kim et al. - 2019 - DataSelfie Empowering People to Design Personaliz.pdf:C\:\\Users\\conny\\Zotero\\storage\\59C6JADG\\Kim et al. - 2019 - DataSelfie Empowering People to Design Personaliz.pdf:application/pdf}
}

@inproceedings{kery_towards_2019,
	address = {Glasgow, Scotland Uk},
	title = {Towards {Effective} {Foraging} by {Data} {Scientists} to {Find} {Past} {Analysis} {Choices}},
	isbn = {978-1-4503-5970-2},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300322},
	doi = {10.1145/3290605.3300322},
	abstract = {Data scientists are responsible for the analysis decisions they make, but it is hard for them to track the process by which they achieved a result. Even when data scientists keep logs, it is onerous to make sense of the resulting large number of history records full of overlapping variants of code, output, plots, etc. We developed algorithmic and visualization techniques for notebook code environments to help data scientists forage for information in their history. To test these interventions, we conducted a think-aloud evaluation with 15 data scientists, where participants were asked to nd speci c information from the history of another person’s data science project. e participants succeed on a median of 80\% of the tasks they performed. e quantitative results suggest promising aspects of our design, while qualitative results motivated a number of design improvements. e resulting system, called Verdant, is released as an open-source extension for JupyterLab.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Kery, Mary Beth and John, Bonnie E. and O'Flaherty, Patrick and Horvath, Amber and Myers, Brad A.},
	year = {2019},
	pages = {1--13},
	annote = {Excluded because it lacks an analytical part
.},
	file = {Kery et al. - 2019 - Towards Effective Foraging by Data Scientists to F.pdf:C\:\\Users\\conny\\Zotero\\storage\\ED52JBCY\\Kery et al. - 2019 - Towards Effective Foraging by Data Scientists to F.pdf:application/pdf}
}

@inproceedings{kim_datatoon:_2019,
	address = {Glasgow, Scotland Uk},
	title = {{DataToon}: {Drawing} {Dynamic} {Network} {Comics} {With} {Pen} + {Touch} {Interaction}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {{DataToon}},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300335},
	doi = {10.1145/3290605.3300335},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Kim, Nam Wook and Pfister, Hanspeter and Henry Riche, Nathalie and Bach, Benjamin and Xu, Guanpeng and Brehmer, Matthew and Hinckley, Ken and Pahud, Michel and Xia, Haijun and McGuffin, Michael J.},
	year = {2019},
	keywords = {Data visualization, Storytelling, Annotation, Tracking},
	pages = {1--12},
	file = {Kim et al. - 2019 - DataToon Drawing Dynamic Network Comics With Pen .pdf:C\:\\Users\\conny\\Zotero\\storage\\W7AH7BAM\\Kim et al. - 2019 - DataToon Drawing Dynamic Network Comics With Pen .pdf:application/pdf}
}

@inproceedings{hu_vizml:_2019,
	address = {Glasgow, Scotland Uk},
	title = {{VizML}: {A} {Machine} {Learning} {Approach} to {Visualization} {Recommendation}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {{VizML}},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300358},
	doi = {10.1145/3290605.3300358},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Hu, Kevin and Bakker, Michiel A. and Li, Stephen and Kraska, Tim and Hidalgo, César},
	year = {2019},
	keywords = {WHEN - Real-Time Applications, HOW - Classification, WHY - Adaptive Systems / Guidance, Type of Work: Technique \& Algorithm, ENCODING - ModelENCODING - Model, HOW - Classification, Type of Work: Technique \& Algorithm, WHEN - Real-Time Applications, WHY - Adaptive Systems / Guidance},
	pages = {1--12},
	file = {Hu et al. - 2019 - VizML A Machine Learning Approach to Visualizatio.pdf:C\:\\Users\\conny\\Zotero\\storage\\48N4JL3S\\Hu et al. - 2019 - VizML A Machine Learning Approach to Visualizatio.pdf:application/pdf}
}

@inproceedings{wallner_aggregated_2019,
	address = {Glasgow, Scotland Uk},
	title = {Aggregated {Visualization} of {Playtesting} {Data}},
	isbn = {978-1-4503-5970-2},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300593},
	doi = {10.1145/3290605.3300593},
	abstract = {Playtesting is a key component in the game development process aimed at improving the quality of games through the collection of gameplay data and identification of design issues. Visualization techniques are currently being employed to help integrate quantitative and qualitative data. Despite that, two existing challenges are to determine the level of detail to be presented to developers based on their needs and to effectively communicate the collected data so that informed design changes can be reached. In this paper, we first propose an aggregated visualization technique that makes use of clustering, territory tessellation, and trajectory aggregation to simultaneously display mixed playtesting data. Secondly, to assess the usefulness of our technique we evaluate it through interviews with professional game developers and compare it to a non-aggregated visualization. The results of this study also provide an important contribution towards identifying areas of improvement in the portrayal of gameplay data.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Wallner, Günter and Halabi, Nour and Mirza-Babaei, Pejman},
	year = {2019},
	keywords = {Visualizing playtesting data, Abstraction Level, Analyze game performance, Interview},
	pages = {1--12},
	file = {Wallner et al. - 2019 - Aggregated Visualization of Playtesting Data.pdf:C\:\\Users\\conny\\Zotero\\storage\\JAADV7CZ\\Wallner et al. - 2019 - Aggregated Visualization of Playtesting Data.pdf:application/pdf}
}

@inproceedings{guo_visualizing_2019,
	address = {Glasgow, Scotland Uk},
	title = {Visualizing {Uncertainty} and {Alternatives} in {Event} {Sequence} {Predictions}},
	isbn = {978-1-4503-5970-2},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300803},
	doi = {10.1145/3290605.3300803},
	abstract = {Data analysts apply machine learning and statistical methods to timestamped event sequences to tackle various problems but face unique challenges when interpreting the results. Especially in event sequence prediction, it is difficult to convey uncertainty and possible alternative paths or outcomes. In this work, informed by interviews with five machine learning practitioners, we iteratively designed a novel visualization for exploring event sequence predictions of multiple records where users are able to review the most probable predictions and possible alternatives alongside uncertainty information. Through a controlled study with 18 participants, we found that users are more confident in making decisions when alternative predictions are displayed and they consider the alternatives more when deciding between two options with similar top predictions.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Guo, Shunan and Du, Fan and Malik, Sana and Koh, Eunyee and Kim, Sungchul and Liu, Zhicheng and Kim, Donghyun and Zha, Hongyuan and Cao, Nan},
	year = {2019},
	keywords = {WHEN - Real-Time Applications, WHY - Adaptive Systems / GuidaENCODING - Sequence, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, Type of Work: Technique \& Algorithm, ENCODING - SequeWHEN - Real-Time Applications, WHY - Adaptive Systems / Guidance},
	pages = {1--12},
	file = {Guo et al. - 2019 - Visualizing Uncertainty and Alternatives in Event .pdf:C\:\\Users\\conny\\Zotero\\storage\\KWMW3A5H\\Guo et al. - 2019 - Visualizing Uncertainty and Alternatives in Event .pdf:application/pdf}
}

@inproceedings{hohman_gamut:_2019,
	address = {Glasgow, Scotland Uk},
	title = {Gamut: {A} {Design} {Probe} to {Understand} {How} {Data} {Scientists} {Understand} {Machine} {Learning} {Models}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {Gamut},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300809},
	doi = {10.1145/3290605.3300809},
	abstract = {Without good models and the right tools to interpret them, data scientists risk making decisions based on hidden biases, spurious correlations, and false generalizations. This has led to a rallying cry for model interpretability. Yet the concept of interpretability remains nebulous, such that researchers and tool designers lack actionable guidelines for how to incorporate interpretability into models and accompanying tools. Through an iterative design process with expert machine learning researchers and practitioners, we designed a visual analytics system, Gamut, to explore how interactive interfaces could better support model interpretation. Using Gamut as a probe, we investigated why and how professional data scientists interpret models, and how interface affordances can support data scientists in answering questions about model interpretability. Our investigation showed that interpretability is not a monolithic concept: data scientists have different reasons to interpret models and tailor explanations for specific audiences, often balancing competing concerns of simplicity and completeness. Participants also asked to use Gamut in their work, highlighting its potential to help data scientists understand their own data.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Hohman, Fred and Head, Andrew and Caruana, Rich and DeLine, Robert and Drucker, Steven M.},
	year = {2019},
	keywords = {visual analytics, Predicitons, Machine learning interpretability, interactive interface},
	pages = {1--13},
	file = {Hohman et al. - 2019 - Gamut A Design Probe to Understand How Data Scien.pdf:C\:\\Users\\conny\\Zotero\\storage\\925JDEJE\\Hohman et al. - 2019 - Gamut A Design Probe to Understand How Data Scien.pdf:application/pdf}
}

@inproceedings{horak_vistribute:_2019,
	address = {Glasgow, Scotland Uk},
	title = {Vistribute: {Distributing} {Interactive} {Visualizations} in {Dynamic} {Multi}-{Device} {Setups}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {Vistribute},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300846},
	doi = {10.1145/3290605.3300846},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Horak, Tom and Mathisen, Andreas and Klokmose, Clemens N. and Dachselt, Raimund and Elmqvist, Niklas},
	year = {2019},
	keywords = {interactive visualization, Logs, distribution},
	pages = {1--13},
	file = {Horak et al. - 2019 - Vistribute Distributing Interactive Visualization.pdf:C\:\\Users\\conny\\Zotero\\storage\\HV2DKN5N\\Horak et al. - 2019 - Vistribute Distributing Interactive Visualization.pdf:application/pdf}
}

@inproceedings{saquib_interactive_2019,
	address = {Glasgow, Scotland Uk},
	title = {Interactive {Body}-{Driven} {Graphics} for {Augmented} {Video} {Performance}},
	isbn = {978-1-4503-5970-2},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300852},
	doi = {10.1145/3290605.3300852},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Saquib, Nazmus and Kazi, Rubaiat Habib and Wei, Li-Yi and Li, Wilmot},
	year = {2019},
	keywords = {presentation, storytelling, Not so relevant, augmented interactive visualization},
	pages = {1--12},
	file = {Saquib et al. - 2019 - Interactive Body-Driven Graphics for Augmented Vid.pdf:C\:\\Users\\conny\\Zotero\\storage\\S5VVMISN\\Saquib et al. - 2019 - Interactive Body-Driven Graphics for Augmented Vid.pdf:application/pdf}
}

@inproceedings{wang_atmseer:_2019,
	address = {Glasgow, Scotland Uk},
	title = {{ATMSeer}: {Increasing} {Transparency} and {Controllability} in {Automated} {Machine} {Learning}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {{ATMSeer}},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300911},
	doi = {10.1145/3290605.3300911},
	abstract = {To relieve the pain of manually selecting machine learning algorithms and tuning hyperparameters, automated machine learning (AutoML) methods have been developed to automatically search for good models. Due to the huge model search space, it is impossible to try all models. Users tend to distrust automatic results and increase the search budget as much as they can, thereby undermining the efficiency of AutoML. To address these issues, we design and implement ATMSeer, an interactive visualization tool that supports users in refining the search space of AutoML and analyzing the results. To guide the design of ATMSeer, we derive a workflow of using AutoML based on interviews with machine learning experts. A multi-granularity visualization is proposed to enable users to monitor the AutoML process, analyze the searched models, and refine the search space in real time. We demonstrate the utility and usability of ATMSeer through two case studies, expert interviews, and a user study with 13 end users.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Wang, Qianwen and Ming, Yao and Jin, Zhihua and Shen, Qiaomu and Liu, Dongyu and Smith, Micah J. and Veeramachaneni, Kalyan and Qu, Huamin},
	year = {2019},
	keywords = {Interview, Automated Machine Learning, Use Case},
	pages = {1--12},
	file = {Wang et al. - 2019 - ATMSeer Increasing Transparency and Controllabili.pdf:C\:\\Users\\conny\\Zotero\\storage\\A739YR67\\Wang et al. - 2019 - ATMSeer Increasing Transparency and Controllabili.pdf:application/pdf}
}

@inproceedings{kim_bayesian_2019,
	address = {Glasgow, Scotland Uk},
	title = {A {Bayesian} {Cognition} {Approach} to {Improve} {Data} {Visualization}},
	isbn = {978-1-4503-5970-2},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300912},
	doi = {10.1145/3290605.3300912},
	abstract = {People naturally bring their prior beliefs to bear on how they interpret the new information, yet few formal models exist for accounting for the influence of users’ prior beliefs in interactions with data presentations like visualizations. We demonstrate a Bayesian cognitive model for understanding how people interpret visualizations in light of prior beliefs and show how this model provides a guide for improving visualization evaluation. In a first study, we show how applying a Bayesian cognition model to a simple visualization scenario indicates that people’s judgments are consistent with a hypothesis that they are doing approximate Bayesian inference. In a second study, we evaluate how sensitive our observations of Bayesian behavior are to different techniques for eliciting people subjective distributions, and to different datasets. We find that people don’t behave consistently with Bayesian predictions for large sample size datasets, and this difference cannot be explained by elicitation technique. In a final study, we show how normative Bayesian inference can be used as an evaluation framework for visualizations, including of uncertainty.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Kim, Yea-Seul and Walls, Logan A. and Krafft, Peter and Hullman, Jessica},
	year = {2019},
	keywords = {Improve data visualizaton, Bayesian Cognition},
	pages = {1--14},
	file = {Kim et al. - 2019 - A Bayesian Cognition Approach to Improve Data Visu.pdf:C\:\\Users\\conny\\Zotero\\storage\\SWDKTHJQ\\Kim et al. - 2019 - A Bayesian Cognition Approach to Improve Data Visu.pdf:application/pdf}
}

@inproceedings{li_predicting_2018,
	address = {Montreal QC, Canada},
	title = {Predicting {Human} {Performance} in {Vertical} {Menu} {Selection} {Using} {Deep} {Learning}},
	isbn = {978-1-4503-5620-6},
	url = {http://dl.acm.org/citation.cfm?doid=3173574.3173603},
	doi = {10.1145/3173574.3173603},
	abstract = {Predicting human performance in interaction tasks allows designers or developers to understand the expected performance of a target interface without actually testing it with real users. In this work, we present a deep neural net to model and predict human performance in performing a sequence of UI tasks. In particular, we focus on a dominant class of tasks, i.e., target selection from a vertical list or menu. We experimented with our deep neural net using a public dataset collected from a desktop laboratory environment and a dataset collected from hundreds of touchscreen smartphone users via crowdsourcing. Our model significantly outperformed previous methods on these datasets. Importantly, our method, as a deep model, can easily incorporate additional UI attributes such as visual appearance and content semantics without changing model architectures. By understanding about how a deep learning model learns from human behaviors, our approach can be seen as a vehicle to discover new patterns about human behaviors to advance analytical modeling.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '18},
	publisher = {ACM Press},
	author = {Li, Yang and Bengio, Samy and Bailly, Gilles},
	year = {2018},
	keywords = {Deep learning, Modeling user Performance, NOT INTERACTIVE VISUALIZATION},
	pages = {1--7},
	file = {Li et al. - 2018 - Predicting Human Performance in Vertical Menu Sele.pdf:C\:\\Users\\conny\\Zotero\\storage\\L83YS79E\\Li et al. - 2018 - Predicting Human Performance in Vertical Menu Sele.pdf:application/pdf}
}

@techreport{bruhlmann_measuring_2018,
	type = {preprint},
	title = {Measuring the “{Why}” of {Interaction}: {Development} and {Validation} of the {User} {Motivation} {Inventory} ({UMI})},
	shorttitle = {Measuring the “{Why}” of {Interaction}},
	url = {https://osf.io/mkw57},
	abstract = {Motivation is a fundamental concept in understanding people’s experiences and behavior. Yet, motivation to engage with an interactive system has received only limited attention in HCI. We report the development and validation of the User Motivation Inventory (UMI). The UMI is an 18-item multidimensional measure of motivation, rooted in self-determination theory (SDT). It is designed to measure intrinsic motivation, integrated, identiﬁed, introjected, and external regulation, as well as amotivation. Results of two studies (total N = 941) conﬁrm the six-factor structure of the UMI with high reliability, as well as convergent and discriminant validity of each subscale. Relationships with core concepts such as need satisfaction, vitality, and usability were studied. Additionally, the UMI was found to detect differences in motivation for people who consider abandoning a technology compared to those who do not question their use. The central role of motivation in users’ behavior and experience is discussed.},
	language = {en},
	urldate = {2019-12-09},
	institution = {PsyArXiv},
	author = {Brühlmann, Florian and Vollenwyder, Beat and Opwis, Klaus and Mekler, Elisa D},
	month = jan,
	year = {2018},
	doi = {10.31234/osf.io/mkw57},
	keywords = {Little bit off-topic, Measure the "Why" of Interaction, user Motivation, Why},
	file = {Brühlmann et al. - 2018 - Measuring the “Why” of Interaction Development an.pdf:C\:\\Users\\conny\\Zotero\\storage\\HT8G2FA6\\Brühlmann et al. - 2018 - Measuring the “Why” of Interaction Development an.pdf:application/pdf}
}

@inproceedings{feng_effects_2018,
	address = {Montreal QC, Canada},
	title = {The {Effects} of {Adding} {Search} {Functionality} to {Interactive} {Visualizations} on the {Web}},
	isbn = {978-1-4503-5620-6},
	url = {http://dl.acm.org/citation.cfm?doid=3173574.3173711},
	doi = {10.1145/3173574.3173711},
	abstract = {The widespread use of text-based search in user interfaces has led designers in visualization to occasionally add search functionality to their creations. Yet it remains unclear how search may impact a person’s behavior. Given the unstructured context of the web, users may not have explicit informationseeking goals and designers cannot make assumptions about user attention. To bridge this gap, we observed the impact of integrating search with ﬁve visualizations across 830 online participants. In an unguided task, we ﬁnd that (1) the presence of text-based search inﬂuences people’s information-seeking goals, (2) search can alter the data that people explore and how they engage with it, and (3) the effects of search are ampliﬁed in visualizations where people are familiar with the underlying dataset. These results suggest that text-search in web visualizations drives users towards more diverse information seeking goals, and may be valuable in a range of existing visualization designs.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '18},
	publisher = {ACM Press},
	author = {Feng, Mi and Deng, Cheng and Peck, Evan M. and Harrison, Lane},
	year = {2018},
	keywords = {How participants engage, information sseking, text-based visualization},
	pages = {1--13},
	file = {Feng et al. - 2018 - The Effects of Adding Search Functionality to Inte.pdf:C\:\\Users\\conny\\Zotero\\storage\\GEC8F8SI\\Feng et al. - 2018 - The Effects of Adding Search Functionality to Inte.pdf:application/pdf}
}

@inproceedings{sultanum_more_2018,
	address = {Montreal QC, Canada},
	title = {More {Text} {Please}! {Understanding} and {Supporting} the {Use} of {Visualization} for {Clinical} {Text} {Overview}},
	isbn = {978-1-4503-5620-6},
	url = {http://dl.acm.org/citation.cfm?doid=3173574.3173996},
	doi = {10.1145/3173574.3173996},
	abstract = {Clinical practice is heavily reliant on the use of unstructured text to document patient stories due to its expressive and ﬂexible nature. However, a physician’s capacity to recover information from text for clinical overview is severely affected when records get longer and time pressure increases. Data visualization strategies have been explored to aid in information retrieval by replacing text with graphical summaries, though often at the cost of omitting important text features. This causes physician mistrust and limits real-world adoption. This work presents our investigation into the role and use of text in clinical practice, and reports on efforts to assess the best of both worlds—text and visualization—to facilitate clinical overview. We report on insights garnered from a ﬁeld study, and the lessons learned from an iterative design process and evaluation of a text-visualization prototype, MedStory, with 14 medical professionals. The results led to a number of grounded design recommendations to guide visualization design to support clinical text overview.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '18},
	publisher = {ACM Press},
	author = {Sultanum, Nicole and Brudno, Michael and Wigdor, Daniel and Chevalier, Fanny},
	year = {2018},
	keywords = {iterative design approach, clinical text},
	pages = {1--13},
	file = {Sultanum et al. - 2018 - More Text Please! Understanding and Supporting the.pdf:C\:\\Users\\conny\\Zotero\\storage\\62RFQXPS\\Sultanum et al. - 2018 - More Text Please! Understanding and Supporting the.pdf:application/pdf}
}

@inproceedings{kong_frames_2018,
	address = {Montreal QC, Canada},
	title = {Frames and {Slants} in {Titles} of {Visualizations} on {Controversial} {Topics}},
	isbn = {978-1-4503-5620-6},
	url = {http://dl.acm.org/citation.cfm?doid=3173574.3174012},
	doi = {10.1145/3173574.3174012},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '18},
	publisher = {ACM Press},
	author = {Kong, Ha-Kyung and Liu, Zhicheng and Karahalios, Karrie},
	year = {2018},
	keywords = {recall, perception of bias, attrtude change, frames for visualization title},
	pages = {1--12},
	file = {Kong et al. - 2018 - Frames and Slants in Titles of Visualizations on C.pdf:C\:\\Users\\conny\\Zotero\\storage\\XA5PAAEJ\\Kong et al. - 2018 - Frames and Slants in Titles of Visualizations on C.pdf:application/pdf}
}

@inproceedings{chang_recipescape:_2018,
	address = {Montreal QC, Canada},
	title = {{RecipeScape}: {An} {Interactive} {Tool} for {Analyzing} {Cooking} {Instructions} at {Scale}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {{RecipeScape}},
	url = {http://dl.acm.org/citation.cfm?doid=3173574.3174025},
	doi = {10.1145/3173574.3174025},
	abstract = {For cooking professionals and culinary students, understanding cooking instructions is an essential yet demanding task. Common tasks include categorizing different approaches to cooking a dish and identifying usage patterns of particular ingredients or cooking methods, all of which require extensive browsing and comparison of multiple recipes. However, no existing system provides support for such in-depth and atscale analysis. We present RecipeScape, an interactive system for browsing and analyzing the hundreds of recipes of a single dish available online. We also introduce a computational pipeline that extracts cooking processes from recipe text and calculates a procedural similarity between them. To evaluate how RecipeScape supports culinary analysis at scale, we conducted a user study with cooking professionals and culinary students with 500 recipes for two different dishes. Results show that RecipeScape clusters recipes into distinct approaches, and captures notable usage patterns of ingredients and cooking actions.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '18},
	publisher = {ACM Press},
	author = {Chang, Minsuk and Guillain, Leonore V. and Jung, Hyeungshik and Hare, Vivian M. and Kim, Juho and Agrawala, Maneesh},
	year = {2018},
	keywords = {user study, clustering distinct approaches, capture usage patterns, ingredients and cooking actions},
	pages = {1--12},
	file = {Chang et al. - 2018 - RecipeScape An Interactive Tool for Analyzing Coo.pdf:C\:\\Users\\conny\\Zotero\\storage\\WC4ZIXHE\\Chang et al. - 2018 - RecipeScape An Interactive Tool for Analyzing Coo.pdf:application/pdf}
}

@inproceedings{battle_beagle:_2018,
	address = {Montreal QC, Canada},
	title = {Beagle: {Automated} {Extraction} and {Interpretation} of {Visualizations} from the {Web}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {Beagle},
	url = {http://dl.acm.org/citation.cfm?doid=3173574.3174168},
	doi = {10.1145/3173574.3174168},
	abstract = {How common is interactive visualization on the web?” “What is the most popular visualization design?” “How prevalent are pie charts really?” These questions intimate the role of interactive visualization in the real (online) world. In this paper, we present our approach (and ﬁndings) to answering these questions. First, we introduce Beagle, which mines the web for SVG-based visualizations and automatically classiﬁes them by type (i.e., bar, pie, etc.). With Beagle, we extract over 41,000 visualizations across ﬁve different tools and repositories, and classify them with 85\% accuracy, across 24 visualization types. Given this visualization collection, we study usage across tools. We ﬁnd that most visualizations fall under four types: bar charts, line charts, scatter charts, and geographic maps. Though controversial, pie charts are relatively rare for the visualization tools that were studied. Our ﬁndings also suggest that the total visualization types supported by a given tool could factor into its ease of use. However this effect appears to be mitigated by providing a variety of diverse expert visualization examples to users.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '18},
	publisher = {ACM Press},
	author = {Battle, Leilani and Duan, Peitong and Miranda, Zachery and Mukusheva, Dana and Chang, Remco and Stonebraker, Michael},
	year = {2018},
	keywords = {Classifying visualizations types, web mining},
	pages = {1--8},
	file = {Battle et al. - 2018 - Beagle Automated Extraction and Interpretation of.pdf:C\:\\Users\\conny\\Zotero\\storage\\3GFSAFQU\\Battle et al. - 2018 - Beagle Automated Extraction and Interpretation of.pdf:application/pdf}
}

@inproceedings{mendez_considering_2018,
	address = {Montreal QC, Canada},
	title = {Considering {Agency} and {Data} {Granularity} in the {Design} of {Visualization} {Tools}},
	isbn = {978-1-4503-5620-6},
	url = {http://dl.acm.org/citation.cfm?doid=3173574.3174212},
	doi = {10.1145/3173574.3174212},
	abstract = {Previous research has identiﬁed trade-offs when it comes to designing visualization tools. While constructive “bottomup” tools promote a hands-on, user-driven design process that enables a deep understanding and control of the visual mapping, automated tools are more efﬁcient and allow people to rapidly explore complex alternative designs, often at the cost of transparency. We investigate how to design visualization tools that support a user-driven, transparent design process while enabling efﬁciency and automation, through a series of design workshops that looked at how both visualization experts and novices approach this problem. Participants produced a variety of solutions that range from example-based approaches expanding constructive visualization to solutions in which the visualization tool infers solutions on behalf of the designer, e.g., based on data attributes. On a higher level, these ﬁndings highlight agency and granularity as dimensions that can guide the design of visualization tools in this space.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '18},
	publisher = {ACM Press},
	author = {Méndez, Gonzalo Gabriel and Nacenta, Miguel A. and Hinrichs, Uta},
	year = {2018},
	keywords = {design workshops, bottom-up visualization tools, Active involemenet in the visualization process, automated process},
	pages = {1--14},
	file = {Méndez et al. - 2018 - Considering Agency and Data Granularity in the Des.pdf:C\:\\Users\\conny\\Zotero\\storage\\G7XVW545\\Méndez et al. - 2018 - Considering Agency and Data Granularity in the Des.pdf:application/pdf}
}

@inproceedings{wood_protection_2018,
	address = {Montreal QC, Canada},
	title = {"{Protection} on that {Erection}?": {Discourses} of {Accountability} \& {Compromising} {Participation} in {Digital} {Sexual} {Health}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {"{Protection} on that {Erection}?},
	url = {http://dl.acm.org/citation.cfm?doid=3173574.3174238},
	doi = {10.1145/3173574.3174238},
	abstract = {This paper analyses sexual health workers’ ‘talk’ around their introduction of a digital platform to enhance a regionally managed condom distribution scheme for young people. In examining the discursive resources workers used in framing the sexual health service, their service users and digital technology, we argue that problematic ideologies around young people and sexuality were exercised and reproduced. Workers positioned themselves as the gatekeepers of young people’s sexual health, who were in turn constructed as ‘mischievous’ and ‘misguided’, with technology having a corruptive role over what was considered to be ‘healthy’ and ‘normal’ sexual relationships. We suggest our findings indicate severe challenges in developing community-commissioned platforms alongside service providers, and questions how plausible user participation can be in attempting to conduct collaborative, participatory and engaged work in this context.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '18},
	publisher = {ACM Press},
	author = {Wood, Matthew and Garbett, Andrew and Morrissey, Kellie and Hopkins, Peter and Balaam, Madeline},
	year = {2018},
	keywords = {interactive visual exploration, automativ pattern detection, arrythmia detection},
	pages = {1--12},
	file = {Wood et al. - 2018 - Protection on that Erection Discourses of Acco.pdf:C\:\\Users\\conny\\Zotero\\storage\\QNA349JR\\Wood et al. - 2018 - Protection on that Erection Discourses of Acco.pdf:application/pdf}
}

@inproceedings{sarracino_user-guided_2017,
	address = {Denver, Colorado, USA},
	title = {User-{Guided} {Synthesis} of {Interactive} {Diagrams}},
	isbn = {978-1-4503-4655-9},
	url = {http://dl.acm.org/citation.cfm?doid=3025453.3025467},
	doi = {10.1145/3025453.3025467},
	abstract = {Interactive diagrams are expensive to build, requiring signiﬁcant programming experience. The cost of building such diagrams often prevents novice programmers or nonprogrammers from doing so. In this paper, we present userguided techniques that transform a static diagram into an interactive one without requiring the user to write code. We also present a tool called EDDIE that prototypes these techniques. We evaluate EDDIE through: (1) a case study in which we use EDDIE to implement existing real-world diagrams from the literature and (2) a usability session with target users in which subjects build several diagrams in EDDIE and provide feedback on EDDIE’s user experience. Our experiments demonstrate that EDDIE is usable and expressive, and that EDDIE enables real-world diagrams to be implemented without requiring programming expertise.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems} - {CHI} '17},
	publisher = {ACM Press},
	author = {Sarracino, John and Barrios-Arciga, Odaris and Zhu, Jasmine and Marcus, Noah and Lerner, Sorin and Wiedermann, Ben},
	year = {2017},
	pages = {195--207},
	file = {Sarracino et al. - 2017 - User-Guided Synthesis of Interactive Diagrams.pdf:C\:\\Users\\conny\\Zotero\\storage\\BFPA5B9G\\Sarracino et al. - 2017 - User-Guided Synthesis of Interactive Diagrams.pdf:application/pdf}
}

@inproceedings{jayatilaka_evaluating_2011,
	address = {Vancouver, BC, Canada},
	title = {Evaluating a pattern-based visual support approach for humanitarian landmine clearance},
	isbn = {978-1-4503-0228-9},
	url = {http://dl.acm.org/citation.cfm?doid=1978942.1979006},
	doi = {10.1145/1978942.1979006},
	abstract = {Unexploded landmines have severe post-conﬂict humanitarian repercussions: landmines cost lives, limbs and land. For deminers engaged in humanitarian landmine clearance, metal detectors remain the primary detection tool as more sophisticated technologies fail to get adopted due to restrictive cost, low reliability, and limited robustness. Metal detectors are, however, of limited effectiveness, as modern landmines contain only minimal amounts of metal, making them difﬁcult to distinguish from the ubiquitous but harmless metallic clutter littering post-combat areas. We seek to improve the safety and efﬁciency of the demining process by developing support tools that will enable deminers to make better decisions using feedback from existing metal detectors. To this end, in this paper we propose and evaluate a novel, pattern-based visual support approach inspired by the documented strategies employed by expert deminers. In our laboratory study, participants provided with a prototype of our support tool were 80\% less likely to mistake a mine for harmless clutter. A follow-up study demonstrates the potential of our pattern-based approach to enable peer decision-making support during landmine clearance. Lastly, we identify several design opportunities for further improving deminers’ decision making capabilities.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2011 annual conference on {Human} factors in computing systems - {CHI} '11},
	publisher = {ACM Press},
	author = {Jayatilaka, Lahiru G. and Bertuccelli, Luca F. and Staszewski, James and Gajos, Krzysztof Z.},
	year = {2011},
	keywords = {pattern-based visual support, Understanding and idetification, identify classification errors},
	pages = {453},
	file = {Jayatilaka et al. - 2011 - Evaluating a pattern-based visual support approach.pdf:C\:\\Users\\conny\\Zotero\\storage\\LSALLPN7\\Jayatilaka et al. - 2011 - Evaluating a pattern-based visual support approach.pdf:application/pdf}
}

@inproceedings{romero_evaluating_2011,
	address = {Vancouver, BC, Canada},
	title = {Evaluating video visualizations of human behavior},
	isbn = {978-1-4503-0228-9},
	url = {http://dl.acm.org/citation.cfm?doid=1978942.1979155},
	doi = {10.1145/1978942.1979155},
	abstract = {Previously, we presented Viz-A-Vis, a VIsualiZation of Activity through computer VISion [17]. Viz-A-Vis visualizes behavior as aggregate motion over observation space. In this paper, we present two complementary user studies of Viz-A-Vis measuring its performance and discovery affordances. First, we present a controlled user study aimed at comparatively measuring behavioral analysis preference and performance for observation and search tasks. Second, we describe a study with architects measuring discovery affordances and potential impacts on their work practices. We conclude: 1) Viz-A-Vis significantly reduced search time; and 2) it increased the number and quality of insightful discoveries.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2011 annual conference on {Human} factors in computing systems - {CHI} '11},
	publisher = {ACM Press},
	author = {Romero, Mario and Vialard, Alice and Peponis, John and Stasko, John and Abowd, Gregory},
	year = {2011},
	keywords = {User behaviour},
	pages = {1441},
	file = {Romero et al. - 2011 - Evaluating video visualizations of human behavior.pdf:C\:\\Users\\conny\\Zotero\\storage\\RV3RGEGW\\Romero et al. - 2011 - Evaluating video visualizations of human behavior.pdf:application/pdf}
}

@inproceedings{zhao_kronominer:_2011,
	address = {Vancouver, BC, Canada},
	title = {{KronoMiner}: using multi-foci navigation for the visual exploration of time-series data},
	isbn = {978-1-4503-0228-9},
	shorttitle = {{KronoMiner}},
	url = {http://dl.acm.org/citation.cfm?doid=1978942.1979195},
	doi = {10.1145/1978942.1979195},
	abstract = {The need for pattern discovery in long time-series data led researchers to develop interactive visualization tools and analytical algorithms for gaining insight into the data. Most of the literature on time-series data visualization either focus on a small number of tasks or a speciﬁc domain. We propose KronoMiner, a tool that embeds new interaction and visualization techniques as well as analytical capabilities for the visual exploration of time-series data. The interface’s design has been iteratively reﬁned based on feedback from expert users. Qualitative evaluation with an expert user not involved in the design process indicates that our prototype is promising for further research.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2011 annual conference on {Human} factors in computing systems - {CHI} '11},
	publisher = {ACM Press},
	author = {Zhao, Jian and Chevalier, Fanny and Balakrishnan, Ravin},
	year = {2011},
	keywords = {Feedback from user, HOW - Observation, Qualitative Approach, Time Series data, Feedback from userType of Work: Model/Framework/Theory, Type of Work: User Study, WHEN - Real-Time Applications, HOW - Observation, WHY - Evaluation of Tools and Systems, Type of Work: Model/Framework/Theory, WHY - Real-Time /Post-hoc Quantification},
	pages = {1737},
	file = {Zhao et al. - 2011 - KronoMiner using multi-foci navigation for the vi.pdf:C\:\\Users\\conny\\Zotero\\storage\\23CBTUPX\\Zhao et al. - 2011 - KronoMiner using multi-foci navigation for the vi.pdf:application/pdf}
}

@inproceedings{willett_commentspace:_2011,
	address = {Vancouver, BC, Canada},
	title = {{CommentSpace}: structured support for collaborative visual analysis},
	isbn = {978-1-4503-0228-9},
	shorttitle = {{CommentSpace}},
	url = {http://dl.acm.org/citation.cfm?doid=1978942.1979407},
	doi = {10.1145/1978942.1979407},
	abstract = {Collaborative visual analysis tools can enhance sensemaking by facilitating social interpretation and parallelization of effort. These systems enable distributed exploration and evidence gathering, allowing many users to pool their effort as they discuss and analyze the data. We explore how adding lightweight tag and link structure to comments can aid this analysis process. We present CommentSpace, a collaborative system in which analysts comment on visualizations and websites and then use tags and links to organize ﬁndings and identify others’ contributions. In a pair of studies comparing CommentSpace to a system without support for tags and links, we ﬁnd that a small, ﬁxed vocabulary of tags (question, hypothesis, to-do) and links (evidencefor, evidence-against) helps analysts more consistently and accurately classify evidence and establish common ground. We also ﬁnd that managing and incentivizing participation is important for analysts to progress from exploratory analysis to deeper analytical tasks. Finally, we demonstrate that tags and links can help teams complete evidence gathering and synthesis tasks and that organizing comments using tags and links improves analytic results.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2011 annual conference on {Human} factors in computing systems - {CHI} '11},
	publisher = {ACM Press},
	author = {Willett, Wesley and Heer, Jeffrey and Hellerstein, Joseph and Agrawala, Maneesh},
	year = {2011},
	keywords = {sensemaking, collaborative visual analysis, tagging},
	pages = {3131},
	file = {Willett et al. - 2011 - CommentSpace structured support for collaborative.pdf:C\:\\Users\\conny\\Zotero\\storage\\CUPGEMIZ\\Willett et al. - 2011 - CommentSpace structured support for collaborative.pdf:application/pdf}
}

@inproceedings{john_using_2011,
	address = {Vancouver, BC, Canada},
	title = {Using predictive human performance models to inspire and support {UI} design recommendations},
	isbn = {978-1-4503-0228-9},
	url = {http://dl.acm.org/citation.cfm?doid=1978942.1979088},
	doi = {10.1145/1978942.1979088},
	abstract = {Predictive human performance modeling has traditionally been used to make quantitative comparisons between alternative designs (e.g., task execution time for skilled users) instead of identifying UI problems or making design recommendations. This note investigates how reliably novice modelers can extract design recommendations from their models. Many HCI evaluation methods have been plagued by the “evaluator effect” [3], i.e., different people using the same method find different UI problems. Our data and analyses show that predictive human performance modeling is no exception. Novice modelers using CogTool [5] display a 34\% Any-Two Agreement in their design recommendations, a result in the upper quartile of evaluator effect studies. However, because these recommendations are grounded in models, they may have more reliable impact on measurable performance than recommendations arising from less formal methods.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2011 annual conference on {Human} factors in computing systems - {CHI} '11},
	publisher = {ACM Press},
	author = {John, Bonnie E.},
	year = {2011},
	keywords = {Guidance, Recommendation, Performance Modeling, exclude because of missing vis context},
	pages = {983},
	file = {John - 2011 - Using predictive human performance models to inspi.pdf:C\:\\Users\\conny\\Zotero\\storage\\MXUWYHFP\\John - 2011 - Using predictive human performance models to inspi.pdf:application/pdf}
}

@inproceedings{navalpakkam_mouse_2012,
	address = {Austin, Texas, USA},
	title = {Mouse tracking: measuring and predicting users' experience of web-based content},
	isbn = {978-1-4503-1015-4},
	shorttitle = {Mouse tracking},
	url = {http://dl.acm.org/citation.cfm?doid=2207676.2208705},
	doi = {10.1145/2207676.2208705},
	abstract = {Previous studies have used mouse tracking as a tool to measure usability of webpages, user attention and search relevance. In this paper, we go beyond measurement of user behavior to prediction of the resulting user experience from mouse patterns alone. Speciﬁcally, we identify mouse markers that can predict user frustration and reading struggles at reasonably high accuracy. We believe that mouse-based prediction of user experience is an important advance, and could potentially offer a scalable way to infer user experience on the web. In addition, we demonstrate that mouse tracking could be used for applications such as evaluating content layout and content noticeability; we apply this in particular to advertisements. More generally, it could be used to infer user attention in complex webpages containing images, text and varied content, including how attention patterns vary with page layout and user distraction.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2012 {ACM} annual conference on {Human} {Factors} in {Computing} {Systems} - {CHI} '12},
	publisher = {ACM Press},
	author = {Navalpakkam, Vidhya and Churchill, Elizabeth},
	year = {2012},
	keywords = {NOT DIRECTLY VISUALIZATION, gaze mapping, mous tracking},
	pages = {2963},
	file = {Navalpakkam and Churchill - 2012 - Mouse tracking measuring and predicting users' ex.pdf:C\:\\Users\\conny\\Zotero\\storage\\VIB2AJAV\\Navalpakkam and Churchill - 2012 - Mouse tracking measuring and predicting users' ex.pdf:application/pdf}
}

@inproceedings{endert_semantic_2012-1,
	address = {Austin, Texas, USA},
	title = {Semantic interaction for visual text analytics},
	isbn = {978-1-4503-1015-4},
	url = {http://dl.acm.org/citation.cfm?doid=2207676.2207741},
	doi = {10.1145/2207676.2207741},
	abstract = {Visual analytics emphasizes sensemaking of large, complex datasets through interactively exploring visualizations generated by statistical models. For example, dimensionality reduction methods use various similarity metrics to visualize textual document collections in a spatial metaphor, where similarities between documents are approximately represented through their relative spatial distances to each other in a 2D layout. This metaphor is designed to mimic analysts’ mental models of the document collection and support their analytic processes, such as clustering similar documents together. However, in current methods, users must interact with such visualizations using controls external to the visual metaphor, such as sliders, menus, or text fields, to directly control underlying model parameters that they do not understand and that do not relate to their analytic process occurring within the visual metaphor. In this paper, we present the opportunity for a new design space for visual analytic interaction, called semantic interaction, which seeks to enable analysts to spatially interact with such models directly within the visual metaphor using interactions that derive from their analytic process, such as searching, highlighting, annotating, and repositioning documents. Further, we demonstrate how semantic interactions can be implemented using machine learning techniques in a visual analytic tool, called ForceSPIRE, for interactive analysis of textual data within a spatial visualization. Analysts can express their expert domain knowledge about the documents by simply moving them, which guides the underlying model to improve the overall layout, taking the user’s feedback into account.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2012 {ACM} annual conference on {Human} {Factors} in {Computing} {Systems} - {CHI} '12},
	publisher = {ACM Press},
	author = {Endert, Alex and Fiaux, Patrick and North, Chris},
	year = {2012},
	keywords = {ENCODING - Model, HOW - Probabilistic Models, Semantic Interactions, Textual information, Type of Work - System, WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, HOW - Probabilistic Models, WHY - Adaptive Systems / Guidance, Type of Work - System, ENCODING - ModelAdaptive Systems / Guidance, WHY - Evaluation of Tools and Systems},
	pages = {473},
	file = {Endert et al. - 2012 - Semantic interaction for visual text analytics.pdf:C\:\\Users\\conny\\Zotero\\storage\\KDV2YAEB\\Endert et al. - 2012 - Semantic interaction for visual text analytics.pdf:application/pdf}
}

@inproceedings{kodagoda_interactive_2012,
author = {Kodagoda, Neesha and Wong, B.L. William and Rooney, Chris and Khan, Nawaz},
title = {Interactive Visualization for Low Literacy Users: From Lessons Learnt to Design},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2208565},
doi = {10.1145/2207676.2208565},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1159–1168},
numpages = {10},
keywords = {visualization, user interface, mental models, low literacy, high literacy, design principles},
location = {Austin, Texas, USA},
series = {CHI ’12}
}


@inproceedings{hullman_contextifier:_2013,
	address = {Paris, France},
	title = {Contextifier: automatic generation of annotated stock visualizations},
	isbn = {978-1-4503-1899-0},
	shorttitle = {Contextifier},
	url = {http://dl.acm.org/citation.cfm?doid=2470654.2481374},
	doi = {10.1145/2470654.2481374},
	abstract = {Online news tools—for aggregation, summarization and automatic generation—are an area of fruitful development as reading news online becomes increasingly commonplace. While textual tools have dominated these developments, annotated information visualizations are a promising way to complement articles based on their ability to add context. But the manual effort required for professional designers to create thoughtful annotations for contextualizing news visualizations is difficult to scale. We describe the design of Contextifier, a novel system that automatically produces custom, annotated visualizations of stock behavior given a news article about a company. Contextifier’s algorithms for choosing annotations is informed by a study of professionally created visualizations and takes into account visual salience, contextual relevance, and a detection of key events in the company’s history. In evaluating our system we find that Contextifier better balances graphical salience and relevance than the baseline.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems} - {CHI} '13},
	publisher = {ACM Press},
	author = {Hullman, Jessica and Diakopoulos, Nicholas and Adar, Eytan},
	year = {2013},
	keywords = {automatic annotated line graphs, textual tool, feature selection},
	pages = {2707},
	file = {Hullman et al. - 2013 - Contextifier automatic generation of annotated st.pdf:C\:\\Users\\conny\\Zotero\\storage\\Y854WR6U\\Hullman et al. - 2013 - Contextifier automatic generation of annotated st.pdf:application/pdf}
}

@misc{noauthor_discussion_nodate,
	title = {Discussion minutes: {Survey} on {Analytics} {Provenance}},
	shorttitle = {Discussion minutes},
	url = {https://docs.google.com/document/d/1AaQAUPg3CI8KjVGA-GPAhcipuNEGprfQQm5P-orT_J0/edit?usp=sharing&usp=embed_facebook},
	abstract = {Progress update 2019-12-02 Marc Looked through TiiS issues from 2010-2019 (see https://dl.acm.org/citation.cfm?id=J1318\&preflayout=flat\#prox) Was not able to find a single paper that matched our criteria/scope Porteus et al, Applying Planning to Interactive Storytelling: Narrative Control Using S...},
	language = {de},
	urldate = {2019-12-09},
	journal = {Google Docs},
	file = {Snapshot:C\:\\Users\\conny\\Zotero\\storage\\SRUEV85G\\edit.html:text/html}
}

@article{cao_targetvue:_2016,
	title = {{TargetVue}: {Visual} {Analysis} of {Anomalous} {User} {Behaviors} in {Online} {Communication} {Systems}},
	volume = {22},
	issn = {2160-9306},
	shorttitle = {{TargetVue}},
	doi = {10.1109/TVCG.2015.2467196},
	abstract = {Users with anomalous behaviors in online communication systems (e.g. email and social medial platforms) are potential threats to society. Automated anomaly detection based on advanced machine learning techniques has been developed to combat this issue; challenges remain, though, due to the difficulty of obtaining proper ground truth for model training and evaluation. Therefore, substantial human judgment on the automated analysis results is often required to better adjust the performance of anomaly detection. Unfortunately, techniques that allow users to understand the analysis results more efficiently, to make a confident judgment about anomalies, and to explore data in their context, are still lacking. In this paper, we propose a novel visual analysis system, TargetVue, which detects anomalous users via an unsupervised learning model and visualizes the behaviors of suspicious users in behavior-rich context through novel visualization designs and multiple coordinated contextual views. Particularly, TargetVue incorporates three new ego-centric glyphs to visually summarize a user's behaviors which effectively present the user's communication activities, features, and social interactions. An efficient layout method is proposed to place these glyphs on a triangle grid, which captures similarities among users and facilitates comparisons of behaviors of different users. We demonstrate the power of TargetVue through its application in a social bot detection challenge using Twitter data, a case study based on email records, and an interview with expert users. Our evaluation shows that TargetVue is beneficial to the detection of users with anomalous communication behaviors.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Cao, Nan and Shi, Conglei and Lin, Sabrina and Lu, Jie and Lin, Yu-Ru and Lin, Ching-Yung},
	month = jan,
	year = {2016},
	keywords = {Type of Work: Tool/Software, HOW - Classification Models, HOW: anomaly detection, HOW: temporal pattern analysis, Maybe related. A tool to analyze anomalous user behaviors in online communication, WHY: analyze user's online (communication) behavior, WHY - User Behavior / User Characteristics / User Modelling, ENCODING - Vector},
	pages = {280--289},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\FFWRBXK4\\7185421.html:text/html}
}

@article{kindlmann_diderot:_2016,
	title = {Diderot: a {Domain}-{Specific} {Language} for {Portable} {Parallel} {Scientific} {Visualization} and {Image} {Analysis}},
	volume = {22},
	issn = {2160-9306},
	shorttitle = {Diderot},
	doi = {10.1109/TVCG.2015.2467449},
	abstract = {Many algorithms for scientific visualization and image analysis are rooted in the world of continuous scalar, vector, and tensor fields, but are programmed in low-level languages and libraries that obscure their mathematical foundations. Diderot is a parallel domain-specific language that is designed to bridge this semantic gap by providing the programmer with a high-level, mathematical programming notation that allows direct expression of mathematical concepts in code. Furthermore, Diderot provides parallel performance that takes advantage of modern multicore processors and GPUs. The high-level notation allows a concise and natural expression of the algorithms and the parallelism allows efficient execution on real-world datasets.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Kindlmann, Gordon and Chiw, Charisee and Seltzer, Nicholas and Samuels, Lamont and Reppy, John},
	month = jan,
	year = {2016},
	pages = {867--876},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\HMMVS56H\\7192663.html:text/html}
}

@article{ai-awami_neuroblocks_2016,
	title = {{NeuroBlocks} – {Visual} {Tracking} of {Segmentation} and {Proofreading} for {Large} {Connectomics} {Projects}},
	volume = {22},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2015.2467441},
	abstract = {In the field of connectomics, neuroscientists acquire electron microscopy volumes at nanometer resolution in order to reconstruct a detailed wiring diagram of the neurons in the brain. The resulting image volumes, which often are hundreds of terabytes in size, need to be segmented to identify cell boundaries, synapses, and important cell organelles. However, the segmentation process of a single volume is very complex, time-intensive, and usually performed using a diverse set of tools and many users. To tackle the associated challenges, this paper presents NeuroBlocks, which is a novel visualization system for tracking the state, progress, and evolution of very large volumetric segmentation data in neuroscience. NeuroBlocks is a multi-user web-based application that seamlessly integrates the diverse set of tools that neuroscientists currently use for manual and semi-automatic segmentation, proofreading, visualization, and analysis. NeuroBlocks is the first system that integrates this heterogeneous tool set, providing crucial support for the management, provenance, accountability, and auditing of large-scale segmentations. We describe the design of NeuroBlocks, starting with an analysis of the domain-specific tasks, their inherent challenges, and our subsequent task abstraction and visual representation. We demonstrate the utility of our design based on two case studies that focus on different user roles and their respective requirements for performing and tracking the progress of segmentation and proofreading in a large real-world connectomics project.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Ai-Awami, Ali K. and Beyer, Johanna and Haehn, Daniel and Kasthuri, Narayanan and Lichtman, Jeff W. and Pfister, Hanspeter and Hadwiger, Markus},
	month = jan,
	year = {2016},
	keywords = {Type of Work: Tool/Software, WHY: reproducibility, HOW: logging},
	pages = {738--746},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\7Q9FZALJ\\Ai-Awami et al. - 2016 - NeuroBlocks – Visual Tracking of Segmentation and .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\SE63V5K7\\7192653.html:text/html}
}

@article{yu_cast:_2016,
	title = {{CAST}: {Effective} and {Efficient} {User} {Interaction} for {Context}-{Aware} {Selection} in {3D} {Particle} {Clouds}},
	volume = {22},
	issn = {2160-9306},
	shorttitle = {{CAST}},
	doi = {10.1109/TVCG.2015.2467202},
	abstract = {We present a family of three interactive Context-Aware Selection Techniques (CAST) for the analysis of large 3D particle datasets. For these datasets, spatial selection is an essential prerequisite to many other analysis tasks. Traditionally, such interactive target selection has been particularly challenging when the data subsets of interest were implicitly defined in the form of complicated structures of thousands of particles. Our new techniques SpaceCast, TraceCast, and PointCast improve usability and speed of spatial selection in point clouds through novel context-aware algorithms. They are able to infer a user's subtle selection intention from gestural input, can deal with complex situations such as partially occluded point clusters or multiple cluster layers, and can all be fine-tuned after the selection interaction has been completed. Together, they provide an effective and efficient tool set for the fast exploratory analysis of large datasets. In addition to presenting Cast, we report on a formal user study that compares our new techniques not only to each other but also to existing state-of-the-art selection methods. Our results show that Cast family members are virtually always faster than existing methods without tradeoffs in accuracy. In addition, qualitative feedback shows that PointCast and TraceCast were strongly favored by our participants for intuitiveness and efficiency.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Yu, Lingyun and Efstathiou, Konstantinos and Isenberg, Petra and Isenberg, Tobias},
	month = jan,
	year = {2016},
	pages = {886--895},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\6SRCGTPJ\\7192726.html:text/html;Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\QLF83KZH\\Yu et al. - 2016 - CAST Effective and Efficient User Interaction for.pdf:application/pdf}
}

@article{schroeder_visualization-by-sketching:_2016,
	title = {Visualization-by-{Sketching}: {An} {Artist}'s {Interface} for {Creating} {Multivariate} {Time}-{Varying} {Data} {Visualizations}},
	volume = {22},
	issn = {2160-9306},
	shorttitle = {Visualization-by-{Sketching}},
	doi = {10.1109/TVCG.2015.2467153},
	abstract = {We present Visualization-by-Sketching, a direct-manipulation user interface for designing new data visualizations. The goals are twofold: First, make the process of creating real, animated, data-driven visualizations of complex information more accessible to artists, graphic designers, and other visual experts with traditional, non-technical training. Second, support and enhance the role of human creativity in visualization design, enabling visual experimentation and workflows similar to what is possible with traditional artistic media. The approach is to conceive of visualization design as a combination of processes that are already closely linked with visual creativity: sketching, digital painting, image editing, and reacting to exemplars. Rather than studying and tweaking low-level algorithms and their parameters, designers create new visualizations by painting directly on top of a digital data canvas, sketching data glyphs, and arranging and blending together multiple layers of animated 2D graphics. This requires new algorithms and techniques to interpret painterly user input relative to data “under” the canvas, balance artistic freedom with the need to produce accurate data visualizations, and interactively explore large (e.g., terabyte-sized) multivariate datasets. Results demonstrate a variety of multivariate data visualization techniques can be rapidly recreated using the interface. More importantly, results and feedback from artists support the potential for interfaces in this style to attract new, creative users to the challenging task of designing more effective data visualizations and to help these users stay “in the creative zone” as they work.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Schroeder, David and Keefe, Daniel F.},
	month = jan,
	year = {2016},
	keywords = {Type of Work: Tool/Software, HOW: visualization by sketching, Maybe related. Generate (scivis) visualization by sketching, WHY: create visual designs that reflect a user's needs},
	pages = {877--885},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\FLHDEQUH\\7185456.html:text/html}
}

@article{wongsuphasawat_voyager:_2016,
	title = {Voyager: {Exploratory} {Analysis} via {Faceted} {Browsing} of {Visualization} {Recommendations}},
	volume = {22},
	issn = {2160-9306},
	shorttitle = {Voyager},
	doi = {10.1109/TVCG.2015.2467191},
	abstract = {General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wongsuphasawat, Kanit and Moritz, Dominik and Anand, Anushka and Mackinlay, Jock and Howe, Bill and Heer, Jeffrey},
	month = jan,
	year = {2016},
	keywords = {WHEN - Real-Time Applications, Type of Work: Tool/Software, HOW - Classification Models, HOW: ??? (Algorithm called Compass), HOW: Clustering, WHY: visualization recommendation, WHY - Model Steering / Active Learning, ENCODING - Grammar},
	pages = {649--658},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\JVGR4EXB\\7192728.html:text/html}
}

@article{satyanarayan_reactive_2016,
	title = {Reactive {Vega}: {A} {Streaming} {Dataflow} {Architecture} for {Declarative} {Interactive} {Visualization}},
	volume = {22},
	issn = {2160-9306},
	shorttitle = {Reactive {Vega}},
	doi = {10.1109/TVCG.2015.2467091},
	abstract = {We present Reactive Vega, a system architecture that provides the first robust and comprehensive treatment of declarative visual and interaction design for data visualization. Starting from a single declarative specification, Reactive Vega constructs a dataflow graph in which input data, scene graph elements, and interaction events are all treated as first-class streaming data sources. To support expressive interactive visualizations that may involve time-varying scalar, relational, or hierarchical data, Reactive Vega's dataflow graph can dynamically re-write itself at runtime by extending or pruning branches in a data-driven fashion. We discuss both compile- and run-time optimizations applied within Reactive Vega, and share the results of benchmark studies that indicate superior interactive performance to both D3 and the original, non-reactive Vega system.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Satyanarayan, Arvind and Russell, Ryan and Hoffswell, Jane and Heer, Jeffrey},
	month = jan,
	year = {2016},
	pages = {659--668},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\V5T9X9KD\\7192704.html:text/html}
}

@inproceedings{zhao_pearl:_2014,
	title = {{PEARL}: {An} interactive visual analytic tool for understanding personal emotion style derived from social media},
	shorttitle = {{PEARL}},
	doi = {10.1109/VAST.2014.7042496},
	abstract = {Hundreds of millions of people leave digital footprints on social media (e.g., Twitter and Facebook). Such data not only disclose a person's demographics and opinions, but also reveal one's emotional style. Emotional style captures a person's patterns of emotions over time, including his overall emotional volatility and resilience. Understanding one's emotional style can provide great benefits for both individuals and businesses alike, including the support of self-reflection and delivery of individualized customer care. We present PEARL, a timeline-based visual analytic tool that allows users to interactively discover and examine a person's emotional style derived from this person's social media text. Compared to other visual text analytic systems, our work offers three unique contributions. First, it supports multi-dimensional emotion analysis from social media text to automatically detect a person's expressed emotions at different time points and summarize those emotions to reveal the person's emotional style. Second, it effectively visualizes complex, multi-dimensional emotion analysis results to create a visual emotional profile of an individual, which helps users browse and interpret one's emotional style. Third, it supports rich visual interactions that allow users to interactively explore and validate emotion analysis results. We have evaluated our work extensively through a series of studies. The results demonstrate the effectiveness of our tool both in emotion analysis from social media and in support of interactive visualization of the emotion analysis results.},
	booktitle = {2014 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {Zhao, Jian and Gou, Liang and Wang, Fei and Zhou, Michelle},
	month = oct,
	year = {2014},
	note = {ISSN: null},
	keywords = {Type of Work: Tool/Software, HOW: clustering, HOW: visualization design, Maybe related. Analysis of personal health/emotion log, WHY: quantified self, WHY: understand self mood over time},
	pages = {203--212},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\GPTDNYDL\\7042496.html:text/html}
}

@inproceedings{bradel_multi-model_2014,
	title = {Multi-model semantic interaction for text analytics},
	doi = {10.1109/VAST.2014.7042492},
	abstract = {Semantic interaction offers an intuitive communication mechanism between human users and complex statistical models. By shielding the users from manipulating model parameters, they focus instead on directly manipulating the spatialization, thus remaining in their cognitive zone. However, this technique is not inherently scalable past hundreds of text documents. To remedy this, we present the concept of multi-model semantic interaction, where semantic interactions can be used to steer multiple models at multiple levels of data scale, enabling users to tackle larger data problems. We also present an updated visualization pipeline model for generalized multi-model semantic interaction. To demonstrate multi-model semantic interaction, we introduce StarSPIRE, a visual text analytics prototype that transforms user interactions on documents into both small-scale display layout updates as well as large-scale relevancy-based document selection.},
	booktitle = {2014 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {Bradel, Lauren and North, Chris and House, Leanna and Leman, Scotland},
	month = oct,
	year = {2014},
	note = {ISSN: null},
	keywords = {HOW - Program Synthesis, WHEN - Real-Time Applications, Type of Work: Tool/Software, HOW: semantic interaction, WHY: model steering (of multiple models), WHY - Model Steering / Active Learning, WHY - User Behavior / User Characteristics / User Modelling, ENCODING - Vector},
	pages = {163--172},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\BNMS24F3\\7042492.html:text/html}
}

@inproceedings{wongsuphasawat_using_2014,
	title = {Using visualizations to monitor changes and harvest insights from a global-scale logging infrastructure at {Twitter}},
	doi = {10.1109/VAST.2014.7042487},
	abstract = {Logging user activities is essential to data analysis for internet products and services. Twitter has built a unified logging infrastructure that captures user activities across all clients it owns, making it one of the largest datasets in the organization. This paper describes challenges and opportunities in applying information visualization to log analysis at this massive scale, and shows how various visualization techniques can be adapted to help data scientists extract insights. In particular, we focus on two scenarios: (1) monitoring and exploring a large collection of log events, and (2) performing visual funnel analysis on log data with tens of thousands of event types. Two interactive visualizations were developed for these purposes: we discuss design choices and the implementation of these systems, along with case studies of how they are being used in day-to-day operations at Twitter.},
	booktitle = {2014 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {Wongsuphasawat, Krist and Lin, Jimmy},
	month = oct,
	year = {2014},
	note = {ISSN: null},
	keywords = {Type of Work: Tool/Software, HOW: visualization design, HOW: query language for querying sequences, Maybe related. Analysis of user usage log from Twitter, WHY: visual log analysis},
	pages = {113--122},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\C2AK84HL\\7042487.html:text/html;Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\XQVDDAPH\\Wongsuphasawat and Lin - 2014 - Using visualizations to monitor changes and harves.pdf:application/pdf}
}

@inproceedings{wang_visual_2014,
	title = {A visual reasoning approach for data-driven transport assessment on urban roads},
	doi = {10.1109/VAST.2014.7042486},
	abstract = {Transport assessment plays a vital role in urban planning and traffic control, which are influenced by multi-faceted traffic factors involving road infrastructure and traffic flow. Conventional solutions can hardly meet the requirements and expectations of domain experts. In this paper we present a data-driven solution by leveraging a visual analysis system to evaluate the real traffic situations based on taxi trajectory data. A sketch-based visual interface is designed to support dynamic query and visual reasoning of traffic situations within multiple coordinated views. In particular, we propose a novel road-based query model for analysts to interactively conduct evaluation tasks. This model is supported by a bi-directional hash structure, TripHash, which enables real-time responses to the data queries over a huge amount of trajectory data. Case studies with a real taxi GPS trajectory dataset ({\textgreater} 30GB) show that our system performs well for on-demand transport assessment and reasoning.},
	booktitle = {2014 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {Wang, Fei and Chen, Wei and Wu, Feiran and Zhao, Ye and Hong, Han and Gu, Tianyu and Wang, Long and Liang, Ronghua and Bao, Hujun},
	month = oct,
	year = {2014},
	note = {ISSN: null},
	keywords = {WHEN - Real-Time Applications, HOW - Pattern Analysis, Type of Work: Tool/Software, WHY - Prefetching, HOW: interactive query construction, WHY - Explain ML Model / Debug Algorithm / Query Plan, HOW: sketch-based querying, WHY: querying trajectory data, ENCODING - Vector},
	pages = {103--112},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\7YGG4JJM\\7042486.html:text/html}
}

@inproceedings{gomez_insight-_2014,
	title = {An insight- and task-based methodology for evaluating spatiotemporal visual analytics},
	doi = {10.1109/VAST.2014.7042482},
	abstract = {We present a method for evaluating visualizations using both tasks and exploration, and demonstrate this method in a study of spatiotemporal network designs for a visual analytics system. The method is well suited for studying visual analytics applications in which users perform both targeted data searches and analyses of broader patterns. In such applications, an effective visualization design is one that helps users complete tasks accurately and efficiently, and supports hypothesis generation during open-ended exploration. To evaluate both of these aims in a single study, we developed an approach called layered insight- and task-based evaluation (LITE) that interposes several prompts for observations about the data model between sequences of predefined search tasks. We demonstrate the evaluation method in a user study of four network visualizations for spatiotemporal data in a visual analytics application. Results include findings that might have been difficult to obtain in a single experiment using a different methodology. For example, with one dataset we studied, we found that on average participants were faster on search tasks using a force-directed layout than using our other designs; at the same time, participants found this design least helpful in understanding the data. Our contributions include a novel evaluation method that combines well-defined tasks with exploration and observation, an evaluation of network visualization designs for spatiotemporal visual analytics, and guidelines for using this evaluation method.},
	booktitle = {2014 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {Gomez, Steven R. and Guo, Hua and Ziemkiewicz, Caroline and Laidlaw, David H.},
	month = oct,
	year = {2014},
	note = {ISSN: null},
	pages = {63--72},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\797QC6HF\\7042482.html:text/html}
}

@inproceedings{behrisch_feedback-driven_2014,
	title = {Feedback-driven interactive exploration of large multidimensional data supported by visual classifier},
	doi = {10.1109/VAST.2014.7042480},
	abstract = {The extraction of relevant and meaningful information from multivariate or high-dimensional data is a challenging problem. One reason for this is that the number of possible representations, which might contain relevant information, grows exponentially with the amount of data dimensions. Also, not all views from a possibly large view space, are potentially relevant to a given analysis task or user. Focus+Context or Semantic Zoom Interfaces can help to some extent to efficiently search for interesting views or data segments, yet they show scalability problems for very large data sets. Accordingly, users are confronted with the problem of identifying interesting views, yet the manual exploration of the entire view space becomes ineffective or even infeasible. While certain quality metrics have been proposed recently to identify potentially interesting views, these often are defined in a heuristic way and do not take into account the application or user context. We introduce a framework for a feedback-driven view exploration, inspired by relevance feedback approaches used in Information Retrieval. Our basic idea is that users iteratively express their notion of interestingness when presented with candidate views. From that expression, a model representing the user's preferences, is trained and used to recommend further interesting view candidates. A decision support system monitors the exploration process and assesses the relevance-driven search process for convergence and stability. We present an instantiation of our framework for exploration of Scatter Plot Spaces based on visual features. We demonstrate the effectiveness of this implementation by a case study on two real-world datasets. We also discuss our framework in light of design alternatives and point out its usefulness for development of user- and context-dependent visual exploration systems.},
	booktitle = {2014 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {Behrisch, Michael and Korkmaz, Fatih and Shao, Lin and Schreck, Tobias},
	month = oct,
	year = {2014},
	note = {ISSN: null},
	pages = {43--52},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\WNQBJ6LE\\7042480.html:text/html;Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\QTFH4PDK\\Behrisch et al. - 2014 - Feedback-driven interactive exploration of large m.pdf:application/pdf}
}

@article{matkovic_visual_2014,
	title = {Visual {Analytics} for {Complex} {Engineering} {Systems}: {Hybrid} {Visual} {Steering} of {Simulation} {Ensembles}},
	volume = {20},
	issn = {2160-9306},
	shorttitle = {Visual {Analytics} for {Complex} {Engineering} {Systems}},
	doi = {10.1109/TVCG.2014.2346744},
	abstract = {In this paper we propose a novel approach to hybrid visual steering of simulation ensembles. A simulation ensemble is a collection of simulation runs of the same simulation model using different sets of control parameters. Complex engineering systems have very large parameter spaces so a naïve sampling can result in prohibitively large simulation ensembles. Interactive steering of simulation ensembles provides the means to select relevant points in a multi-dimensional parameter space (design of experiment). Interactive steering efficiently reduces the number of simulation runs needed by coupling simulation and visualization and allowing a user to request new simulations on the fly. As system complexity grows, a pure interactive solution is not always sufficient. The new approach of hybrid steering combines interactive visual steering with automatic optimization. Hybrid steering allows a domain expert to interactively (in a visualization) select data points in an iterative manner, approximate the values in a continuous region of the simulation space (by regression) and automatically find the “best” points in this continuous region based on the specified constraints and objectives (by optimization). We argue that with the full spectrum of optimization options, the steering process can be improved substantially. We describe an integrated system consisting of a simulation, a visualization, and an optimization component. We also describe typical tasks and propose an interactive analysis workflow for complex engineering systems. We demonstrate our approach on a case study from automotive industry, the optimization of a hydraulic circuit in a high pressure common rail Diesel injection system.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Matković, Kreŝimir and Gračanin, Denis and Splechtna, Rainer and Jelović, Mario and Stehno, Benedikt and Hauser, Helwig and Purgathofer, Werner},
	month = dec,
	year = {2014},
	pages = {1803--1812},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\IN4EHDXL\\6876045.html:text/html}
}

@article{brown_finding_2014,
	title = {Finding {Waldo}: {Learning} about {Users} from their {Interactions}},
	volume = {20},
	issn = {2160-9306},
	shorttitle = {Finding {Waldo}},
	doi = {10.1109/TVCG.2014.2346575},
	abstract = {Visual analytics is inherently a collaboration between human and computer. However, in current visual analytics systems, the computer has limited means of knowing about its users and their analysis processes. While existing research has shown that a user's interactions with a system reflect a large amount of the user's reasoning process, there has been limited advancement in developing automated, real-time techniques that mine interactions to learn about the user. In this paper, we demonstrate that we can accurately predict a user's task performance and infer some user personality traits by using machine learning techniques to analyze interaction data. Specifically, we conduct an experiment in which participants perform a visual search task, and apply well-known machine learning algorithms to three encodings of the users' interaction data. We achieve, depending on algorithm and encoding, between 62\% and 83\% accuracy at predicting whether each user will be fast or slow at completing the task. Beyond predicting performance, we demonstrate that using the same techniques, we can infer aspects of the user's personality factors, including locus of control, extraversion, and neuroticism. Further analyses show that strong results can be attained with limited observation time: in one case 95\% of the final accuracy is gained after a quarter of the average task completion time. Overall, our findings show that interactions can provide information to the computer about its human collaborator, and establish a foundation for realizing mixed-initiative visual analytics systems.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Brown, Eli T and Ottley, Alvitta and Zhao, Helen and Lin, Quan and Souvenir, Richard and Endert, Alex and Chang, Remco},
	month = dec,
	year = {2014},
	keywords = {Type of Work: User Study, WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, HOW: classification, WHY: predict user performance, WHY: predict user personality traits, HOW - Probabilistic Models / Prediction, HOW - Classification Models, Type of Work: Technique \& Algorithm, WHY - Model Steering / Active Learning, WHY - User Behavior / User Characteristics / User Modelling, ENCODING - Sequence},
	pages = {1663--1672},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\CXEZCE3A\\6875913.html:text/html;Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\E6JRYD5K\\Brown et al. - 2014 - Finding Waldo Learning about Users from their Int.pdf:application/pdf}
}

@article{mahyar_supporting_2014,
	title = {Supporting {Communication} and {Coordination} in {Collaborative} {Sensemaking}},
	volume = {20},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2014.2346573},
	abstract = {When people work together to analyze a data set, they need to organize their findings, hypotheses, and evidence, share that information with their collaborators, and coordinate activities amongst team members. Sharing externalizations (recorded information such as notes) could increase awareness and assist with team communication and coordination. However, we currently know little about how to provide tool support for this sort of sharing. We explore how linked common work (LCW) can be employed within a `collaborative thinking space', to facilitate synchronous collaborative sensemaking activities in Visual Analytics (VA). Collaborative thinking spaces provide an environment for analysts to record, organize, share and connect externalizations. Our tool, CLIP, extends earlier thinking spaces by integrating LCW features that reveal relationships between collaborators' findings. We conducted a user study comparing CLIP to a baseline version without LCW. Results demonstrated that LCW significantly improved analytic outcomes at a collaborative intelligence task. Groups using CLIP were also able to more effectively coordinate their work, and held more discussion of their findings and hypotheses. LCW enabled them to maintain awareness of each other's activities and findings and link those findings to their own work, preventing disruptive oral awareness notifications.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Mahyar, Narges and Tory, Melanie},
	month = dec,
	year = {2014},
	keywords = {Type of Work: Case Study, Type of Work: Tool/Software, WHY: collaborative analysis, HOW: organization of evidence and finding as a graph, HOW: visualization of sensemaking history, Maybe related. Manual organization of finding / provenance},
	pages = {1633--1642},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\DIKPSU7Z\\6875986.html:text/html}
}

@article{choi_vivaldi:_2014,
	title = {Vivaldi: {A} {Domain}-{Specific} {Language} for {Volume} {Processing} and {Visualization} on {Distributed} {Heterogeneous} {Systems}},
	volume = {20},
	issn = {2160-9306},
	shorttitle = {Vivaldi},
	doi = {10.1109/TVCG.2014.2346322},
	abstract = {As the size of image data from microscopes and telescopes increases, the need for high-throughput processing and visualization of large volumetric data has become more pressing. At the same time, many-core processors and GPU accelerators are commonplace, making high-performance distributed heterogeneous computing systems affordable. However, effectively utilizing GPU clusters is difficult for novice programmers, and even experienced programmers often fail to fully leverage the computing power of new parallel architectures due to their steep learning curve and programming complexity. In this paper, we propose Vivaldi, a new domain-specific language for volume processing and visualization on distributed heterogeneous computing systems. Vivaldi's Python-like grammar and parallel processing abstractions provide flexible programming tools for non-experts to easily write high-performance parallel computing code. Vivaldi provides commonly used functions and numerical operators for customized visualization and high-throughput image processing applications. We demonstrate the performance and usability of Vivaldi on several examples ranging from volume rendering to image segmentation.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Choi, Hyungsuk and Choi, Woohyuk and Quan, Tran Minh and Hildebrand, David G. C. and Pfister, Hanspeter and Jeong, Won-Ki},
	month = dec,
	year = {2014},
	pages = {2407--2416},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\6GHBJED4\\6875916.html:text/html}
}

@article{rautek_vislang:_2014,
	title = {{ViSlang}: {A} {System} for {Interpreted} {Domain}-{Specific} {Languages} for {Scientific} {Visualization}},
	volume = {20},
	issn = {2160-9306},
	shorttitle = {{ViSlang}},
	doi = {10.1109/TVCG.2014.2346318},
	abstract = {Researchers from many domains use scientific visualization in their daily practice. Existing implementations of algorithms usually come with a graphical user interface (high-level interface), or as software library or source code (low-level interface). In this paper we present a system that integrates domain-specific languages (DSLs) and facilitates the creation of new DSLs. DSLs provide an effective interface for domain scientists avoiding the difficulties involved with low-level interfaces and at the same time offering more flexibility than high-level interfaces. We describe the design and implementation of ViSlang, an interpreted language specifically tailored for scientific visualization. A major contribution of our design is the extensibility of the ViSlang language. Novel DSLs that are tailored to the problems of the domain can be created and integrated into ViSlang. We show that our approach can be added to existing user interfaces to increase the flexibility for expert users on demand, but at the same time does not interfere with the user experience of novice users. To demonstrate the flexibility of our approach we present new DSLs for volume processing, querying and visualization. We report the implementation effort for new DSLs and compare our approach with Matlab and Python implementations in terms of run-time performance.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Rautek, Peter and Bruckner, Stefan and Gröller, M. Eduard and Hadwiger, Markus},
	month = dec,
	year = {2014},
	pages = {2388--2396},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\ZDQ5ZC7Z\\6876040.html:text/html}
}

@article{kindlmann_algebraic_2014,
	title = {An {Algebraic} {Process} for {Visualization} {Design}},
	volume = {20},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2014.2346325},
	abstract = {We present a model of visualization design based on algebraic considerations of the visualization process. The model helps characterize visual encodings, guide their design, evaluate their effectiveness, and highlight their shortcomings. The model has three components: the underlying mathematical structure of the data or object being visualized, the concrete representation of the data in a computer, and (to the extent possible) a mathematical description of how humans perceive the visualization. Because we believe the value of our model lies in its practical application, we propose three general principles for good visualization design. We work through a collection of examples where our model helps explain the known properties of existing visualizations methods, both good and not-so-good, as well as suggesting some novel methods. We describe how to use the model alongside experimental user studies, since it can help frame experiment outcomes in an actionable manner. Exploring the implications and applications of our model and its design principles should provide many directions for future visualization research.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Kindlmann, Gordon and Scheidegger, Carlos},
	month = dec,
	year = {2014},
	pages = {2181--2190},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\A7DZ6YG4\\6875930.html:text/html}
}

@article{perin_revisiting_2014,
	title = {Revisiting {Bertin} {Matrices}: {New} {Interactions} for {Crafting} {Tabular} {Visualizations}},
	volume = {20},
	issn = {2160-9306},
	shorttitle = {Revisiting {Bertin} {Matrices}},
	doi = {10.1109/TVCG.2014.2346279},
	abstract = {We present Bertifier, a web app for rapidly creating tabular visualizations from spreadsheets. Bertifier draws from Jacques Bertin's matrix analysis method, whose goal was to “simplify without destroying” by encoding cell values visually and grouping similar rows and columns. Although there were several attempts to bring this method to computers, no implementation exists today that is both exhaustive and accessible to a large audience. Bertifier remains faithful to Bertin's method while leveraging the power of today's interactive computers. Tables are formatted and manipulated through crossets, a new interaction technique for rapidly applying operations on rows and columns. We also introduce visual reordering, a semi-interactive reordering approach that lets users apply and tune automatic reordering algorithms in a WYSIWYG manner. Sessions with eight users from different backgrounds suggest that Bertifier has the potential to bring Bertin's method to a wider audience of both technical and non-technical users, and empower them with data analysis and communication tools that were so far only accessible to a handful of specialists.COMPUTER},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Perin, Charles and Dragicevic, Pierre and Fekete, Jean-Daniel},
	month = dec,
	year = {2014},
	pages = {2082--2091},
	file = {Accepted Version:C\:\\Users\\conny\\Zotero\\storage\\YGJ9IJVR\\Perin et al. - 2014 - Revisiting Bertin Matrices New Interactions for C.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\D3J3F3LD\\6875988.html:text/html}
}

@article{kondo_dimpvis:_2014,
	title = {{DimpVis}: {Exploring} {Time}-varying {Information} {Visualizations} by {Direct} {Manipulation}},
	volume = {20},
	issn = {2160-9306},
	shorttitle = {{DimpVis}},
	doi = {10.1109/TVCG.2014.2346250},
	abstract = {We introduce a new direct manipulation technique, DimpVis, for interacting with visual items in information visualizations to enable exploration of the time dimension. DimpVis is guided by visual hint paths which indicate how a selected data item changes through the time dimension in a visualization. Temporal navigation is controlled by manipulating any data item along its hint path. All other items are updated to reflect the new time. We demonstrate how the DimpVis technique can be designed to directly manipulate position, colour, and size in familiar visualizations such as bar charts and scatter plots, as a means for temporal navigation. We present results from a comparative evaluation, showing that the DimpVis technique was subjectively preferred and quantitatively competitive with the traditional time slider, and significantly faster than small multiples for a variety of tasks.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Kondo, Brittany and Collins, Christopher},
	month = dec,
	year = {2014},
	keywords = {Type of work: system / technique, HOW: semantic interaction, WHY: interactive querying of temporal data, Maybe related... Not clear},
	pages = {2003--2012},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\8BWCE2AS\\6875985.html:text/html}
}

@article{ferreira_visual_2013,
	title = {Visual {Exploration} of {Big} {Spatio}-{Temporal} {Urban} {Data}: {A} {Study} of {New} {York} {City} {Taxi} {Trips}},
	volume = {19},
	issn = {2160-9306},
	shorttitle = {Visual {Exploration} of {Big} {Spatio}-{Temporal} {Urban} {Data}},
	doi = {10.1109/TVCG.2013.226},
	abstract = {As increasing volumes of urban data are captured and become available, new opportunities arise for data-driven analysis that can lead to improvements in the lives of citizens through evidence-based decision making and policies. In this paper, we focus on a particularly important urban data set: taxi trips. Taxis are valuable sensors and information associated with taxi trips can provide unprecedented insight into many different aspects of city life, from economic activity and human behavior to mobility patterns. But analyzing these data presents many challenges. The data are complex, containing geographical and temporal components in addition to multiple variables associated with each trip. Consequently, it is hard to specify exploratory queries and to perform comparative analyses (e.g., compare different regions over time). This problem is compounded due to the size of the data-there are on average 500,000 taxi trips each day in NYC. We propose a new model that allows users to visually query taxi trips. Besides standard analytics queries, the model supports origin-destination queries that enable the study of mobility across the city. We show that this model is able to express a wide range of spatio-temporal queries, and it is also flexible in that not only can queries be composed but also different aggregations and visual representations can be applied, allowing users to explore and compare results. We have built a scalable system that implements this model which supports interactive response times; makes use of an adaptive level-of-detail rendering strategy to generate clutter-free visualization for large results; and shows hidden details to the users in a summary through the use of overlay heat maps. We present a series of case studies motivated by traffic engineers and economists that show how our model and system enable domain experts to perform tasks that were previously unattainable for them.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Ferreira, Nivan and Poco, Jorge and Vo, Huy T. and Freire, Juliana and Silva, Cláudio T.},
	month = dec,
	year = {2013},
	keywords = {Type of work: system / technique, HOW: interactive query construction, Maybe related. User interactions result in constraints to a query, WHY: analyze large scale Taxi data, ENCODING - Model},
	pages = {2149--2158},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\WRDZRZAH\\6634127.html:text/html}
}

@article{shadoan_visual_2013,
	title = {Visual {Analysis} of {Higher}-{Order} {Conjunctive} {Relationships} in {Multidimensional} {Data} {Using} a {Hypergraph} {Query} {System}},
	volume = {19},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2013.220},
	abstract = {Visual exploration and analysis of multidimensional data becomes increasingly difficult with increasing dimensionality. We want to understand the relationships between dimensions of data, but lack flexible techniques for exploration beyond low-order relationships. Current visual techniques for multidimensional data analysis focus on binary conjunctive relationships between dimensions. Recent techniques, such as cross-filtering on an attribute relationship graph, facilitate the exploration of some higher-order conjunctive relationships, but require a great deal of care and precision to do so effectively. This paper provides a detailed analysis of the expressive power of existing visual querying systems and describes a more flexible approach in which users can explore n-ary conjunctive inter- and intra- dimensional relationships by interactively constructing queries as visual hypergraphs. In a hypergraph query, nodes represent subsets of values and hyperedges represent conjunctive relationships. Analysts can dynamically build and modify the query using sequences of simple interactions. The hypergraph serves not only as a query specification, but also as a compact visual representation of the interactive state. Using examples from several domains, focusing on the digital humanities, we describe the design considerations for developing the querying system and incorporating it into visual analysis tools. We analyze query expressiveness with regard to the kinds of questions it can and cannot pose, and describe how it simultaneously expands the expressiveness of and is complemented by cross-filtering.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Shadoan, Rachel and Weaver, Chris},
	month = dec,
	year = {2013},
	keywords = {HOW: interactive query construction, Type of Work: Technique \& Algorithm, Construct a user's query as a hypergraph, Emphasis on cross-filtering interactions, Query can be edited and re-used, WHY: query reuseencoding - graph, how - other (generate query, represent concept, find similar concepts), HOW: interactive query construction, Query can be edited and re-used, Type of Work: Technique \& Algorithm, WHY - Explain ML Model / Debug Algorithm / Query Plan, encoding - graph, how - other (generateWHY: query, represent concept, find similar concepts)use},
	pages = {2070--2079},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\G4UYE2L3\\6634154.html:text/html}
}

@article{kodagoda_using_2013,
	title = {Using {Interactive} {Visual} {Reasoning} to {Support} {Sense}-{Making}: {Implications} for {Design}},
	volume = {19},
	issn = {2160-9306},
	shorttitle = {Using {Interactive} {Visual} {Reasoning} to {Support} {Sense}-{Making}},
	doi = {10.1109/TVCG.2013.211},
	abstract = {This research aims to develop design guidelines for systems that support investigators and analysts in the exploration and assembly of evidence and inferences. We focus here on the problem of identifying candidate 'influencers' within a community of practice. To better understand this problem and its related cognitive and interaction needs, we conducted a user study using a system called INVISQUE (INteractive Visual Search and QUery Environment) loaded with content from the ACM Digital Library. INVISQUE supports search and manipulation of results over a freeform infinite 'canvas'. The study focuses on the representations user create and their reasoning process. It also draws on some pre-established theories and frameworks related to sense-making and cognitive work in general, which we apply as a 'theoretical lenses' to consider findings and articulate solutions. Analysing the user-study data in the light of these provides some understanding of how the high-level problem of identifying key players within a domain can translate into lower-level questions and interactions. This, in turn, has informed our understanding of representation and functionality needs at a level of description which abstracts away from the specifics of the problem at hand to the class of problems of interest. We consider the study outcomes from the perspective of implications for design.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Kodagoda, Neesha and Attfield, Simon and Wong, B.L. William and Rooney, Chris and Choudhury, Sharmin},
	month = dec,
	year = {2013},
	keywords = {WHEN - Real-Time Applications, HOW - Pattern Analysis, Type of Work: Tool/Software, HOW: pattern analysis, WHY: sensemaking, WHY: visualize history, WHY - User Behavior / User Characteristics / User Modelling},
	pages = {2217--2226},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\2BPH55B4\\6651935.html:text/html}
}

@article{hajizadeh_supporting_2013,
	title = {Supporting awareness through collaborative brushing and linking of tabular data},
	volume = {19},
	number = {12},
	journal = {IEEE transactions on visualization and computer graphics},
	author = {Hajizadeh, Amir Hossein and Tory, Melanie and Leung, Rock},
	year = {2013},
	pages = {2189--2197},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\GTJK3YU9\\Hajizadeh et al. - 2013 - Supporting awareness through collaborative brushin.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\Q5WBPLN5\\6634130.html:text/html}
}

@article{hu_semantics_2013,
	title = {Semantics of directly manipulating spatializations},
	volume = {19},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Hu, Xinran and Bradel, Lauren and Maiti, Dipayan and House, Leanna and North, Chris},
	year = {2013},
	keywords = {Type of Work: Technique, HOW: semantic interaction, WHY: model steering},
	pages = {2052--2059},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\5VZWZH6C\\Hu et al. - 2013 - Semantics of directly manipulating spatializations.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\JNJG5JFI\\6634115.html:text/html}
}

@ARTICLE{snapshot-to-point, author={S. {van den Elzen} and D. {Holten} and J. {Blaas} and J. J. {van Wijk}}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={Reducing Snapshots to Points: A Visual Analytics Approach to Dynamic Network Exploration}, year={2016}, volume={22}, number={1}, pages={1-10},}

@article{zhao_interactive_2013,
	title = {Interactive exploration of implicit and explicit relations in faceted datasets},
	volume = {19},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Zhao, Jian and Collins, Christopher and Chevalier, Fanny and Balakrishnan, Ravin},
	year = {2013},
	pages = {2080--2089},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\CEL3JZCC\\Zhao et al. - 2013 - Interactive exploration of implicit and explicit r.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\8YDXHGQ9\\6634163.html:text/html}
}

@article{willett_identifying_2013,
	title = {Identifying redundancy and exposing provenance in crowdsourced data analysis},
	volume = {19},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Willett, Wesley and Ginosar, Shiry and Steinitz, Avital and Hartmann, Björn and Agrawala, Maneesh},
	year = {2013},
	keywords = {WHEN - Real-Time Applications, Type of Work: Case Study, Type of Work: Tool/Software, HOW - Classification Models, HOW: clustering of explanations of analysis finding, Type, WHY: crowdsourcing / collaborative analysis, WHY - Report Generation / Storytelling},
	pages = {2198--2206},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\YL2884GF\\Willett et al. - 2013 - Identifying redundancy and exposing provenance in .pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\V78A4REK\\6634191.html:text/html}
}

@article{broeksema_decision_2013,
	title = {Decision exploration lab: {A} visual analytics solution for decision management},
	volume = {19},
	shorttitle = {Decision exploration lab},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Broeksema, Bertjan and Baudel, Thomas and Telea, Alex and Crisafulli, Paolo},
	year = {2013},
	pages = {1972--1981},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\LKQ36FTV\\Broeksema et al. - 2013 - Decision exploration lab A visual analytics solut.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\Z3CMFVZJ\\6634184.html:text/html}
}

@article{borkin_evaluation_2013,
	title = {Evaluation of filesystem provenance visualization tools},
	volume = {19},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Borkin, Michelle A. and Yeh, Chelsea S. and Boyd, Madelaine and Macko, Peter and Gajos, Krzysztof Z. and Seltzer, Margo and Pfister, Hanspeter},
	year = {2013},
	keywords = {Type of Work: User Study, HOW: ???, WHY: visualize history, Maybe related. Visualization of provenance of filesystem usage},
	pages = {2476--2485},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\KLM8JNEK\\Borkin et al. - 2013 - Evaluation of filesystem provenance visualization .pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\BVH3A6HK\\6634189.html:text/html}
}

@inproceedings{gou_socialnetsense:_2012,
	title = {{SocialNetSense}: {Supporting} sensemaking of social and structural features in networks with interactive visualization},
	shorttitle = {{SocialNetSense}},
	booktitle = {2012 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	publisher = {IEEE},
	author = {Gou, Liang and Zhang, Xiaolong and Luo, Airong and Anderson, Patricia F.},
	year = {2012},
	keywords = {Type of Work: Tool/Software, HOW: ???, WHY: visualize history, WHY: go back to previous state},
	pages = {133--142},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\PZJXWHAY\\Gou et al. - 2012 - SocialNetSense Supporting sensemaking of social a.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\IEJTVTF7\\6400558.html:text/html}
}

@inproceedings{mistelbauer_smart_2012,
	title = {Smart super views—{A} knowledge-assisted interface for medical visualization},
	booktitle = {2012 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	publisher = {IEEE},
	author = {Mistelbauer, Gabriel and Köchl, Arnold and Schernthaner, Rudiger and Baclija, Ivan and Schernthaner, Rüdiger and Bruckner, Stefan and Sramek, Milos and Gröller, Meister Eduard},
	year = {2012},
	pages = {163--172},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\JNQUQZL7\\Mistelbauer et al. - 2012 - Smart super views—A knowledge-assisted interface f.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\SFTRBYLR\\6400555.html:text/html}
}

@inproceedings{angelini_information_2012,
	title = {Information retrieval failure analysis: {Visual} analytics as a support for interactive “what-if” investigation},
	shorttitle = {Information retrieval failure analysis},
	booktitle = {2012 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	publisher = {IEEE},
	author = {Angelini, Marco and Ferro, Nicola and Granato, Guido and Santucci, Guiseppe and Silvello, Gianmaria},
	year = {2012},
	pages = {204--206},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\TF8IXH6H\\Angelini et al. - 2012 - Information retrieval failure analysis Visual ana.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\X65BVQMJ\\6400551.html:text/html}
}

@inproceedings{wei_visual_2012,
	title = {Visual cluster exploration of web clickstream data},
	booktitle = {2012 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	publisher = {IEEE},
	author = {Wei, Jishang and Shen, Zeqian and Sundaresan, Neel and Ma, Kwan-Liu},
	year = {2012},
	keywords = {HOW - Pattern Analysis, WHEN - Hybrid Approaches, Type of work: system / technique, HOW: clustering, WHY: analyze clickstream data, HOW - Probabilistic Models / Prediction, HOW - Classification Models, WHY - User Behavior / User Characteristics / User Modelling, ENCODING - Grammar},
	pages = {3--12},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\WQSM6Y4T\\Wei et al. - 2012 - Visual cluster exploration of web clickstream data.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\IV7MSJ3F\\6400494.html:text/html}
}

@inproceedings{hoferlin_inter-active_2012,
	title = {Inter-active learning of ad-hoc classifiers for video visual analytics},
	booktitle = {2012 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	publisher = {IEEE},
	author = {Höferlin, Benjamin and Netzel, Rudolf and Höferlin, Markus and Weiskopf, Daniel and Heidemann, Gunther},
	year = {2012},
	pages = {23--32},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\M9WT5NQP\\Höferlin et al. - 2012 - Inter-active learning of ad-hoc classifiers for vi.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\L9D2XNYQ\\6400492.html:text/html}
}

@inproceedings{kandogan_just--time_2012,
	title = {Just-in-time annotation of clusters, outliers, and trends in point-based data visualizations},
	booktitle = {2012 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	publisher = {IEEE},
	author = {Kandogan, Eser},
	year = {2012},
	pages = {73--82},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\J9UNAYGK\\Kandogan - 2012 - Just-in-time annotation of clusters, outliers, and.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\VC6BEZKA\\6400487.html:text/html}
}

@inproceedings{brown_dis-function:_2012,
	title = {Dis-function: {Learning} distance functions interactively},
	shorttitle = {Dis-function},
	booktitle = {2012 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	publisher = {IEEE},
	author = {Brown, Eli T. and Liu, Jingjing and Brodley, Carla E. and Chang, Remco},
	year = {2012},
	keywords = {WHEN - Real-Time Applications, HOW: semantic interaction, WHY: model steering, HOW - Classification Models, Type of Work: Technique \& Algorithm, WHY - Model Steering / Active Learning, ENCODING - Vector},
	pages = {83--92},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\8B5NTNP8\\Brown et al. - 2012 - Dis-function Learning distance functions interact.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\WC5CP8CL\\6400486.html:text/html}
}

@article{pohl_user_2012,
	title = {The user puzzle—explaining the interaction with visual analytics systems},
	volume = {18},
	number = {12},
	journal = {IEEE transactions on visualization and computer graphics},
	author = {Pohl, Margit and Smuc, Michael and Mayr, Eva},
	year = {2012},
	pages = {2908--2916},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\7KSUBUH4\\Pohl et al. - 2012 - The user puzzle—explaining the interaction with vi.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\BY3BBW35\\6327297.html:text/html}
}

@article{hossain_scatter/gather_2012,
	title = {Scatter/gather clustering: {Flexibly} incorporating user feedback to steer clustering results},
	volume = {18},
	shorttitle = {Scatter/gather clustering},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Hossain, M. Shahriar and Ojili, Praveen Kumar Reddy and Grimm, Cindy and Müller, Rolf and Watson, Layne T. and Ramakrishnan, Naren},
	year = {2012},
	keywords = {Type of work: system / technique, WHY: model steering, HOW: user interactions used as constraints, Maybe related... User interaction is used as constraints in clustering, HOW - Classification Models},
	pages = {2829--2838},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\2MIZ3889\\Hossain et al. - 2012 - Scattergather clustering Flexibly incorporating .pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\R2ZPN7PN\\6327289.html:text/html}
}

@article{dork_pivotpaths:_2012,
	title = {Pivotpaths: {Strolling} through faceted information spaces},
	volume = {18},
	shorttitle = {Pivotpaths},
	number = {12},
	journal = {IEEE transactions on visualization and computer graphics},
	author = {Dörk, Marian and Riche, Nathalie Henry and Ramos, Gonzalo and Dumais, Susan},
	year = {2012},
	pages = {2709--2718},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\KQ9TP8HN\\Dörk et al. - 2012 - Pivotpaths Strolling through faceted information .pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\4LRK4Q4G\\6327277.html:text/html}
}

@misc{noauthor_pivotpaths:_nodate,
	title = {{PivotPaths}: {Strolling} through {Faceted} {Information} {Spaces} - {IEEE} {Journals} \& {Magazine}},
	url = {https://ieeexplore.ieee.org/abstract/document/6327277},
	urldate = {2019-12-12},
	file = {PivotPaths\: Strolling through Faceted Information Spaces - IEEE Journals & Magazine:C\:\\Users\\conny\\Zotero\\storage\\U4AAK6AG\\6327277.html:text/html}
}

@article{tominski_interaction_2012,
	title = {Interaction support for visual comparison inspired by natural behavior},
	volume = {18},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Tominski, Christian and Forsell, Camilla and Johansson, Jimmy},
	year = {2012},
	pages = {2719--2728},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\S7KE45N3\\Tominski et al. - 2012 - Interaction support for visual comparison inspired.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\CRJY7ZPI\\6327278.html:text/html}
}

@inproceedings{canfield_interactive_2011,
	title = {Interactive data analysis with {nSpace2}®},
	booktitle = {2011 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	publisher = {IEEE},
	author = {Canfield, Casey M. and Sheffield, David},
	year = {2011},
	pages = {327--328},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\SDBK29MY\\Canfield and Sheffield - 2011 - Interactive data analysis with nSpace2®.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\HFNEYX4Y\\6102497.html:text/html}
}

@inproceedings{jeong_state_2011,
	title = {A state transition approach to understanding users' interactions},
	booktitle = {2011 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	publisher = {IEEE},
	author = {Jeong, Dong Hyun and Ji, Soo-Yeon and Ribarsky, William and Chang, Remco},
	year = {2011},
	pages = {285--286},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\8Z6WZYSR\\Jeong et al. - 2011 - A state transition approach to understanding users.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\HQ7KLKIT\\6102476.html:text/html}
}

@inproceedings{harrison_analysts_2011,
	title = {Analysts aren't machines: {Inferring} frustration through visualization interaction},
	shorttitle = {Analysts aren't machines},
	booktitle = {2011 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	publisher = {IEEE},
	author = {Harrison, Lane and Dou, Wenwen and Lu, Aidong and Ribarsky, William and Wang, Xiaoyu},
	year = {2011},
	pages = {279--280},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\5ZUJHCE3\\Harrison et al. - 2011 - Analysts aren't machines Inferring frustration th.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\Y85FSQVZ\\6102473.html:text/html}
}

@inproceedings{cottam_reasonable_2011,
	title = {Reasonable abstractions: {Semantics} for dynamic data visualization},
	shorttitle = {Reasonable abstractions},
	booktitle = {2011 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	publisher = {IEEE},
	author = {Cottam, Joseph A. and Lumsdaine, Andrew},
	year = {2011},
	pages = {269--270},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\TIMFZ4TB\\Cottam and Lumsdaine - 2011 - Reasonable abstractions Semantics for dynamic dat.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\VXUAWZIJ\\6102468.html:text/html}
}

@inproceedings{chen_supporting_2011,
	title = {Supporting effective common ground construction in asynchronous collaborative visual analytics},
	booktitle = {2011 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	publisher = {IEEE},
	author = {Chen, Yang and Alsakran, Jamal and Barlowe, Scott and Yang, Jing and Zhao, Ye},
	year = {2011},
	pages = {101--110},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\LUR6RPHS\\Chen et al. - 2011 - Supporting effective common ground construction in.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\HNI6TDZK\\6102447.html:text/html}
}

@inproceedings{lipford_helping_2010,
	title = {Helping users recall their reasoning process},
	booktitle = {2010 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	publisher = {IEEE},
	author = {Lipford, Heather Richter and Stukes, Felesia and Dou, Wenwen and Hawkins, Matthew E. and Chang, Remco},
	year = {2010},
	keywords = {Type of Work: User Study, HOW: study, WHY: recall of past actions},
	pages = {187--194},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\B7Q3VRQW\\Lipford et al. - 2010 - Helping users recall their reasoning process.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\L2PLRSM9\\5653598.html:text/html}
}

@inproceedings{green_towards_2010,
	title = {Towards the personal equation of interaction: {The} impact of personality factors on visual analytics interface interaction},
	shorttitle = {Towards the personal equation of interaction},
	booktitle = {2010 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	publisher = {IEEE},
	author = {Green, Tear Marie and Fisher, Brian},
	year = {2010},
	pages = {203--210},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\2WF7PGZI\\Green and Fisher - 2010 - Towards the personal equation of interaction The .pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\8EJ6L69Q\\5653587.html:text/html}
}

@inproceedings{chung_vizcept:_2010,
	title = {Vizcept: {Supporting} synchronous collaboration for constructing visualizations in intelligence analysis},
	shorttitle = {Vizcept},
	booktitle = {2010 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	publisher = {IEEE},
	author = {Chung, Haeyong and Yang, Seungwon and Massjouni, Naveed and Andrews, Christopher and Kanna, Rahul and North, Chris},
	year = {2010},
	keywords = {WHY - Evaluation of Tools and Systems, Type of Work: Tool/Software, HOW: aggregation of user-found concepts into a concept map, WHY: collaborative analysis, HOW - Classification Models, WHY - Report Generation / Storytelling, WHY - Real-time or post-hoc Quantification and Re-Application, ENCODING - Grammar},
	pages = {107--114},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\QTMKTCJ8\\Chung et al. - 2010 - Vizcept Supporting synchronous collaboration for .pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\G6LU5VAM\\5652932.html:text/html}
}

@inproceedings{liu_netclinic:_2010,
	title = {Netclinic: {Interactive} visualization to enhance automated fault diagnosis in enterprise networks},
	shorttitle = {Netclinic},
	booktitle = {2010 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	publisher = {IEEE},
	author = {Liu, Zhicheng and Lee, Bongshin and Kandula, Srikanth and Mahajan, Ratul},
	year = {2010},
	pages = {131--138},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\EXHYJHIS\\Liu et al. - 2010 - Netclinic Interactive visualization to enhance au.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\WWSSHVT3\\5652910.html:text/html}
}

@inproceedings{chen_click2annotate:_2010,
	title = {Click2annotate: {Automated} insight externalization with rich semantics},
	shorttitle = {Click2annotate},
	booktitle = {2010 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	publisher = {IEEE},
	author = {Chen, Yang and Barlowe, Scott and Yang, Jing},
	year = {2010},
	pages = {155--162},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\3458SD2I\\Chen et al. - 2010 - Click2annotate Automated insight externalization .pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\482TM5SS\\5652885.html:text/html}
}

@inproceedings{garg_visual_2010,
	title = {A visual analytics approach to model learning},
	booktitle = {2010 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	publisher = {IEEE},
	author = {Garg, Supriya and Ramakrishnan, I. V. and Mueller, Klaus},
	year = {2010},
	pages = {67--74},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\HBU77DB6\\Garg et al. - 2010 - A visual analytics approach to model learning.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\UPNMXDV4\\5652484.html:text/html}
}

@inproceedings{gutman_enhancing_2010,
	title = {Enhancing text-based chat with visuals for hazardous weather decision making},
	doi = {10.1109/VAST.2010.5650815},
	abstract = {We created a visual chat application for use during hazardous weather events. The application, NWSChat2, allows National Weather Service forecasters, media members, and storm trackers to communicate with each other, basing their conversation on a common shared radar map of the storm. Users can additionally annotate the map with `pins' or draw notes with a stylus. These annotations are automatically shared with all other users. The collaborative nature of NWSChat2 makes it well-suited for disseminating information to all users during weather emergencies.},
	booktitle = {2010 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	author = {Gutman, Moshe and Eosco, Gina and Zappa, Monica and Weaver, Chris},
	month = oct,
	year = {2010},
	note = {ISSN: null},
	pages = {225--226},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\ECX32RVY\\5650815.html:text/html}
}

@inproceedings{nafari_poster:_2010,
	title = {Poster: {Translating} cross-filtered queries into questions},
	shorttitle = {Poster},
	doi = {10.1109/VAST.2010.5650251},
	abstract = {Complex combinations of coordinated multiple views are increasingly used to design tools for highly interactive visual exploration and analysis of multidimensional data. While complex coordination patterns provide substantial utility through expressive querying, they also exhibit usability problems for users when learning required interaction sequences, recalling past queries, and interpreting visual states. As visual analysis tools grow more sophisticated, there is a growing need to make them more understandable as well. Our long-term goal is to exploit natural language familiarity and literacy to directly facilitate individual and collaborative use of visual analysis tools. In this poster, we present work in progress on an automatically generated query-to-question user interface to translate interactive states during visual analysis into an accompanying visual log of formatted text. Our effort currently focuses on a symmetric and thus relatively simple coordination pattern: cross-filtered views. We describe our current thinking about query-to-question translation in a typical cross-filtered visualization of movies, people, and genres in the Internet Movie Database.},
	booktitle = {2010 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	author = {Nafari, Maryam and Weaver, Chris},
	month = oct,
	year = {2010},
	note = {ISSN: null},
	pages = {245--246},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\ESW98MPP\\5650251.html:text/html}
}

@article{liu_mental_2010,
	title = {Mental {Models}, {Visual} {Reasoning} and {Interaction} in {Information} {Visualization}: {A} {Top}-down {Perspective}},
	volume = {16},
	issn = {2160-9306},
	shorttitle = {Mental {Models}, {Visual} {Reasoning} and {Interaction} in {Information} {Visualization}},
	doi = {10.1109/TVCG.2010.177},
	abstract = {Although previous research has suggested that examining the interplay between internal and external representations can benefit our understanding of the role of information visualization (InfoVis) in human cognitive activities, there has been little work detailing the nature of internal representations, the relationship between internal and external representations and how interaction is related to these representations. In this paper, we identify and illustrate a specific kind of internal representation, mental models, and outline the high-level relationships between mental models and external visualizations. We present a top-down perspective of reasoning as model construction and simulation, and discuss the role of visualization in model based reasoning. From this perspective, interaction can be understood as active modeling for three primary purposes: external anchoring, information foraging, and cognitive offloading. Finally we discuss the implications of our approach for design, evaluation and theory development.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Liu, Zhicheng and Stasko, John},
	month = nov,
	year = {2010},
	keywords = {Maybe related. This paper lists "types of interactions"},
	pages = {999--1008},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\MULEKKD9\\5613437.html:text/html}
}

@article{fuchs_visual_2009,
	title = {Visual {Human}+{Machine} {Learning}},
	volume = {15},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2009.199},
	abstract = {In this paper we describe a novel method to integrate interactive visual analysis and machine learning to support the insight generation of the user. The suggested approach combines the vast search and processing power of the computer with the superior reasoning and pattern recognition capabilities of the human user. An evolutionary search algorithm has been adapted to assist in the fuzzy logic formalization of hypotheses that aim at explaining features inside multivariate, volumetric data. Up to now, users solely rely on their knowledge and expertise when looking for explanatory theories. However, it often remains unclear whether the selected attribute ranges represent the real explanation for the feature of interest. Other selections hidden in the large number of data variables could potentially lead to similar features. Moreover, as simulation complexity grows, users are confronted with huge multidimensional data sets making it almost impossible to find meaningful hypotheses at all. We propose an interactive cycle of knowledge-based analysis and automatic hypothesis generation. Starting from initial hypotheses, created with linking and brushing, the user steers a heuristic search algorithm to look for alternative or related hypotheses. The results are analyzed in information visualization views that are linked to the volume rendering. Individual properties as well as global aggregates are visually presented to provide insight into the most relevant aspects of the generated hypotheses. This novel approach becomes computationally feasible due to a GPU implementation of the time-critical parts in the algorithm. A thorough evaluation of search times and noise sensitivity as well as a case study on data from the automotive domain substantiate the usefulness of the suggested approach.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Fuchs, Raphael and Waser, Jürgen and Groller, Meister Eduard},
	month = nov,
	year = {2009},
	keywords = {WHEN - Real-Time Applications, WHY - Model Steering, Type of work: system / technique, HOW: multiple user demonstrations / interactions are used in a genetic algorithm search, WHY: Real-time steering of machine learning (for CFD), HOW - Probabilistic Models / Prediction, ENCODING - Image},
	pages = {1327--1334},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\XBXG7TZF\\5290745.html:text/html}
}

@inproceedings{koch_iterative_2009,
	title = {Iterative integration of visual insights during patent search and analysis},
	booktitle = {2009 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	publisher = {IEEE},
	author = {Koch, Steffen and Bosch, Harald and Giereth, Mark and Ertl, Thomas},
	year = {2009},
	keywords = {HOW - Program Synthesis, WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, WHY: resuse of previous search queries in patent analysis, WHY - Adaptive Systems / Guidance, Type of Work: System, ENCODING - Grammar (boolean logic)},
	pages = {203--210},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\WDQRGNVU\\Koch et al. - 2009 - Iterative integration of visual insights during pa.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\F7IDN2KC\\5333564.html:text/html}
}

@inproceedings{sun_articulate:_2009,
	title = {Articulate: a conversational interface for visual analytics},
	shorttitle = {Articulate},
	booktitle = {2009 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	publisher = {IEEE},
	author = {Sun, Yiwen and Leigh, Jason and Johnson, Andrew and Chau, Dennis},
	year = {2009},
	pages = {233--234},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\5HBNESVD\\Sun et al. - 2009 - Articulate a conversational interface for visual .pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\N2SX99A7\\5333099.html:text/html}
}

@inproceedings{wongsuphasawat_finding_2009,
	title = {Finding comparable temporal categorical records: {A} similarity measure with an interactive visualization},
	shorttitle = {Finding comparable temporal categorical records},
	booktitle = {2009 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	publisher = {IEEE},
	author = {Wongsuphasawat, Krist and Shneiderman, Ben},
	year = {2009},
	pages = {27--34},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\MRTYXWZS\\Wongsuphasawat and Shneiderman - 2009 - Finding comparable temporal categorical records A.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\XU5W3YL7\\5332595.html:text/html}
}

@article{bresciani_benefits_2009,
	title = {The benefits of synchronous collaborative information visualization: {Evidence} from an experimental evaluation},
	volume = {15},
	shorttitle = {The benefits of synchronous collaborative information visualization},
	number = {6},
	journal = {IEEE transactions on visualization and computer graphics},
	author = {Bresciani, Sabrina and Eppler, Martin J.},
	year = {2009},
	keywords = {WHY: synchronous collaboration},
	pages = {1073--1080},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\UBNS5NBT\\Bresciani and Eppler - 2009 - The benefits of synchronous collaborative informat.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\HPANNX6S\\5290714.html:text/html}
}

@article{tobiasz_lark:_2009,
	title = {Lark: {Coordinating} co-located collaboration with information visualization},
	volume = {15},
	shorttitle = {Lark},
	number = {6},
	journal = {IEEE transactions on visualization and computer graphics},
	author = {Tobiasz, Matthew and Isenberg, Petra and Carpendale, Sheelagh},
	year = {2009},
	keywords = {Type of Work: Tool/Software, WHY: collaborative analysis, Can clone past sequence of user actions. No analysis of provenance info, HOW: ???},
	pages = {1065--1072},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\S6IBUYKC\\Tobiasz et al. - 2009 - Lark Coordinating co-located collaboration with i.pdf:application/pdf;Snapshot:C\:\\Users\\conny\\Zotero\\storage\\BF4R9LJ4\\5290713.html:text/html}
}

@article{weaver_conjunctive_2009,
	title = {Conjunctive {Visual} {Forms}},
	volume = {15},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2009.129},
	abstract = {Visual exploration of multidimensional data is a process of isolating and extracting relationships within and between dimensions. Coordinated multiple view approaches are particularly effective for visual exploration because they support precise expression of heterogeneous multidimensional queries using simple interactions. Recent visual analytics research has made significant progress in identifying and understanding patterns of composed views and coordinations that support fast, flexible, and open-ended data exploration. What is missing is formalization of the space of expressible queries in terms of visual representation and interaction. This paper introduces the conjunctive visual form model in which visual exploration consists of interactively-driven sequences of transitions between visual states that correspond to conjunctive normal forms in boolean logic. The model predicts several new and useful ways to extend the space of rapidly expressible queries through addition of simple interactive capabilities to existing compositional patterns. Two recent related visual tools offer a subset of these capabilities, providing a basis for conjecturing about such extensions.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Weaver, Chris},
	month = nov,
	year = {2009},
	keywords = {WHY - Evaluation of Tools and Systems, HOW: program synthesis, Conjuctive normal form (boolean logic) representation of user interactions, Type of Work: Theory, WHY: ???, ENCODING - Grammar (boolean logic)},
	pages = {929--936},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\ZRGMYUYZ\\5290696.html:text/html}
}

@article{andrews_system_2012,
	title = {System {Personality} and {Persuasion} in {Human}-{Computer} {Dialogue}},
	volume = {2},
	issn = {2160-6455},
	url = {http://doi.acm.org/10.1145/2209310.2209315},
	doi = {10.1145/2209310.2209315},
	abstract = {The human-computer dialogue research field has been studying interaction with computers since the early stage of Artificial Intelligence, however, research has often focused on very practical tasks to be completed with the dialogues. A new trend in the field tries to implement persuasive techniques with automated interactive agents; unlike booking a train ticket, for example, such dialogues require the system to show more anthropomorphic qualities. The influences of such qualities in the effectiveness of persuasive dialogue is only starting to be studied. In this article we focus on one important perceived trait of the system: personality, and explore how it influences the persuasiveness of a dialogue system. We introduce a new persuasive dialogue system and combine it with a state of the art personality utterance generator. By doing so, we can control the system’s extraversion personality trait and observe its influence on the user’s perception of the dialogue and its output. In particular, we observe that the user’s extraversion influences their perception of the dialogue and its persuasiveness, and that the perceived personality of the system can affect its trustworthiness and persuasiveness. We believe that theses observations will help to set up guidelines to tailor dialogue systems to the user’s interaction expectations and improve the persuasive interventions.},
	number = {2},
	urldate = {2019-12-12},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Andrews, Pierre Y.},
	month = jun,
	year = {2012},
	pages = {12:1--12:27},
	file = {ACM Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\47VUGI9K\\Andrews - 2012 - System Personality and Persuasion in Human-Compute.pdf:application/pdf}
}

@article{cremonesi_investigating_2012,
	title = {Investigating the {Persuasion} {Potential} of {Recommender} {Systems} from a {Quality} {Perspective}: {An} {Empirical} {Study}},
	volume = {2},
	issn = {2160-6455},
	shorttitle = {Investigating the {Persuasion} {Potential} of {Recommender} {Systems} from a {Quality} {Perspective}},
	url = {http://doi.acm.org/10.1145/2209310.2209314},
	doi = {10.1145/2209310.2209314},
	abstract = {Recommender Systems (RSs) help users search large amounts of digital contents and services by allowing them to identify the items that are likely to be more attractive or useful. RSs play an important persuasion role, as they can potentially augment the users’ trust towards in an application and orient their decisions or actions towards specific directions. This article explores the persuasiveness of RSs, presenting two vast empirical studies that address a number of research questions. First, we investigate if a design property of RSs, defined by the statistically measured quality of algorithms, is a reliable predictor of their potential for persuasion. This factor is measured in terms of perceived quality, defined by the overall satisfaction, as well as by how users judge the accuracy and novelty of recommendations. For our purposes, we designed an empirical study involving 210 subjects and implemented seven full-sized versions of a commercial RS, each one using the same interface and dataset (a subset of Netflix), but each with a different recommender algorithm. In each experimental configuration we computed the statistical quality (recall and F-measures) and collected data regarding the quality perceived by 30 users. The results show us that algorithmic attributes are less crucial than we might expect in determining the user’s perception of an RS’s quality, and suggest that the user’s judgment and attitude towards a recommender are likely to be more affected by factors related to the user experience. Second, we explore the persuasiveness of RSs in the context of large interactive TV services. We report a study aimed at assessing whether measurable persuasion effects (e.g., changes of shopping behavior) can be achieved through the introduction of a recommender. Our data, collected for more than one year, allow us to conclude that, (1) the adoption of an RS can affect both the lift factor and the conversion rate, determining an increased volume of sales and influencing the user’s decision to actually buy one of the recommended products, (2) the introduction of an RS tends to diversify purchases and orient users towards less obvious choices (the long tail), and (3) the perceived novelty of recommendations is likely to be more influential than their perceived accuracy. Overall, the results of these studies improve our understanding of the persuasion phenomena induced by RSs, and have implications that can be of interest to academic scholars, designers, and adopters of this class of systems.},
	number = {2},
	urldate = {2019-12-12},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Cremonesi, Paolo and Garzotto, Franca and Turrin, Roberto},
	month = jun,
	year = {2012},
	pages = {11:1--11:41},
	file = {ACM Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\38U79N7U\\Cremonesi et al. - 2012 - Investigating the Persuasion Potential of Recommen.pdf:application/pdf}
}

@article{berkovsky_influencing_2012,
	title = {Influencing {Individually}: {Fusing} {Personalization} and {Persuasion}},
	volume = {2},
	issn = {2160-6455},
	shorttitle = {Influencing {Individually}},
	url = {http://doi.acm.org/10.1145/2209310.2209312},
	doi = {10.1145/2209310.2209312},
	abstract = {Personalized technologies aim to enhance user experience by taking into account users’ interests, preferences, and other relevant information. Persuasive technologies aim to modify user attitudes, intentions, or behavior through computer-human dialogue and social influence. While both personalized and persuasive technologies influence user interaction and behavior, we posit that this influence could be significantly increased if the two technologies were combined to create personalized and persuasive systems. For example, the persuasive power of a one-size-fits-all persuasive intervention could be enhanced by considering the users being influenced and their susceptibility to the persuasion being offered. Likewise, personalized technologies could cash in on increased success, in terms of user satisfaction, revenue, and user experience, if their services used persuasive techniques. Hence, the coupling of personalization and persuasion has the potential to enhance the impact of both technologies. This new, developing area clearly offers mutual benefits to both research areas, as we illustrate in this special issue.},
	number = {2},
	urldate = {2019-12-12},
	journal = {ACM Trans. Interact. Intell. Syst.},
	editor = {Berkovsky, Shlomo and Freyne, Jill and Oinas-Kukkonen, Harri},
	month = jun,
	year = {2012},
	pages = {9:1--9:8},
	file = {ACM Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\FE9MUEL7\\Berkovsky et al. - 2012 - Influencing Individually Fusing Personalization a.pdf:application/pdf}
}

@inproceedings{gao_datatone:_2015,
	address = {Daegu, Kyungpook, Republic of Korea},
	title = {{DataTone}: {Managing} {Ambiguity} in {Natural} {Language} {Interfaces} for {Data} {Visualization}},
	isbn = {978-1-4503-3779-3},
	shorttitle = {{DataTone}},
	url = {http://dl.acm.org/citation.cfm?doid=2807442.2807478},
	doi = {10.1145/2807442.2807478},
	abstract = {Answering questions with data is a difﬁcult and timeconsuming process. Visual dashboards and templates make it easy to get started, but asking more sophisticated questions often requires learning a tool designed for expert analysts. Natural language interaction allows users to ask questions directly in complex programs without having to learn how to use an interface. However, natural language is often ambiguous. In this work we propose a mixed-initiative approach to managing ambiguity in natural language interfaces for data visualization. We model ambiguity throughout the process of turning a natural language query into a visualization and use algorithmic disambiguation coupled with interactive ambiguity widgets. These widgets allow the user to resolve ambiguities by surfacing system decisions at the point where the ambiguity matters. Corrections are stored as constraints and inﬂuence subsequent queries. We have implemented these ideas in a system, DataTone. In a comparative study, we ﬁnd that DataTone is easy to learn and lets users ask questions without worrying about syntax and proper question form.},
	language = {en},
	urldate = {2019-12-19},
	booktitle = {Proceedings of the 28th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} \& {Technology} - {UIST} '15},
	publisher = {ACM Press},
	author = {Gao, Tong and Dontcheva, Mira and Adar, Eytan and Liu, Zhicheng and Karahalios, Karrie G.},
	year = {2015},
	keywords = {WHEN - Real-Time Applications, WHY - Report Generation and Storytelling, WHY - User Behaviour, User Characteristics, User Modelling, WHY - Adaptive Systems / Guidance, Type of Work: Empirical Study, HOW - Classification Models, WHEN - Retrospective Analyses, Type of Work: Technique \& Algorithm},
	pages = {489--500},
	file = {Gao et al. - 2015 - DataTone Managing Ambiguity in Natural Language I.pdf:C\:\\Users\\conny\\Zotero\\storage\\AKQ83SEB\\Gao et al. - 2015 - DataTone Managing Ambiguity in Natural Language I.pdf:application/pdf}
}

@inproceedings{hottelier_programming_2014,
	address = {Honolulu, Hawaii, USA},
	title = {Programming by manipulation for layout},
	isbn = {978-1-4503-3069-5},
	url = {http://dl.acm.org/citation.cfm?doid=2642918.2647378},
	doi = {10.1145/2642918.2647378},
	abstract = {We present Programming by Manipulation, a new programming methodology for specifying the layout of data visualizations, targeted at non-programmers. We address the two central sources of bugs that arise when programming with constraints: ambiguities and conﬂicts (inconsistencies). We rule out conﬂicts by design and exploit ambiguity to explore possible layout designs. Our users design layouts by highlighting undesirable aspects of a current design, effectively breaking spurious constraints and introducing ambiguity by giving some elements freedom to move or resize. Subsequently, the tool indicates how the ambiguity can be removed, by computing how the free elements can be ﬁxed with available constraints. To support this workﬂow, our tool computes the ambiguity and summarizes it visually. We evaluate our work with two user-studies demonstrating that both non-programmers and programmers can effectively use our prototype. Our results suggest that our tool is 5-times more productive than direct programming with constraints.},
	language = {en},
	urldate = {2019-12-19},
	booktitle = {Proceedings of the 27th annual {ACM} symposium on {User} interface software and technology - {UIST} '14},
	publisher = {ACM Press},
	author = {Hottelier, Thibaud and Bodik, Ras and Ryokai, Kimiko},
	year = {2014},
	keywords = {WHEN - Real-Time Applications, WHY - Adaptive Systems / Guidance, Type of Work: Empirical Study, WHEN - Retrospective Analyses, Type of Work: Application \& Design Study, WHY - Model Steering / Active Learning},
	pages = {231--241},
	file = {Hottelier et al. - 2014 - Programming by manipulation for layout.pdf:C\:\\Users\\conny\\Zotero\\storage\\BZSZ5UI8\\Hottelier et al. - 2014 - Programming by manipulation for layout.pdf:application/pdf}
}

@article{bylinskii_learning_2017,
	title = {Learning {Visual} {Importance} for {Graphic} {Designs} and {Data} {Visualizations}},
	url = {http://arxiv.org/abs/1708.02660},
	doi = {10.1145/3126594.3126653},
	abstract = {Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process.},
	urldate = {2019-12-19},
	journal = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology - UIST '17},
	author = {Bylinskii, Zoya and Kim, Nam Wook and O'Donovan, Peter and Alsheikh, Sami and Madan, Spandan and Pfister, Hanspeter and Durand, Fredo and Russell, Bryan and Hertzmann, Aaron},
	year = {2017},
	note = {arXiv: 1708.02660},
	keywords = {WHEN - Real-Time Applications, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, WHEN - Retrospective Analyses, Type of Work: Theory \& Model, WHY - Model Steering / Active Learning},
	pages = {57--69},
	annote = {I'm not sure that this should be included. They build a neural network to infer important portions of an inforgraphic/ static data visualization based on mouse interaction and eye tracking data.},
	file = {arXiv Fulltext PDF:C\:\\Users\\conny\\Zotero\\storage\\6JTAMMLU\\Bylinskii et al. - 2017 - Learning Visual Importance for Graphic Designs and.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\conny\\Zotero\\storage\\UEWDKE7E\\1708.html:text/html}
}

@inproceedings{setlur_eviza:_2016,
	address = {Tokyo, Japan},
	title = {Eviza: {A} {Natural} {Language} {Interface} for {Visual} {Analysis}},
	isbn = {978-1-4503-4189-9},
	shorttitle = {Eviza},
	url = {http://dl.acm.org/citation.cfm?doid=2984511.2984588},
	doi = {10.1145/2984511.2984588},
	abstract = {Natural language interfaces for visualizations have emerged as a promising new way of interacting with data and performing analytics. Many of these systems have fundamental limitations. Most return minimally interactive visualizations in response to queries and often require experts to perform modeling for a set of predicted user queries before the systems are effective. Eviza provides a natural language interface for an interactive query dialog with an existing visualization rather than starting from a blank sheet and asking closed-ended questions that return a single text answer or static visualization. The system employs a probabilistic grammar based approach with predeﬁned rules that are dynamically updated based on the data from the visualization, as opposed to computationally intensive deep learning or knowledge based approaches.},
	language = {en},
	urldate = {2019-12-19},
	booktitle = {Proceedings of the 29th {Annual} {Symposium} on {User} {Interface} {Software} and {Technology} - {UIST} '16},
	publisher = {ACM Press},
	author = {Setlur, Vidya and Battersby, Sarah E. and Tory, Melanie and Gossweiler, Rich and Chang, Angel X.},
	year = {2016},
	keywords = {HOW - Program Synthesis, WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, WHY - Adaptive Systems / Guidance, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, WHY - User Behavior / User Characteristics / User Modelling, Type of Work: System},
	pages = {365--377},
	file = {Setlur et al. - 2016 - Eviza A Natural Language Interface for Visual Ana.pdf:C\:\\Users\\conny\\Zotero\\storage\\4GCVNNT9\\Setlur et al. - 2016 - Eviza A Natural Language Interface for Visual Ana.pdf:application/pdf}
}

@article{bors_capturing_2019,
	title = {Capturing and {Visualizing} {Provenance} {From} {Data} {Wrangling}},
	volume = {39},
	issn = {1558-1756},
	doi = {10.1109/MCG.2019.2941856},
	abstract = {Data quality management and assessment play a vital role for ensuring the trust in the data and its fitness-of-use for subsequent analysis. The transformation history of a data wrangling system is often insufficient for determining the usability of a dataset, lacking information how changes affected the dataset. Capturing workflow provenance along the wrangling process and combining it with descriptive information as data provenance can enable users to comprehend how these changes affected the dataset, and if they benefited data quality. We present DQProv Explorer, a system that captures and visualizes provenance from data wrangling operations. It features three visualization components: allowing the user to explore the provenance graph of operations and the data stream, the development of quality over time for a sequence of wrangling operations applied to the dataset, and the distribution of issues across the entirety of the dataset to determine error patterns.},
	number = {6},
	journal = {IEEE Computer Graphics and Applications},
	author = {Bors, Christian and Gschwandtner, Theresia and Miksch, Silvia},
	month = nov,
	year = {2019},
	keywords = {data quality, WHY - Data Wrangling, WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, Type of Work: Empirical Study, WHY - Real-time or post-hoc Quantification and Re-Application, NO HOW},
	pages = {61--75},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\C35N5Z97\\Bors et al. - 2019 - Capturing and Visualizing Provenance From Data Wra.pdf:application/pdf}
}

@article{borland_contextual_2018,
	title = {Contextual {Visualization}},
	volume = {38},
	issn = {1558-1756},
	doi = {10.1109/MCG.2018.2874782},
	abstract = {Unseen information can lead to various “threats to validity” when analyzing complex datasets using visual tools, resulting in potentially biased findings. We enumerate sources of unseen information and argue that a new focus on contextual visualization methods is needed to inform users of these threats and to mitigate their effects.},
	number = {6},
	journal = {IEEE Computer Graphics and Applications},
	author = {Borland, David and Wang, Wenyuan and Gotz, David},
	month = nov,
	year = {2018},
	keywords = {data quality, analysis quality, WHEN - Real-Time Applications},
	pages = {17--23},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\H7BIUQPK\\Borland et al. - 2018 - Contextual Visualization.pdf:application/pdf}
}

@article{dou_recovering_2009,
	title = {Recovering {Reasoning} {Processes} from {User} {Interactions}},
	volume = {29},
	issn = {1558-1756},
	doi = {10.1109/MCG.2009.49},
	abstract = {Understanding how analysts use visual-analytics (VA) tools can help reveal their reasoning processes when using these tools. By examining analysts' interaction logs, the authors identified the analysts' strategies, methods, and findings when using a financial VA tool.},
	number = {3},
	journal = {IEEE Computer Graphics and Applications},
	author = {Dou, Wenwen and Jeong, Dong Hyun and Stukes, Felesia and Ribarsky, William and Lipford, Heather Richter and Chang, Remco},
	month = may,
	year = {2009},
	keywords = {WHEN - Retrospective Analysis, HOW - Program Synthesis, WHY - User Behaviour, User Characteristics, User Modelling, Type of Work: Empirical Study, HOW - Classification Models},
	pages = {52--61},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\JPLICUCM\\Dou et al. - 2009 - Recovering Reasoning Processes from User Interacti.pdf:application/pdf}
}

@article{shrinivasan_supporting_2009,
	title = {Supporting {Exploration} {Awareness} in {Information} {Visualization}},
	volume = {29},
	issn = {1558-1756},
	doi = {10.1109/MCG.2009.87},
	abstract = {When users want to continue an analysis performed in the past, done by themselves or by a collaborator, they need an overview of what has been done and found so far. Such an overview helps them to gain a shared knowledge about each otherspsila analysis strategy and continue the analysis. We aim to support users in this process, and thereby support their exploration awareness. We present an information visualization framework with three linked processes: overview, search and retrieve for this purpose. First, we present a userpsilas information interest model that captures key aspects of the exploration process. Exploration overview, and keyword and similarity based search mechanisms are designed based on these key aspects. A metadata view is used to visualize the search results and help users to retrieve specific visualizations from past analysis. Finally, we present three case studies and discuss the support offered by the framework for developing exploration awareness.},
	number = {5},
	journal = {IEEE Computer Graphics and Applications},
	author = {Shrinivasan, Yedendra Babu and van Wijk, Jarke},
	month = sep,
	year = {2009},
	keywords = {WHEN - Real-Time Applications, HOW - Pattern Analysis, WHY - Adaptive Systems / Guidance, Type of Work: Empirical Study, WHEN - Retrospective Analyses, WHY - Report Generation / StorytellingHOW - Pattern Analysis, Type of Work: Empirical Study, WHEN - Real-Time Applications, WHEN - Retrospective Analyses, WHY - Adaptive Systems / Guidance, WHY - Real-time or post-hoc Quantification and Re-Application, WHY - Report Generation / Storytelling},
	pages = {34--43},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\D9TQF7LB\\Shrinivasan and van Wijk - 2009 - Supporting Exploration Awareness in Information Vi.pdf:application/pdf}
}

@article{smuc_score_2009,
	title = {To {Score} or {Not} to {Score}? {Tripling} {Insights} for {Participatory} {Design}},
	volume = {29},
	issn = {1558-1756},
	shorttitle = {To {Score} or {Not} to {Score}?},
	doi = {10.1109/MCG.2009.53},
	abstract = {For evaluating visual-analytics tools, many studies confine to scoring user insights into data. For participatory design of those tools, we propose a three-level methodology to make more out of users' insights. The relational insight organizer (RIO) helps to understand how insights emerge and build on one each other. In recent years, computers have also been used to develop visual methods and tools that further support the data analysis process. With the advent of the emerging field of visual analytics (VA), the underlying concept of visual tools is taken a step further. In essence, VA combines human analytical capabilities with computer processing capacities. In the human-computer interaction process, the user generates new knowledge and gains insights.},
	number = {3},
	journal = {IEEE Computer Graphics and Applications},
	author = {Smuc, Michael and Mayr, Eva and Lammarsch, Tim and Aigner, Wolfgang and Miksch, Silvia and Gärtner, Johannes},
	month = may,
	year = {2009},
	keywords = {Type of Work: Empirical Study, WHEN - Retrospective Analyses, HOW - Other, Type of Work: Theory \& Model},
	pages = {29--38},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\35A3H3K2\\Smuc et al. - 2009 - To Score or Not to Score Tripling Insights for Pa.pdf:application/pdf;Smuc et al. - 2009 - To Score or Not to Score Tripling Insights for Pa.pdf:C\:\\Users\\conny\\Zotero\\storage\\DHRI7C6U\\Smuc et al. - 2009 - To Score or Not to Score Tripling Insights for Pa.pdf:application/pdf}
}

@article{macinnes_visual_2010,
	title = {Visual {Classification}: {Expert} {Knowledge} {Guides} {Machine} {Learning}},
	volume = {30},
	issn = {1558-1756},
	shorttitle = {Visual {Classification}},
	doi = {10.1109/MCG.2010.18},
	abstract = {Humans use intuition and experience to classify everything they perceive, but only if the distinguishing patterns are visible. Machine-learning algorithms can learn class information from data sets, but the created classes' meaning isn't always clear. A proposed mixed-initiative approach combines intuitive visualizations with machine learning to tap into the strengths of human and machine classification. The use of visualizations in an expert-guided clustering technique allows the display of complex data sets in a way that allows human input into machine clustering. Test participants successfully employed this technique to classify analytic activities using behavioral observations of a creative-analysis task. The results demonstrate how visualization of the machine-learned classification can help users create more robust and intuitive categories.},
	number = {1},
	journal = {IEEE Computer Graphics and Applications},
	author = {MacInnes, Joseph and Santosa, Stephanie and Wright, William},
	month = jan,
	year = {2010},
	keywords = {WHEN - Real-Time Applications, HOW - Pattern Analysis, WHY - Adaptive Systems / Guidance, Type of Work: Empirical Study, HOW - Classification Models, WHY - Model Steering / Active Learning},
	pages = {8--14},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\RU5546RA\\MacInnes et al. - 2010 - Visual Classification Expert Knowledge Guides Mac.pdf:application/pdf}
}

@article{endert_semantic_2014,
	title = {Semantic {Interaction} for {Visual} {Analytics}: {Toward} {Coupling} {Cognition} and {Computation}},
	volume = {34},
	issn = {1558-1756},
	shorttitle = {Semantic {Interaction} for {Visual} {Analytics}},
	doi = {10.1109/MCG.2014.73},
	abstract = {Alex Endert's dissertation "Semantic Interaction for Visual Analytics: Inferring Analytical Reasoning for Model Steering" described semantic interaction, a user interaction methodology for visual analytics (VA). It showed that user interaction embodies users' analytic process and can thus be mapped to model-steering functionality for "human-in-the-loop" system design. The dissertation contributed a framework (or pipeline) that describes such a process, a prototype VA system to test semantic interaction, and a user evaluation to demonstrate semantic interaction's impact on the analytic process. This research is influencing current VA research and has implications for future VA research.},
	number = {4},
	journal = {IEEE Computer Graphics and Applications},
	author = {Endert, Alex},
	month = jul,
	year = {2014},
	keywords = {WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, Type of Work: Survey, WHY - User Behaviour, User Characteristics, User Modelling, WHY - Adaptive Systems / Guidance, HOW - Other, Type of Work: Theory \& Model, WHY - Model Steering / Active Learning},
	pages = {8--15},
	file = {Endert - 2014 - Semantic Interaction for Visual Analytics Toward .pdf:C\:\\Users\\conny\\Zotero\\storage\\CNX3LQBP\\Endert - 2014 - Semantic Interaction for Visual Analytics Toward .pdf:application/pdf;IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\HQGEE3Z3\\Endert - 2014 - Semantic Interaction for Visual Analytics Toward .pdf:application/pdf}
}

@article{choe_characterizing_2015,
	title = {Characterizing {Visualization} {Insights} from {Quantified} {Selfers}' {Personal} {Data} {Presentations}},
	volume = {35},
	issn = {1558-1756},
	doi = {10.1109/MCG.2015.51},
	abstract = {Data visualization and analytics research has great potential to empower people to improve their lives by leveraging their own personal data. However, most quantified selfers (Q-Selfers) are neither visualization experts nor data scientists. Consequently, visualizations Q-Selfers created with their data are often not ideal for conveying insights. Aiming to design a visualization system to help nonexperts gain and communicate personal data insights, the authors conducted a predesign empirical study. Through the lens of Q-Selfers, they examined what insights people gain specifically from their personal data and how they use visualizations to communicate their insights. Based on their analysis of 30 quantified self-presentations, they characterized eight insight types (detail, self-reflection, trend, comparison, correlation, data summary, distribution, and outlier) and mapped the visual annotations used to communicate them. They further discussed four areas for the design of personal visualization systems, including support for encouraging self-reflection, gaining valid insight, communicating insight, and using visual annotations.},
	number = {4},
	journal = {IEEE Computer Graphics and Applications},
	author = {Choe, Eun Kyoung and Lee, Bongshin and schraefel, m.c.},
	month = jul,
	year = {2015},
	keywords = {HOW - Manually coded data-driven insights, WHY - User Behaviour, User Characteristics, User Modelling, HOW - Classification Models, WHEN - Retrospective Analyses, HOW - Other, WHY - Report Generation / Storytelling},
	pages = {28--37},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\MLVEZ8LK\\Choe et al. - 2015 - Characterizing Visualization Insights from Quantif.pdf:application/pdf}
}

@article{endert_semantic_2015,
	title = {Semantic {Interaction}: {Coupling} {Cognition} and {Computation} through {Usable} {Interactive} {Analytics}},
	volume = {35},
	issn = {1558-1756},
	shorttitle = {Semantic {Interaction}},
	doi = {10.1109/MCG.2015.91},
	abstract = {The success of visual analytics is predicated on the ability of users to interactively explore information. Humans think about their data through interactive visual exploration, including testing hypotheses, exploring anomalies, and other cognitive processes of building understanding from data. The claim that these insights are generated as a result of the interaction led the attendees at the Pacific Northwest National Laboratory (PNNL) workshop on "Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics" to posit that user interaction must play a more central role in visual analytics systems, serving as the method for coupling cognition and computation. The claims and design principles discussed in this workshop report present research directions to advance visual analytics via a user interaction approach called semantic interaction.},
	number = {4},
	journal = {IEEE Computer Graphics and Applications},
	author = {Endert, Alex and Chang, Remco and North, Chris and Zhou, Michelle},
	month = jul,
	year = {2015},
	keywords = {WHEN - Real-Time Applications, Type of Work: Model/Framework/Theory, Type of Work: Survey, WHY - User Behaviour, User Characteristics, User Modelling, WHY - Adaptive Systems / Guidance, HOW - Probabilistic Models / Prediction, HOW - Classification Models},
	pages = {94--99},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\VT5743W4\\Endert et al. - 2015 - Semantic Interaction Coupling Cognition and Compu.pdf:application/pdf}
}

@article{bors_provenance_2019,
	title = {A {Provenance} {Task} {Abstraction} {Framework}},
	volume = {39},
	issn = {1558-1756},
	doi = {10.1109/MCG.2019.2945720},
	abstract = {Visual analytics tools integrate provenance recording to externalize analytic processes or user insights. Provenance can be captured on varying levels of detail, and in turn activities can be characterized from different granularities. However, current approaches do not support inferring activities that can only be characterized across multiple levels of provenance. We propose a task abstraction framework that consists of a three stage approach, composed of 1) initializing a provenance task hierarchy, 2) parsing the provenance hierarchy by using an abstraction mapping mechanism, and 3) leveraging the task hierarchy in an analytical tool. Furthermore, we identify implications to accommodate iterative refinement, context, variability, and uncertainty during all stages of the framework. We describe a use case which exemplifies our abstraction framework, demonstrating how context can influence the provenance hierarchy to support analysis. The article concludes with an agenda, raising and discussing challenges that need to be considered for successfully implementing such a framework.},
	number = {6},
	journal = {IEEE Computer Graphics and Applications},
	author = {Bors, Christian and Wenskovitch, John and Dowling, Michelle and Attfield, Simon and Battle, Leilani and Endert, Alex and Kulyk, Olga and Laramee, Robert S.},
	month = nov,
	year = {2019},
	keywords = {WHY - User Behaviour, User Characteristics, User Modelling, HOW - Classification Models, WHEN - Retrospective Analyses, Type of Work: Theory \& Model},
	pages = {46--60},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\4MSRJCZ5\\Bors et al. - 2019 - A Provenance Task Abstraction Framework.pdf:application/pdf}
}

@article{fekete_provenance_2019,
	title = {Provenance {Analysis} for {Sensemaking}},
	volume = {39},
	issn = {1558-1756},
	doi = {10.1109/MCG.2019.2945378},
	abstract = {The articles in this special section examine the concept of "sensemaking", which refers to how we structure the unknown so as to be able to act in it. In the context of data analysis it involves understanding the data, generating hypotheses, selecting analysis methods, creating novel solutions, and critical thinking and learning wherever needed. Due to its explorative and creative nature, sensemaking is arguably the most challenging part of any data analysis.},
	number = {6},
	journal = {IEEE Computer Graphics and Applications},
	author = {Fekete, Jean-Daniel and Jankun-Kelly, T. J. and Tory, Melanie and Xu, Kai},
	month = nov,
	year = {2019},
	keywords = {Type of Work: Survey},
	pages = {27--29},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\9B5URJ2C\\Fekete et al. - 2019 - Provenance Analysis for Sensemaking.pdf:application/pdf}
}

@article{madanagopal_analytic_2019,
	title = {Analytic {Provenance} in {Practice}: {The} {Role} of {Provenance} in {Real}-{World} {Visualization} and {Data} {Analysis} {Environments}},
	volume = {39},
	issn = {1558-1756},
	shorttitle = {Analytic {Provenance} in {Practice}},
	doi = {10.1109/MCG.2019.2933419},
	abstract = {Practical data analysis scenarios involve more than just the interpretation of data through visual and algorithmic analysis. Many real-world analysis environments involve multiple types of experts and analysts working together to solve problems and make decisions, adding organizational and social requirements to the mix. We aim to provide new knowledge about the role of provenance for practical problems in a variety of analysis scenarios central to national security. We present the findings from interviews with data analysts from domains, such as intelligence analysis, cyber-security, and geospatial intelligence. In addition to covering multiple analysis domains, our study also considers practical workplace implications related to organizational roles and the level of analyst experience. The results demonstrate how different needs for provenance depend on different roles in the analysis effort (e.g., data analyst, task managers, data analyst trainers, and quality control analysts). By considering the core challenges reported along with an analysis of existing provenance-support techniques through existing research and systems, we contribute new insights about needs and opportunities for improvements to provenance-support methods.},
	number = {6},
	journal = {IEEE Computer Graphics and Applications},
	author = {Madanagopal, Karthic and Ragan, Eric D. and Benjamin, Perakath},
	month = nov,
	year = {2019},
	keywords = {WHY - Methods, WHY - Purposes, Provenance Requirements, HOW - Program Synthesis, WHY - User Behaviour, User Characteristics, User Modelling, Type of Work: Empirical Study, WHEN - Retrospective AnalysesHOW - Program Synthesis, Provenance Requirements, Type of Work: Empirical Study, WHEN - Retrospective Analyses, WHY - Methods, WHY - Purposes, WHY - User Behaviour, User Characteristics, User Modelling},
	pages = {30--45},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\CGLK36N2\\Madanagopal et al. - 2019 - Analytic Provenance in Practice The Role of Prove.pdf:application/pdf}
}

@article{lee_you_2020,
	title = {You can't always sketch what you want: {Understanding} {Sensemaking} in {Visual} {Query} {Systems}},
	volume = {26},
	issn = {2160-9306},
	shorttitle = {You can't always sketch what you want},
	doi = {10.1109/TVCG.2019.2934666},
	abstract = {Visual query systems (VQSs) empower users to interactively search for line charts with desired visual patterns, typically specified using intuitive sketch-based interfaces. Despite decades of past work on VQSs, these efforts have not translated to adoption in practice, possibly because VQSs are largely evaluated in unrealistic lab-based settings. To remedy this gap in adoption, we collaborated with experts from three diverse domains—astronomy, genetics, and material science—via a year-long user-centered design process to develop a VQS that supports their workflow and analytical needs, and evaluate how VQSs can be used in practice. Our study results reveal that ad-hoc sketch-only querying is not as commonly used as prior work suggests, since analysts are often unable to precisely express their patterns of interest. In addition, we characterize three essential sensemaking processes supported by our enhanced VQS. We discover that participants employ all three processes, but in different proportions, depending on the analytical needs in each domain. Our findings suggest that all three sensemaking processes must be integrated in order to make future VQSs useful for a wide range of analytical inquiries.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Lee, Doris Jung-Lin and Lee, John and Siddiqui, Tarique and Kim, Jaewoo and Karahalios, Karrie and Parameswaran, Aditya},
	month = jan,
	year = {2020},
	pages = {1267--1277},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\WXFJEGFQ\\8807280.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\3GH8QS5U\\Lee et al. - 2020 - You can't always sketch what you want Understandi.pdf:application/pdf}
}

@article{huang_natural-language-based_2020,
	title = {A {Natural}-language-based {Visual} {Query} {Approach} of {Uncertain} {Human} {Trajectories}},
	volume = {26},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2019.2934671},
	abstract = {Visual querying is essential for interactively exploring massive trajectory data. However, the data uncertainty imposes profound challenges to fulfill advanced analytics requirements. On the one hand, many underlying data does not contain accurate geographic coordinates, e.g., positions of a mobile phone only refer to the regions (i.e., mobile cell stations) in which it resides, instead of accurate GPS coordinates. On the other hand, domain experts and general users prefer a natural way, such as using a natural language sentence, to access and analyze massive movement data. In this paper, we propose a visual analytics approach that can extract spatial-temporal constraints from a textual sentence and support an effective query method over uncertain mobile trajectory data. It is built up on encoding massive, spatially uncertain trajectories by the semantic information of the POls and regions covered by them, and then storing the trajectory documents in text database with an effective indexing scheme. The visual interface facilitates query condition specification, situation-aware visualization, and semantic exploration of large trajectory data. Usage scenarios on real-world human mobility datasets demonstrate the effectiveness of our approach.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Huang, Zhaosong and Zhao, Ye and Chen, Wei and Gao, Shengjie and Yu, Kejie and Xu, Weixia and Tang, Mingjie and Zhu, Minfeng and Xu, Mingliang},
	month = jan,
	year = {2020},
	pages = {1256--1266},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\WSH3K64X\\8807274.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\32SEP5FV\\Huang et al. - 2020 - A Natural-language-based Visual Query Approach of .pdf:application/pdf}
}

@article{cui_text--viz:_2020,
	title = {Text-to-{Viz}: {Automatic} {Generation} of {Infographics} from {Proportion}-{Related} {Natural} {Language} {Statements}},
	volume = {26},
	issn = {2160-9306},
	shorttitle = {Text-to-{Viz}},
	doi = {10.1109/TVCG.2019.2934785},
	abstract = {Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Cui, Weiwei and Zhang, Xiaoyu and Wang, Yun and Huang, He and Chen, Bei and Fang, Lei and Zhang, Haidong and Lou, Jian-Guan and Zhang, Dongmei},
	month = jan,
	year = {2020},
	pages = {906--916},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\EAGQ4HAI\\8813126.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\RXXYBFEW\\Cui et al. - 2020 - Text-to-Viz Automatic Generation of Infographics .pdf:application/pdf}
}

@article{gehrmann_visual_2020,
	title = {Visual {Interaction} with {Deep} {Learning} {Models} through {Collaborative} {Semantic} {Inference}},
	volume = {26},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2019.2934595},
	abstract = {Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem, which means that a user can both understand and control parts of the model reasoning process. We demonstrate the feasibility of CSI with a co-designed case study of a document summarization system.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Gehrmann, Sebastian and Strobelt, Hendrik and Krüger, Robert and Pfister, Hanspeter and Rush, Alexander M.},
	month = jan,
	year = {2020},
	pages = {884--894},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\P5CW2RAI\\8805457.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\V2RKFZ6X\\Gehrmann et al. - 2020 - Visual Interaction with Deep Learning Models throu.pdf:application/pdf}
}

@article{saket_investigating_2020,
	title = {Investigating {Direct} {Manipulation} of {Graphical} {Encodings} as a {Method} for {User} {Interaction}},
	volume = {26},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2019.2934534},
	abstract = {We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Saket, Bahador and Huron, Samuel and Perin, Charles and Endert, Alex},
	month = jan,
	year = {2020},
	pages = {482--491},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\UQGC574I\\8809678.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\IBLV46SR\\Saket et al. - 2020 - Investigating Direct Manipulation of Graphical Enc.pdf:application/pdf}
}

@article{yu_flowsense:_2020,
	title = {{FlowSense}: {A} {Natural} {Language} {Interface} for {Visual} {Data} {Exploration} within a {Dataflow} {System}},
	volume = {26},
	issn = {2160-9306},
	shorttitle = {{FlowSense}},
	doi = {10.1109/TVCG.2019.2934668},
	abstract = {Dataflow visualization systems enable flexible visual data exploration by allowing the user to construct a dataflow diagram that composes query and visualization modules to specify system functionality. However learning dataflow diagram usage presents overhead that often discourages the user. In this work we design FlowSense, a natural language interface for dataflow visualization systems that utilizes state-of-the-art natural language processing techniques to assist dataflow diagram construction. FlowSense employs a semantic parser with special utterance tagging and special utterance placeholders to generalize to different datasets and dataflow diagrams. It explicitly presents recognized dataset and diagram special utterances to the user for dataflow context awareness. With FlowSense the user can expand and adjust dataflow diagrams more conveniently via plain English. We apply FlowSense to the VisFlow subset-flow visualization system to enhance its usability. We evaluate FlowSense by one case study with domain experts on a real-world data analysis problem and a formal user study.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Yu, Bowen and Silva, Cláudio T.},
	month = jan,
	year = {2020},
	keywords = {WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, Type of Work: Tool/Software, HOW - Classification Models, WHY - Explain ML Model / Debug Algorithm / Query Plan, HOW: converts natural language into queries / instructions, HOW: natural language, Maybe related. Creation of a data flow diagram (provenance), WHY - User Behavior / User Characteristics / User Modelling, WHY: natural languag is easier for users, ENCODING - Grammar},
	pages = {1--11},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\AYZ9IVGL\\8807265.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\MY5M3UJC\\Yu and Silva - 2020 - FlowSense A Natural Language Interface for Visual.pdf:application/pdf}
}

@article{du_eventaction:_2019,
	title = {{EventAction}: {A} {Visual} {Analytics} {Approach} to {Explainable} {Recommendation} for {Event} {Sequences}},
	volume = {9},
	issn = {2160-6455},
	shorttitle = {{EventAction}},
	url = {http://doi.acm.org/10.1145/3301402},
	doi = {10.1145/3301402},
	abstract = {People use recommender systems to improve their decisions; for example, item recommender systems help them find films to watch or books to buy. Despite the ubiquity of item recommender systems, they can be improved by giving users greater transparency and control. This article develops and assesses interactive strategies for transparency and control, as applied to event sequence recommender systems, which provide guidance in critical life choices such as medical treatments, careers decisions, and educational course selections. This article’s main contribution is the use of both record attributes and temporal event information as features to identify similar records and provide appropriate recommendations. While traditional item recommendations are based on choices by people with similar attributes, such as those who looked at this product or watched this movie, our event sequence recommendation approach allows users to select records that share similar attribute values and start with a similar event sequence. Then users see how different choices of actions and the orders and times between them might lead to users’ desired outcomes. This paper applies a visual analytics approach to present and explain recommendations of event sequences. It presents a workflow for event sequence recommendation that is implemented in EventAction and reports on three case studies in two domains to illustrate the use of generating event sequence recommendations based on personal histories. It also offers design guidelines for the construction of user interfaces for event sequence recommendation and discusses ethical issues in dealing with personal histories. A demo video of EventAction is available at https://hcil.umd.edu/eventaction.},
	number = {4},
	urldate = {2019-12-16},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Du, Fan and Plaisant, Catherine and Spring, Neil and Crowley, Kenyon and Shneiderman, Ben},
	month = aug,
	year = {2019},
	pages = {21:1--21:31},
	file = {ACM Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\UALGNL7X\\Du et al. - 2019 - EventAction A Visual Analytics Approach to Explai.pdf:application/pdf}
}

@article{koskela_proactive_2018,
	title = {Proactive {Information} {Retrieval} by {Capturing} {Search} {Intent} from {Primary} {Task} {Context}},
	volume = {8},
	issn = {2160-6455},
	url = {http://doi.acm.org/10.1145/3150975},
	doi = {10.1145/3150975},
	abstract = {A significant fraction of information searches are motivated by the user’s primary task. An ideal search engine would be able to use information captured from the primary task to proactively retrieve useful information. Previous work has shown that many information retrieval activities depend on the primary task in which the retrieved information is to be used, but fairly little research has been focusing on methods that automatically learn the informational intents from the primary task context. We study how the implicit primary task context can be used to model the user’s search intent and to proactively retrieve relevant and useful information. Data comprising of logs from a user study, in which users are writing an essay, demonstrate that users’ search intents can be captured from the task and relevant and useful information can be proactively retrieved. Data from simulations with several datasets of different complexity show that the proposed approach of using primary task context generalizes to a variety of data. Our findings have implications for the design of proactive search systems that can infer users’ search intent implicitly by monitoring users’ primary task activities.},
	number = {3},
	urldate = {2019-12-16},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Koskela, Markus and Luukkonen, Petri and Ruotsalo, Tuukka and SjÖberg, Mats and Floréen, Patrik},
	month = jul,
	year = {2018},
	pages = {20:1--20:25},
	file = {ACM Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\RRLKIJ2Q\\Koskela et al. - 2018 - Proactive Information Retrieval by Capturing Searc.pdf:application/pdf}
}

@article{smith_predicting_2018,
	title = {Predicting {User} {Confidence} {During} {Visual} {Decision} {Making}},
	volume = {8},
	issn = {2160-6455},
	url = {http://doi.acm.org/10.1145/3185524},
	doi = {10.1145/3185524},
	abstract = {People are not infallible consistent “oracles”: their confidence in decision-making may vary significantly between tasks and over time. We have previously reported the benefits of using an interface and algorithms that explicitly captured and exploited users’ confidence: error rates were reduced by up to 50\% for an industrial multi-class learning problem; and the number of interactions required in a design-optimisation context was reduced by 33\%. Having access to users’ confidence judgements could significantly benefit intelligent interactive systems in industry, in areas such as intelligent tutoring systems and in health care. There are many reasons for wanting to capture information about confidence implicitly. Some are ergonomic, but others are more “social”—such as wishing to understand (and possibly take account of) users’ cognitive state without interrupting them. We investigate the hypothesis that users’ confidence can be accurately predicted from measurements of their behaviour. Eye-tracking systems were used to capture users’ gaze patterns as they undertook a series of visual decision tasks, after each of which they reported their confidence on a 5-point Likert scale. Subsequently, predictive models were built using “conventional” machine learning approaches for numerical summary features derived from users’ behaviour. We also investigate the extent to which the deep learning paradigm can reduce the need to design features specific to each application by creating “gaze maps”—visual representations of the trajectories and durations of users’ gaze fixations—and then training deep convolutional networks on these images. Treating the prediction of user confidence as a two-class problem (confident/not confident), we attained classification accuracy of 88\% for the scenario of new users on known tasks, and 87\% for known users on new tasks. Considering the confidence as an ordinal variable, we produced regression models with a mean absolute error of ≈0.7 in both cases. Capturing just a simple subset of non-task-specific numerical features gave slightly worse, but still quite high accuracy (e.g., MAE ≈ 1.0). Results obtained with gaze maps and convolutional networks are competitive, despite not having access to longer-term information about users and tasks, which was vital for the “summary” feature sets. This suggests that the gaze-map-based approach forms a viable, transferable alternative to handcrafting features for each different application. These results provide significant evidence to confirm our hypothesis, and offer a way of substantially improving many interactive artificial intelligence applications via the addition of cheap non-intrusive hardware and computationally cheap prediction algorithms.},
	number = {2},
	urldate = {2019-12-16},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Smith, Jim and Legg, Phil and Matovic, Milos and Kinsey, Kristofer},
	month = jun,
	year = {2018},
	keywords = {WHY - User Behaviour, User Characteristics, User Modelling, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, WHEN - Retrospective Analyses, Type of Work: Theory \& Model, WHY - Real-time or post-hoc Quantification and Re-Application},
	pages = {10:1--10:30},
	file = {Smith et al. - 2018 - Predicting User Confidence During Visual Decision .pdf:C\:\\Users\\conny\\Zotero\\storage\\N6CLY2YV\\Smith et al. - 2018 - Predicting User Confidence During Visual Decision .pdf:application/pdf}
}

@article{gotz_adaptive_2017,
	title = {Adaptive {Contextualization} {Methods} for {Combating} {Selection} {Bias} {During} {High}-{Dimensional} {Visualization}},
	volume = {7},
	issn = {2160-6455},
	url = {http://doi.acm.org/10.1145/3009973},
	doi = {10.1145/3009973},
	abstract = {Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. This article describes Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is specifically designed to combat the invisible introduction of selection bias. The AC approach (1) monitors and models a user’s visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. This article expands on an earlier article presented at ACM IUI 2016 [16] by providing a more detailed review of the AC methodology and additional evaluation results.},
	number = {4},
	urldate = {2019-12-16},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Gotz, David and Sun, Shun and Cao, Nan and Kundu, Rita and Meyer, Anne-Marie},
	month = nov,
	year = {2017},
	keywords = {WHEN - Real-Time Applications, HOW - Pattern Analysis, WHY - Real-time or post-hoc quantification and re-application, WHY - User Behaviour, User Characteristics, User Modelling, Type of Work: Empirical Study, Type of Work: Theory \& Model},
	pages = {17:1--17:23},
	file = {Gotz et al. - 2017 - Adaptive Contextualization Methods for Combating S.pdf:C\:\\Users\\conny\\Zotero\\storage\\NF7ZTDQE\\Gotz et al. - 2017 - Adaptive Contextualization Methods for Combating S.pdf:application/pdf}
}

@inproceedings{shrinivasan_connecting_2009,
	title = {Connecting the dots in visual analysis},
	booktitle = {2009 {IEEE} symposium on visual analytics science and technology},
	publisher = {IEEE},
	author = {Shrinivasan, Yedendra B. and Gotz, David and Lu, Jie},
	year = {2009},
	keywords = {ENCODING - Sequence, HOW - Program Synthesis, WHEN - Real-Time Applications, HOW: graph analysis of past interactions, WHY: recommendation and retrieval (of past annotations), Type of Work: Empirical Study, Type of Work: Technique \& Algorithm, WHEN - Real-Time Applications, WHY - Model Steering / Active Learning, ENCODING - SequenceWHY: recommendation and retrieval (of past annotations)},
	pages = {123--130},
	file = {Snapshot:C\:\\Users\\conny\\Zotero\\storage\\7S5RUTKD\\5333023.html:text/html}
}

@inproceedings{kadivar_capturing_2009,
	title = {Capturing and supporting the analysis process},
	booktitle = {2009 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	publisher = {Ieee},
	author = {Kadivar, Nazanin and Chen, Victor and Dunsmuir, Dustin and Lee, Eric and Qian, Cheryl and Dill, John and Shaw, Christopher and Woodbury, Robert},
	year = {2009},
	keywords = {HOW - Program Synthesis, WHEN - Real-Time Applications, Type of Work: Technique \& Algorithm, WHY - Model Steering / Active Learning, ENCODING - Grammar (proprietary scripting language)},
	pages = {131--138},
	file = {Snapshot:C\:\\Users\\conny\\Zotero\\storage\\NJKYZZFB\\5333020.html:text/html}
}

@article{zhang_effect_2016,
	title = {The {Effect} of {Embodied} {Interaction} in {Visual}-{Spatial} {Navigation}},
	volume = {7},
	issn = {2160-6455},
	url = {http://doi.acm.org/10.1145/2953887},
	doi = {10.1145/2953887},
	abstract = {This article aims to assess the effect of embodied interaction on attention during the process of solving spatio-visual navigation problems. It presents a method that links operator's physical interaction, feedback, and attention. Attention is inferred through networks called Bayesian Attentional Networks (BANs). BANs are structures that describe cause-effect relationship between attention and physical action. Then, a utility function is used to determine the best combination of interaction modalities and feedback. Experiments involving five physical interaction modalities (vision-based gesture interaction, glove-based gesture interaction, speech, feet, and body stance) and two feedback modalities (visual and sound) are described. The main findings are: (i) physical expressions have an effect in the quality of the solutions to spatial navigation problems; (ii) the combination of feet gestures with visual feedback provides the best task performance.},
	number = {1},
	urldate = {2019-12-13},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Zhang, Ting and Li, Yu-Ting and Wachs, Juan P.},
	month = dec,
	year = {2016},
	pages = {3:1--3:36},
	file = {ACM Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\SEPN6KBT\\Zhang et al. - 2016 - The Effect of Embodied Interaction in Visual-Spati.pdf:application/pdf}
}

@article{mutlu_vizrec:_2016,
	title = {{VizRec}: {Recommending} {Personalized} {Visualizations}},
	volume = {6},
	issn = {2160-6455},
	shorttitle = {{VizRec}},
	url = {http://doi.acm.org/10.1145/2983923},
	doi = {10.1145/2983923},
	abstract = {Visualizations have a distinctive advantage when dealing with the information overload problem: Because they are grounded in basic visual cognition, many people understand them. However, creating proper visualizations requires specific expertise of the domain and underlying data. Our quest in this article is to study methods to suggest appropriate visualizations autonomously. To be appropriate, a visualization has to follow known guidelines to find and distinguish patterns visually and encode data therein. A visualization tells a story of the underlying data; yet, to be appropriate, it has to clearly represent those aspects of the data the viewer is interested in. Which aspects of a visualization are important to the viewer? Can we capture and use those aspects to recommend visualizations? This article investigates strategies to recommend visualizations considering different aspects of user preferences. A multi-dimensional scale is used to estimate aspects of quality for visualizations for collaborative filtering. Alternatively, tag vectors describing visualizations are used to recommend potentially interesting visualizations based on content. Finally, a hybrid approach combines information on what a visualization is about (tags) and how good it is (ratings). We present the design principles behind VizRec, our visual recommender. We describe its architecture, the data acquisition approach with a crowd sourced study, and the analysis of strategies for visualization recommendation.},
	number = {4},
	urldate = {2019-12-13},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Mutlu, Belgin and Veas, Eduardo and Trattner, Christoph},
	month = nov,
	year = {2016},
	keywords = {HOW - Pattern Analysis, WHY - User Behaviour, User Characteristics, User Modelling, WHY - Adaptive Systems / Guidance, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, WHEN - Retrospective Analyses, Type of Work: Application \& Design Study},
	pages = {31:1--31:39},
	file = {Mutlu et al. - 2016 - VizRec Recommending Personalized Visualizations.pdf:C\:\\Users\\conny\\Zotero\\storage\\EAQBM3AT\\Mutlu et al. - 2016 - VizRec Recommending Personalized Visualizations.pdf:application/pdf}
}

@article{steichen_inferring_2014,
	title = {Inferring {Visualization} {Task} {Properties}, {User} {Performance}, and {User} {Cognitive} {Abilities} from {Eye} {Gaze} {Data}},
	volume = {4},
	issn = {2160-6455},
	url = {http://doi.acm.org/10.1145/2633043},
	doi = {10.1145/2633043},
	abstract = {Information visualization systems have traditionally followed a one-size-fits-all model, typically ignoring an individual user's needs, abilities, and preferences. However, recent research has indicated that visualization performance could be improved by adapting aspects of the visualization to the individual user. To this end, this article presents research aimed at supporting the design of novel user-adaptive visualization systems. In particular, we discuss results on using information on user eye gaze patterns while interacting with a given visualization to predict properties of the user's visualization task; the user's performance (in terms of predicted task completion time); and the user's individual cognitive abilities, such as perceptual speed, visual working memory, and verbal working memory. We provide a detailed analysis of different eye gaze feature sets, as well as over-time accuracies. We show that these predictions are significantly better than a baseline classifier even during the early stages of visualization usage. These findings are then discussed with a view to designing visualization systems that can adapt to the individual user in real time.},
	number = {2},
	urldate = {2019-12-13},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Steichen, Ben and Conati, Cristina and Carenini, Giuseppe},
	month = jul,
	year = {2014},
	keywords = {HOW - Pattern Analysis, WHY - User Behaviour, User Characteristics, User Modelling, WHEN - Hybrid Approaches, WHY - Adaptive Systems / Guidance, HOW - Classification Models, Type of Work: Application \& Design Study},
	pages = {11:1--11:29},
	file = {Steichen et al. - 2014 - Inferring Visualization Task Properties, User Perf.pdf:C\:\\Users\\conny\\Zotero\\storage\\7G7S8RQX\\Steichen et al. - 2014 - Inferring Visualization Task Properties, User Perf.pdf:application/pdf}
}

@ARTICLE{vaps, author={M. {Sedlmair} and C. {Heinzl} and S. {Bruckner} and H. {Piringer} and T. {Möller}}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={Visual Parameter Space Analysis: A Conceptual Framework}, year={2014}, volume={20}, number={12}, pages={2161-2170},}

@article{dagstuhl_provenance_2019,
	title = {Provenance and Logging for Sense Making (Dagstuhl Seminar 18462)},
	volume = {8},
	rights = {All rights reserved},
	issn = {2192-5283},
	url = {http://drops.dagstuhl.de/opus/volltexte/2019/10355},
	doi = {10.4230/DagRep.8.11.35},
	pages = {35--62},
	number = {11},
	journal = {Dagstuhl Reports},
	author = {Fekete, Jean-Daniel and Jankun-Kelly, T. J. and Tory, Melanie and Xu, Kai},
	urldate = {2020-01-02},
	year = {2019},
	file = {Full Text PDF:/Users/kaixu/OneDrive - Middlesex University/zotero/storage/6AH576GU/Fekete et al. - 2019 - Provenance and Logging for Sense
	Making (Dagstuhl .pdf:application/pdf}
}
@article{chen_employing_2014,
	title = {Employing a {Parametric} {Model} for {Analytic} {Provenance}},
	volume = {4},
	issn = {2160-6455},
	url = {http://doi.acm.org/10.1145/2591510},
	doi = {10.1145/2591510},
	abstract = {We introduce a propagation-based parametric symbolic model approach to supporting analytic provenance. This approach combines a script language to capture and encode the analytic process and a parametrically controlled symbolic model to represent and reuse the logic of the analysis process. Our approach first appeared in a visual analytics system called CZSaw. Using a script to capture the analyst’s interactions at a meaningful system action level allows the creation of a parametrically controlled symbolic model in the form of a Directed Acyclic Graph (DAG). Using the DAG allows propagating changes. Graph nodes correspond to variables in CZSaw scripts, which are results (data and data visualizations) generated from user interactions. The user interacts with variables representing entities or relations to create the next step’s results. Graph edges represent dependency relationships among nodes. Any change to a variable triggers the propagation mechanism to update downstream dependent variables and in turn updates data views to reflect the change. The analyst can reuse parts of the analysis process by assigning new values to a node in the graph. We evaluated this symbolic model approach by solving three IEEE VAST Challenge contest problems (from IEEE VAST 2008, 2009, and 2010). In each of these challenges, the analyst first created a symbolic model to explore, understand, analyze, and solve a particular subproblem and then reused the model via its dependency graph propagation mechanism to solve similar subproblems. With the script and model, CZSaw supports the analytic provenance by capturing, encoding, and reusing the analysis process. The analyst can recall the chronological states of the analysis process with the CZSaw script and may interpret the underlying rationale of the analysis with the symbolic model.},
	number = {1},
	urldate = {2019-12-13},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Chen, Yingjie Victor and Qian, Zhenyu Cheryl and Woodbury, Robert and Dill, John and Shaw, Chris D.},
	month = apr,
	year = {2014},
	keywords = {HOW - Program Synthesis, WHEN - Hybrid Approaches, WHY - Adaptive Systems / Guidance, Type of Work: Empirical Study, Type of Work: Theory \& ModelType of Work: Empirical Study, Type of Work: Theory \& Model, WHEN - Hybrid Approaches, WHY - Adaptive Systems / Guidance},
	pages = {6:1--6:32},
	file = {Chen et al. - 2014 - Employing a Parametric Model for Analytic Provenan.pdf:C\:\\Users\\conny\\Zotero\\storage\\75AS3E33\\Chen et al. - 2014 - Employing a Parametric Model for Analytic Provenan.pdf:application/pdf}
}

@article{stitz_knowledgepearls:_2019,
	title = {{KnowledgePearls}: {Provenance}-{Based} {Visualization} {Retrieval}},
	volume = {25},
	issn = {2160-9306},
	shorttitle = {{KnowledgePearls}},
	doi = {10.1109/TVCG.2018.2865024},
	abstract = {Storing analytical provenance generates a knowledge base with a large potential for recalling previous results and guiding users in future analyses. However, without extensive manual creation of meta information and annotations by the users, search and retrieval of analysis states can become tedious. We present KnowledgePearls, a solution for efficient retrieval of analysis states that are structured as provenance graphs containing automatically recorded user interactions and visualizations. As a core component, we describe a visual interface for querying and exploring analysis states based on their similarity to a partial definition of a requested analysis state. Depending on the use case, this definition may be provided explicitly by the user by formulating a search query or inferred from given reference states. We explain our approach using the example of efficient retrieval of demographic analyses by Hans Rosling and discuss our implementation for a fast look-up of previous states. Our approach is independent of the underlying visualization framework. We discuss the applicability for visualizations which are based on the declarative grammar Vega and we use a Vega-based implementation of Gapminder as guiding example. We additionally present a biomedical case study to illustrate how KnowledgePearls facilitates the exploration process by recalling states from earlier analyses.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Stitz, Holger and Gratzl, Samuel and Piringer, Harald and Zichner, Thomas and Streit, Marc},
	month = jan,
	year = {2019},
	keywords = {WHEN - Real-Time Applications, Type of Work: Tool/Software, HOW - Classification Models, WHY - Explain ML Model / Debug Algorithm / Query Plan, HOW: attribute (string) based search, WHY: retrieval of visualization state, ENCODING - Sequence},
	pages = {120--130},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\BAZLGDVY\\8440831.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\86W7AYCE\\Stitz et al. - 2019 - KnowledgePearls Provenance-Based Visualization Re.pdf:application/pdf}
}

@article{sacha_vis4ml:_2019,
	title = {{VIS4ML}: {An} {Ontology} for {Visual} {Analytics} {Assisted} {Machine} {Learning}},
	volume = {25},
	issn = {2160-9306},
	shorttitle = {{VIS4ML}},
	doi = {10.1109/TVCG.2018.2864838},
	abstract = {While many VA workflows make use of machine-learned models to support analytical tasks, VA workflows have become increasingly important in understanding and improving Machine Learning (ML) processes. In this paper, we propose an ontology (VIS4ML) for a subarea of VA, namely “VA-assisted ML”. The purpose of VIS4ML is to describe and understand existing VA workflows used in ML as well as to detect gaps in ML processes and the potential of introducing advanced VA techniques to such processes. Ontologies have been widely used to map out the scope of a topic in biology, medicine, and many other disciplines. We adopt the scholarly methodologies for constructing VIS4ML, including the specification, conceptualization, formalization, implementation, and validation of ontologies. In particular, we reinterpret the traditional VA pipeline to encompass model-development workflows. We introduce necessary definitions, rules, syntaxes, and visual notations for formulating VIS4ML and make use of semantic web technologies for implementing it in the Web Ontology Language (OWL). VIS4ML captures the high-level knowledge about previous workflows where VA is used to assist in ML. It is consistent with the established VA concepts and will continue to evolve along with the future developments in VA and ML. While this ontology is an effort for building the theoretical foundation of VA, it can be used by practitioners in real-world applications to optimize model-development workflows by systematically examining the potential benefits that can be brought about by either machine or human capabilities. Meanwhile, VIS4ML is intended to be extensible and will continue to be updated to reflect future advancements in using VA for building high-quality data-analytical models or for building such models rapidly.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Sacha, Dominik and Kraus, Matthias and Keim, Daniel A. and Chen, Min},
	month = jan,
	year = {2019},
	pages = {385--395},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\L82SQSQM\\8440124.html:text/html;Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\33QC44E4\\Sacha et al. - 2019 - VIS4ML An Ontology for Visual Analytics Assisted .pdf:application/pdf}
}

@article{shih_declarative_2019,
	title = {A {Declarative} {Grammar} of {Flexible} {Volume} {Visualization} {Pipelines}},
	volume = {25},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2018.2864841},
	abstract = {This paper presents a declarative grammar for conveniently and effectively specifying advanced volume visualizations. Existing methods for creating volume visualizations either lack the flexibility to specify sophisticated visualizations or are difficult to use for those unfamiliar with volume rendering implementation and parameterization. Our design provides the ability to quickly create expressive visualizations without knowledge of the volume rendering implementation. It attempts to capture aspects of those difficult but powerful methods while remaining flexible and easy to use. As a proof of concept, our current implementation of the grammar allows users to combine multiple data variables in various ways and define transfer functions for diverse input data. The grammar also has the ability to describe advanced shading effects and create animations. We demonstrate the power and flexibility of our approach using multiple practical volume visualizations.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Shih, Min and Rozhon, Charles and Ma, Kwan-Liu},
	month = jan,
	year = {2019},
	pages = {1050--1059},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\5JNIHQNC\\8440063.html:text/html}
}

@article{moritz_formalizing_2019,
	title = {Formalizing {Visualization} {Design} {Knowledge} as {Constraints}: {Actionable} and {Extensible} {Models} in {Draco}},
	volume = {25},
	issn = {2160-9306},
	shorttitle = {Formalizing {Visualization} {Design} {Knowledge} as {Constraints}},
	doi = {10.1109/TVCG.2018.2865240},
	abstract = {There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Moritz, Dominik and Wang, Chenglong and Nelson, Greg L. and Lin, Halden and Smith, Adam M. and Howe, Bill and Heer, Jeffrey},
	month = jan,
	year = {2019},
	pages = {438--448},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\LES692NF\\8440847.html:text/html;Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\ITQYUZZA\\Moritz et al. - 2019 - Formalizing Visualization Design Knowledge as Cons.pdf:application/pdf}
}

@article{subramonyam_smartcues:_2019,
	title = {{SmartCues}: {A} {Multitouch} {Query} {Approach} for {Details}-on-{Demand} through {Dynamically} {Computed} {Overlays}},
	volume = {25},
	issn = {2160-9306},
	shorttitle = {{SmartCues}},
	doi = {10.1109/TVCG.2018.2865231},
	abstract = {Details-on-demand is a crucial feature in the visual information-seeking process but is often only implemented in highly constrained settings. The most common solution, hover queries (i.e., tooltips), are fast and expressive but are usually limited to single mark (e.g., a bar in a bar chart). `Queries' to retrieve details for more complex sets of objects (e.g., comparisons between pairs of elements, averages across multiple items, trend lines, etc.) are difficult for end-users to invoke explicitly. Further, the output of these queries require complex annotations and overlays which need to be displayed and dismissed on demand to avoid clutter. In this work we introduce SmartCues, a library to support details-on-demand through dynamically computed overlays. For end-users, SmartCues provides multitouch interactions to construct complex queries for a variety of details. For designers, SmartCues offers an interaction library that can be used out-of-the-box, and can be extended for new charts and detail types. We demonstrate how SmartCues can be implemented across a wide array of visualization types and, through a lab study, show that end users can effectively use SmartCues.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Subramonyam, Hariharan and Adar, Eytan},
	month = jan,
	year = {2019},
	pages = {597--607},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\MZGMYTXZ\\8440833.html:text/html}
}

@article{liu_nlize:_2019,
	title = {{NLIZE}: {A} {Perturbation}-{Driven} {Visual} {Interrogation} {Tool} for {Analyzing} and {Interpreting} {Natural} {Language} {Inference} {Models}},
	volume = {25},
	issn = {2160-9306},
	shorttitle = {{NLIZE}},
	doi = {10.1109/TVCG.2018.2865230},
	abstract = {With the recent advances in deep learning, neural network models have obtained state-of-the-art performances for many linguistic tasks in natural language processing. However, this rapid progress also brings enormous challenges. The opaque nature of a neural network model leads to hard-to-debug-systems and difficult-to-interpret mechanisms. Here, we introduce a visualization system that, through a tight yet flexible integration between visualization elements and the underlying model, allows a user to interrogate the model by perturbing the input, internal state, and prediction while observing changes in other parts of the pipeline. We use the natural language inference problem as an example to illustrate how a perturbation-driven paradigm can help domain experts assess the potential limitation of a model, probe its inner states, and interpret and form hypotheses about fundamental model mechanisms such as attention.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Liu, Shusen and Li, Zhimin and Li, Tao and Srikumar, Vivek and Pascucci, Valerio and Bremer, Peer-Timo},
	month = jan,
	year = {2019},
	pages = {651--660},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\FQEI9DD7\\8454904.html:text/html}
}

@article{faust_dimreader:_2019,
	title = {{DimReader}: {Axis} lines that explain non-linear projections},
	volume = {25},
	issn = {2160-9306},
	shorttitle = {{DimReader}},
	doi = {10.1109/TVCG.2018.2865194},
	abstract = {Non-linear dimensionality reduction (NDR) methods such as LLE and t-SNE are popular with visualization researchers and experienced data analysts, but present serious problems of interpretation. In this paper, we present DimReader, a technique that recovers readable axes from such techniques. DimReader is based on analyzing infinitesimal perturbations of the dataset with respect to variables of interest. The perturbations define exactly how we want to change each point in the original dataset and we measure the effect that these changes have on the projection. The recovered axes are in direct analogy with the axis lines (grid lines) of traditional scatterplots. We also present methods for discovering perturbations on the input data that change the projection the most. The calculation of the perturbations is efficient and easily integrated into programs written in modern programming languages. We present results of DimReader on a variety of NDR methods and datasets both synthetic and real-life, and show how it can be used to compare different NDR methods. Finally, we discuss limitations of our proposal and situations where further research is needed.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Faust, Rebecca and Glickenstein, David and Scheidegger, Carlos},
	month = jan,
	year = {2019},
	pages = {481--490},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\9RYZCIUN\\8440820.html:text/html;Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\WVTR5BXE\\Faust et al. - 2019 - DimReader Axis lines that explain non-linear proj.pdf:application/pdf}
}

@article{ren_charticulator:_2019,
	title = {Charticulator: {Interactive} {Construction} of {Bespoke} {Chart} {Layouts}},
	volume = {25},
	issn = {2160-9306},
	shorttitle = {Charticulator},
	doi = {10.1109/TVCG.2018.2865158},
	abstract = {We present Charticulator, an interactive authoring tool that enables the creation of bespoke and reusable chart layouts. Charticulator is our response to most existing chart construction interfaces that require authors to choose from predefined chart layouts, thereby precluding the construction of novel charts. In contrast, Charticulator transforms a chart specification into mathematical layout constraints and automatically computes a set of layout attributes using a constraint-solving algorithm to realize the chart. It allows for the articulation of compound marks or glyphs as well as links between these glyphs, all without requiring any coding or knowledge of constraint satisfaction. Furthermore, thanks to the constraint-based layout approach, Charticulator can export chart designs into reusable templates that can be imported into other visualization tools. In addition to describing Charticulator's conceptual framework and design, we present three forms of evaluation: a gallery to illustrate its expressiveness, a user study to verify its usability, and a click-count comparison between Charticulator and three existing tools. Finally, we discuss the limitations and potentials of Charticulator as well as directions for future research. Charticulator is available with its source code at https://charticulator.com.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Ren, Donghao and Lee, Bongshin and Brehmer, Matthew},
	month = jan,
	year = {2019},
	keywords = {Type of Work: Tool/Software, HOW: grammar based, HOW: constraint solving, Maybe related. A tool for designing info vis designs, WHY: auto generation of visualization designs thorugh interactive specification, ENCODING - Grammar},
	pages = {789--799},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\K5E683B5\\8440827.html:text/html}
}

@article{jo_declarative_2019,
	title = {A {Declarative} {Rendering} {Model} for {Multiclass} {Density} {Maps}},
	volume = {25},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2018.2865141},
	abstract = {Multiclass maps are scatterplots, multidimensional projections, or thematic geographic maps where data points have a categorical attribute in addition to two quantitative attributes. This categorical attribute is often rendered using shape or color, which does not scale when overplotting occurs. When the number of data points increases, multiclass maps must resort to data aggregation to remain readable. We present multiclass density maps: multiple 2D histograms computed for each of the category values. Multiclass density maps are meant as a building block to improve the expressiveness and scalability of multiclass map visualization. In this article, we first present a short survey of aggregated multiclass maps, mainly from cartography. We then introduce a declarative model-a simple yet expressive JSON grammar associated with visual semantics-that specifies a wide design space of visualizations for multiclass density maps. Our declarative model is expressive and can be efficiently implemented in visualization front-ends such as modern web browsers. Furthermore, it can be reconfigured dynamically to support data exploration tasks without recomputing the raw data. Finally, we demonstrate how our model can be used to reproduce examples from the past and support exploring data at scale.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Jo, Jaemin and Vernier, Frédéric and Dragicevic, Pierre and Fekete, Jean-Daniel},
	month = jan,
	year = {2019},
	pages = {470--480},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\LRJQM8RH\\8440817.html:text/html}
}

@article{feng_patterns_2019,
	title = {Patterns and {Pace}: {Quantifying} {Diverse} {Exploration} {Behavior} with {Visualizations} on the {Web}},
	volume = {25},
	issn = {2160-9306},
	shorttitle = {Patterns and {Pace}},
	doi = {10.1109/TVCG.2018.2865117},
	abstract = {The diverse and vibrant ecosystem of interactive visualizations on the web presents an opportunity for researchers and practitioners to observe and analyze how everyday people interact with data visualizations. However, existing metrics of visualization interaction behavior used in research do not fully reveal the breadth of peoples' open-ended explorations with visualizations. One possible way to address this challenge is to determine high-level goals for visualization interaction metrics, and infer corresponding features from user interaction data that characterize different aspects of peoples' explorations of visualizations. In this paper, we identify needs for visualization behavior measurement, and develop corresponding candidate features that can be inferred from users' interaction data. We then propose metrics that capture novel aspects of peoples' open-ended explorations, including exploration uniqueness and exploration pacing. We evaluate these metrics along with four other metrics recently proposed in visualization literature by applying them to interaction data from prior visualization studies. The results of these evaluations suggest that these new metrics 1) reveal new characteristics of peoples' use of visualizations, 2) can be used to evaluate statistical differences between visualization designs, and 3) are statistically independent of prior metrics used in visualization research. We discuss implications of these results for future studies, including the potential for applying these metrics in visualization interaction analysis, as well as emerging challenges in developing and selecting metrics depicting visualization explorations.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Feng, Mi and Peck, Evan and Harrison, Lane},
	month = jan,
	year = {2019},
	keywords = {Type of Work: Case Study, HOW - Probabilistic Models / Prediction, WHEN - Retrospective Analyses, HOW: TFIDF, HOW: wavelet transform, WHY: characterizing exploration behaviorType of Work: Case Study, WHEN - Retrospective Analyses, WHY - User Behavior / User Characteristics / User Modelling, WHY: characterizing exploration behaviors},
	pages = {501--511},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\VRTZF6DM\\8454489.html:text/html}
}

@inproceedings{federico_role_2017,
	title = {The {Role} of {Explicit} {Knowledge}: {A} {Conceptual} {Model} of {Knowledge}-{Assisted} {Visual} {Analytics}},
	shorttitle = {The {Role} of {Explicit} {Knowledge}},
	doi = {10.1109/VAST.2017.8585498},
	abstract = {Visual Analytics (VA) aims to combine the strengths of humans and computers for effective data analysis. In this endeavor, humans' tacit knowledge from prior experience is an important asset that can be leveraged by both human and computer to improve the analytic process. While VA environments are starting to include features to formalize, store, and utilize such knowledge, the mechanisms and degree in which these environments integrate explicit knowledge varies widely. Additionally, this important class of VA environments has never been elaborated on by existing work on VA theory. This paper proposes a conceptual model of Knowledge-assisted VA conceptually grounded on the visualization model by van Wijk. We apply the model to describe various examples of knowledge-assisted VA from the literature and elaborate on three of them in finer detail. Moreover, we illustrate the utilization of the model to compare different design alternatives and to evaluate existing approaches with respect to their use of knowledge. Finally, the model can inspire designers to generate novel VA environments using explicit knowledge effectively.},
	booktitle = {2017 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {Federico, Paolo and Wagner, Markus and Rind, Alexander and Amor-Amorós, Albert and Miksch, Silvia and Aigner, Wolfgang},
	month = oct,
	year = {2017},
	note = {ISSN: null},
	pages = {92--103},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\9SVCVIMK\\8585498.html:text/html}
}

@inproceedings{chung_cricto:_2017,
	title = {{CRICTO}: {Supporting} {Sensemaking} through {Crowdsourced} {Information} {Schematization}},
	shorttitle = {{CRICTO}},
	doi = {10.1109/VAST.2017.8585484},
	abstract = {We present CRICTO, a new crowdsourcing visual analytics environment for making sense of and analyzing text data, whereby multiple crowdworkers are able to parallelize the simple information schematization tasks of relating and connecting entities across documents. The diverse links from these schematization tasks are then automatically combined and the system visualizes them based on the semantic types of the linkages. CRICTO also includes several tools that allow analysts to interactively explore and refine crowdworkers' results to better support their own sensemaking processes. We evaluated CRICTO's techniques and analysis workflow with deployments of CRICTO using Amazon Mechanical Turk and a user study that assess the effect of crowdsourced schematization in sensemaking tasks. The results of our evaluation show that CRICTO's crowdsourcing approaches and workflow help analysts explore diverse aspects of datasets, and uncover more accurate hidden stories embedded in the text datasets.},
	booktitle = {2017 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {Chung, Haeyong and Dasari, Sai Prashanth and Nandhakumar, Santhosh and Andrews, Christopher},
	month = oct,
	year = {2017},
	note = {ISSN: null},
	pages = {139--150},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\T7R95ITP\\8585484.html:text/html}
}

@article{forbes_dynamic_2018,
	title = {Dynamic {Influence} {Networks} for {Rule}-{Based} {Models}},
	volume = {24},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2017.2745280},
	abstract = {We introduce the Dynamic Influence Network (DIN), a novel visual analytics technique for representing and analyzing rule-based models of protein-protein interaction networks. Rule-based modeling has proved instrumental in developing biological models that are concise, comprehensible, easily extensible, and that mitigate the combinatorial complexity of multi-state and multi-component biological molecules. Our technique visualizes the dynamics of these rules as they evolve over time. Using the data produced by KaSim, an open source stochastic simulator of rule-based models written in the Kappa language, DINs provide a node-link diagram that represents the influence that each rule has on the other rules. That is, rather than representing individual biological components or types, we instead represent the rules about them (as nodes) and the current influence of these rules (as links). Using our interactive DIN-Viz software tool, researchers are able to query this dynamic network to find meaningful patterns about biological processes, and to identify salient aspects of complex rule-based models. To evaluate the effectiveness of our approach, we investigate a simulation of a circadian clock model that illustrates the oscillatory behavior of the KaiC protein phosphorylation cycle.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Forbes, Angus G. and Burks, Andrew and Lee, Kristine and Li, Xing and Boutillier, Pierre and Krivine, Jean and Fontana, Walter},
	month = jan,
	year = {2018},
	pages = {184--194},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\YYHD7S5M\\8017593.html:text/html;Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\8YA69FIU\\Forbes et al. - 2018 - Dynamic Influence Networks for Rule-Based Models.pdf:application/pdf}
}

@article{zhao_supporting_2018,
	title = {Supporting {Handoff} in {Asynchronous} {Collaborative} {Sensemaking} {Using} {Knowledge}-{Transfer} {Graphs}},
	volume = {24},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2017.2745279},
	abstract = {During asynchronous collaborative analysis, handoff of partial findings is challenging because externalizations produced by analysts may not adequately communicate their investigative process. To address this challenge, we developed techniques to automatically capture and help encode tacit aspects of the investigative process based on an analyst's interactions, and streamline explicit authoring of handoff annotations. We designed our techniques to mediate awareness of analysis coverage, support explicit communication of progress and uncertainty with annotation, and implicit communication through playback of investigation histories. To evaluate our techniques, we developed an interactive visual analysis system, KTGraph, that supports an asynchronous investigative document analysis task. We conducted a two-phase user study to characterize a set of handoff strategies and to compare investigative performance with and without our techniques. The results suggest that our techniques promote the use of more effective handoff strategies, help increase an awareness of prior investigative process and insights, as well as improve final investigative outcomes.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Zhao, Jian and Glueck, Michael and Isenberg, Petra and Chevalier, Fanny and Khan, Azam},
	month = jan,
	year = {2018},
	keywords = {Type of Work: Tool/Software, WHY: collaborative analysis, HOW: construction of knowledge graph, WHY: knowledge transfer},
	pages = {340--350},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\P32ZFARP\\8017596.html:text/html;Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\D6LDRS9U\\Zhao et al. - 2018 - Supporting Handoff in Asynchronous Collaborative S.pdf:application/pdf}
}

@article{edge_beyond_2018,
	title = {Beyond {Tasks}: {An} {Activity} {Typology} for {Visual} {Analytics}},
	volume = {24},
	issn = {2160-9306},
	shorttitle = {Beyond {Tasks}},
	doi = {10.1109/TVCG.2017.2745180},
	abstract = {As Visual Analytics (VA) research grows and diversifies to encompass new systems, techniques, and use contexts, gaining a holistic view of analytic practices is becoming ever more challenging. However, such a view is essential for researchers and practitioners seeking to develop systems for broad audiences that span multiple domains. In this paper, we interpret VA research through the lens of Activity Theory (AT) - a framework for modelling human activities that has been influential in the field of Human-Computer Interaction. We first provide an overview of Activity Theory, showing its potential for thinking beyond tasks, representations, and interactions to the broader systems of activity in which interactive tools are embedded and used. Next, we describe how Activity Theory can be used as an organizing framework in the construction of activity typologies, building and expanding upon the tradition of abstract task taxonomies in the field of Information Visualization. We then apply the resulting process to create an activity typology for Visual Analytics, synthesizing a wide range of systems and activity concepts from the literature. Finally, we use this typology as the foundation of an activity-centered design process, highlighting both tensions and opportunities in the design space of VA systems.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Edge, Darren and Riche, Nathalie Henry and Larson, Jonathan and White, Christopher},
	month = jan,
	year = {2018},
	pages = {267--277},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\MEIVTBN4\\8019880.html:text/html}
}

@article{wall_podium:_2018,
	title = {Podium: {Ranking} {Data} {Using} {Mixed}-{Initiative} {Visual} {Analytics}},
	volume = {24},
	issn = {2160-9306},
	shorttitle = {Podium},
	doi = {10.1109/TVCG.2017.2745078},
	abstract = {People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wall, Emily and Das, Subhajit and Chawla, Ravish and Kalidindi, Bharath and Brown, Eli T. and Endert, Alex},
	month = jan,
	year = {2018},
	keywords = {WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, Type of Work: Tool/Software, HOW: semantic interaction, HOW - Classification Models, WHY: semi-automated sorting of tabular data, WHY - User Behavior / User Characteristics / User Modelling},
	pages = {288--297},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\N3PUSIZ7\\8019863.html:text/html}
}

@article{srinivasan_graphiti:_2018,
	title = {Graphiti: {Interactive} {Specification} of {Attribute}-{Based} {Edges} for {Network} {Modeling} and {Visualization}},
	volume = {24},
	issn = {2160-9306},
	shorttitle = {Graphiti},
	doi = {10.1109/TVCG.2017.2744843},
	abstract = {Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Srinivasan, Arjun and Park, Hyunwoo and Endert, Alex and Basole, Rahul C.},
	month = jan,
	year = {2018},
	keywords = {HOW - Program Synthesis, WHY - Data Wrangling, WHEN - Real-Time Applications, Type of Work: Tool/Software, WHY - Adaptive Systems / Guidance, Type of Work: Technique \& Algorithm, HOW: program synthesis (similar to Wrangler), WHY: interactive generation of a graph from tabular data, ENCODING - Grammar},
	pages = {226--235},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\RESGPTUV\\8019835.html:text/html}
}

@article{sacha_somflow:_2018,
	title = {{SOMFlow}: {Guided} {Exploratory} {Cluster} {Analysis} with {Self}-{Organizing} {Maps} and {Analytic} {Provenance}},
	volume = {24},
	issn = {2160-9306},
	shorttitle = {{SOMFlow}},
	doi = {10.1109/TVCG.2017.2744805},
	abstract = {Clustering is a core building block for data analysis, aiming to extract otherwise hidden structures and relations from raw datasets, such as particular groups that can be effectively related, compared, and interpreted. A plethora of visual-interactive cluster analysis techniques has been proposed to date, however, arriving at useful clusterings often requires several rounds of user interactions to fine-tune the data preprocessing and algorithms. We present a multi-stage Visual Analytics (VA) approach for iterative cluster refinement together with an implementation (SOMFlow) that uses Self-Organizing Maps (SOM) to analyze time series data. It supports exploration by offering the analyst a visual platform to analyze intermediate results, adapt the underlying computations, iteratively partition the data, and to reflect previous analytical activities. The history of previous decisions is explicitly visualized within a flow graph, allowing to compare earlier cluster refinements and to explore relations. We further leverage quality and interestingness measures to guide the analyst in the discovery of useful patterns, relations, and data partitions. We conducted two pair analytics experiments together with a subject matter expert in speech intonation research to demonstrate that the approach is effective for interactive data analysis, supporting enhanced understanding of clustering results as well as the interactive process itself.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Sacha, Dominik and Kraus, Matthias and Bernard, Jürgen and Behrisch, Michael and Schreck, Tobias and Asano, Yuki and Keim, Daniel A.},
	month = jan,
	year = {2018},
	keywords = {Semantic Interactions, WHY - Data Wrangling, WHEN - Real-Time Applications, Type of Work: Tool/Software, WHY - Adaptive Systems / Guidance, HOW - Classification Models, HOW: visualization, WHY: visualization of analysis history, GUidance, Maybe related. It visualizes the history but there is no analysis over the past interactions},
	pages = {120--130},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\7HMJH5ID\\8019867.html:text/html;Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\QSWJZB3V\\Sacha et al. - 2018 - SOMFlow Guided Exploratory Cluster Analysis with .pdf:application/pdf}
}

@article{hoque_applying_2018,
	title = {Applying {Pragmatics} {Principles} for {Interaction} with {Visual} {Analytics}},
	volume = {24},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2017.2744684},
	abstract = {Interactive visual data analysis is most productive when users can focus on answering the questions they have about their data, rather than focusing on how to operate the interface to the analysis tool. One viable approach to engaging users in interactive conversations with their data is a natural language interface to visualizations. These interfaces have the potential to be both more expressive and more accessible than other interaction paradigms. We explore how principles from language pragmatics can be applied to the flow of visual analytical conversations, using natural language as an input modality. We evaluate the effectiveness of pragmatics support in our system Evizeon, and present design considerations for conversation interfaces to visual analytics tools.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Hoque, Enamul and Setlur, Vidya and Tory, Melanie and Dykeman, Isaac},
	month = jan,
	year = {2018},
	keywords = {WHEN - Real-Time Applications, Type of Work: Technique, Type of Work: Tool/Software, HOW - Probabilistic Models / Prediction, HOW: "pragmatics" from linguistic theory, WHY: natural language interaction between user and visualization, WHY: recommendation of new queries, WHY - Model Steering / Active Learning, WHY - User Behavior / User Characteristics / User Modelling, ENCODING - Grammar},
	pages = {309--318},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\ATDKZZ8C\\8019833.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\WEMN7BHQ\\Hoque et al. - 2018 - Applying Pragmatics Principles for Interaction wit.pdf:application/pdf}
}

@article{rubel_bastet:_2018,
	title = {{BASTet}: {Shareable} and {Reproducible} {Analysis} and {Visualization} of {Mass} {Spectrometry} {Imaging} {Data} via {OpenMSI}},
	volume = {24},
	issn = {2160-9306},
	shorttitle = {{BASTet}},
	doi = {10.1109/TVCG.2017.2744479},
	abstract = {Mass spectrometry imaging (MSI) is a transformative imaging method that supports the untargeted, quantitative measurement of the chemical composition and spatial heterogeneity of complex samples with broad applications in life sciences, bioenergy, and health. While MSI data can be routinely collected, its broad application is currently limited by the lack of easily accessible analysis methods that can process data of the size, volume, diversity, and complexity generated by MSI experiments. The development and application of cutting-edge analytical methods is a core driver in MSI research for new scientific discoveries, medical diagnostics, and commercial-innovation. However, the lack of means to share, apply, and reproduce analyses hinders the broad application, validation, and use of novel MSI analysis methods. To address this central challenge, we introduce the Berkeley Analysis and Storage Toolkit (BASTet), a novel framework for shareable and reproducible data analysis that supports standardized data and analysis interfaces, integrated data storage, data provenance, workflow management, and a broad set of integrated tools. Based on BASTet, we describe the extension of the OpenMSI mass spectrometry imaging science gateway to enable web-based sharing, reuse, analysis, and visualization of data analyses and derived data products. We demonstrate the application of BASTet and OpenMSI in practice to identify and compare characteristic substructures in the mouse brain based on their chemical composition measured via MSI.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Rübel, Oliver and Bowen, Benjamin P.},
	month = jan,
	year = {2018},
	keywords = {WHEN - Real-Time Applications, Type of Work: Tool/Software, HOW - Classification Models, WHY - Real-time or post-hoc Quantification and Re-Application, HOW: logging function calls of a standard API, WHY: reproducibility, ENCODING - Grammar},
	pages = {1025--1035},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\HLRK462V\\8017614.html:text/html;Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\JLYGJH22\\Rübel and Bowen - 2018 - BASTet Shareable and Reproducible Analysis and Vi.pdf:application/pdf}
}

@article{cappers_exploring_2018,
	title = {Exploring {Multivariate} {Event} {Sequences} {Using} {Rules}, {Aggregations}, and {Selections}},
	volume = {24},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2017.2745278},
	abstract = {Multivariate event sequences are ubiquitous: travel history, telecommunication conversations, and server logs are some examples. Besides standard properties such as type and timestamp, events often have other associated multivariate data. Current exploration and analysis methods either focus on the temporal analysis of a single attribute or the structural analysis of the multivariate data only. We present an approach where users can explore event sequences at multivariate and sequential level simultaneously by interactively defining a set of rewrite rules using multivariate regular expressions. Users can store resulting patterns as new types of events or attributes to interactively enrich or simplify event sequences for further investigation. In Eventpad we provide a bottom-up glyph-oriented approach for multivariate event sequence analysis by searching, clustering, and aligning them according to newly defined domain specific properties. We illustrate the effectiveness of our approach with real-world data sets including telecommunication traffic and hospital treatments.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Cappers, Bram C.M. and van Wijk, Jarke J.},
	month = jan,
	year = {2018},
	keywords = {HOW - Pattern Analysis, Type of Work: Tool/Software, WHY - Real-time or post-hoc Quantification and Re-Application, HOW: grammar based, HOW: visualization, HOW: pattern matching, Maybe related. A tool for analyzing sequence data, WHY: analyze multivariate event sequence data, WHY: reuse the analysis / query, ENCODING - Grammar},
	pages = {532--541},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\8Q8IPD94\\8019837.html:text/html}
}

@article{lam_bridging_2018,
	title = {Bridging from {Goals} to {Tasks} with {Design} {Study} {Analysis} {Reports}},
	volume = {24},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2017.2744319},
	abstract = {Visualization researchers and practitioners engaged in generating or evaluating designs are faced with the difficult problem of transforming the questions asked and actions taken by target users from domain-specific language and context into more abstract forms. Existing abstract task classifications aim to provide support for this endeavour by providing a carefully delineated suite of actions. Our experience is that this bottom-up approach is part of the challenge: low-level actions are difficult to interpret without a higher-level context of analysis goals and the analysis process. To bridge this gap, we propose a framework based on analysis reports derived from open-coding 20 design study papers published at IEEE InfoVis 2009-2015, to build on the previous work of abstractions that collectively encompass a broad variety of domains. The framework is organized in two axes illustrated by nine analysis goals. It helps situate the analysis goals by placing each goal under axes of specificity (Explore, Describe, Explain, Confirm) and number of data populations (Single, Multiple). The single-population types are Discover Observation, Describe Observation, Identify Main Cause, and Collect Evidence. The multiple-population types are Compare Entities, Explain Differences, and Evaluate Hypothesis. Each analysis goal is scoped by an input and an output and is characterized by analysis steps reported in the design study papers. We provide examples of how we and others have used the framework in a top-down approach to abstracting domain problems: visualization designers or researchers first identify the analysis goals of each unit of analysis in an analysis stream, and then encode the individual steps using existing task classifications with the context of the goal, the level of specificity, and the number of populations involved in the analysis.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Lam, Heidi and Tory, Melanie and Munzner, Tamara},
	month = jan,
	year = {2018},
	pages = {435--445},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\U9LXL96S\\8023762.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\J5HII93E\\Lam et al. - 2018 - Bridging from Goals to Tasks with Design Study Ana.pdf:application/pdf}
}

@inproceedings{blascheck_visual_2016,
	title = {Visual analysis and coding of data-rich user behavior},
	doi = {10.1109/VAST.2016.7883520},
	abstract = {Investigating user behavior involves abstracting low-level events to higher-level concepts. This requires an analyst to study individual user activities, assign codes which categorize behavior, and develop a consistent classification scheme. To better support this reasoning process of an analyst, we suggest a novel visual analytics approach which integrates rich user data including transcripts, videos, eye movement data, and interaction logs. Word-sized visualizations embedded into a tabular representation provide a space-efficient and detailed overview of user activities. An analyst assigns codes, grouped into code categories, as part of an interactive process. Filtering and searching helps to select specific activities and focus an analysis. A comparison visualization summarizes results of coding and reveals relationships between codes. Editing features support efficient assignment, refinement, and aggregation of codes. We demonstrate the practical applicability and usefulness of our approach in a case study and describe expert feedback.},
	booktitle = {2016 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {Blascheck, Tanja and Beck, Fabian and Baltes, Sebastian and Ertl, Thomas and Weiskopf, Daniel},
	month = oct,
	year = {2016},
	note = {ISSN: null},
	keywords = {HOW - Pattern Analysis, Type of Work: Tool/Software, HOW: visual analytics tool, Maybe related. A tool for helping an analyst code a user's interaction data, WHY: coding user's provenance (interaction log) data, WHY - User Behavior / User Characteristics / User Modelling},
	pages = {141--150},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\VFK9D6RT\\7883520.html:text/html}
}

@inproceedings{correll_semantics_2016,
	title = {The semantics of sketch: {Flexibility} in visual query systems for time series data},
	shorttitle = {The semantics of sketch},
	doi = {10.1109/VAST.2016.7883519},
	abstract = {Sketching allows analysts to specify complex and free-form patterns of interest. Visual query systems can make use of sketches to locate these patterns of interest in large datasets. However, sketching is ambiguous: the same drawing could represent a multitude of potential queries. In this work, we investigate these ambiguities as they apply to visual query systems for time series data. We define a class of “invariants” - the properties of a time series that the analyst wishes to ignore when performing a sketch-based query. We present the results of a crowd-sourced study, showing that these invariants are key components of how people rate the strength of match between sketch and target. We adapt a number of algorithms for time series matching to support invariants in sketches. Lastly, we present a web-deployed prototype sketch-based visual query system that relies on these invariants. We apply the prototype to data from finance, the digital humanities, and political science.},
	booktitle = {2016 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {Correll, Michael and Gleicher, Michael},
	month = oct,
	year = {2016},
	note = {ISSN: null},
	keywords = {WHEN - Real-Time Applications, HOW - Pattern Analysis, Type of Work: Technique, WHY - Adaptive Systems / Guidance, HOW - Classification Models, WHY - Explain ML Model / Debug Algorithm / Query Plan, HOW: sketch-based querying, WHY: interactive querying of temporal data, ENCODING - Grammar, how},
	pages = {131--140},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\WWGFSXKR\\7883519.html:text/html}
}

@inproceedings{muthumanickam_shape_2016,
	title = {Shape grammar extraction for efficient query-by-sketch pattern matching in long time series},
	doi = {10.1109/VAST.2016.7883518},
	abstract = {Long time-series, involving thousands or even millions of time steps, are common in many application domains but remain very difficult to explore interactively. Often the analytical task in such data is to identify specific patterns, but this is a very complex and computationally difficult problem and so focusing the search in order to only identify interesting patterns is a common solution. We propose an efficient method for exploring user-sketched patterns, incorporating the domain expert's knowledge, in time series data through a shape grammar based approach. The shape grammar is extracted from the time series by considering the data as a combination of basic elementary shapes positioned across different amplitudes. We represent these basic shapes using a ratio value, perform binning on ratio values and apply a symbolic approximation. Our proposed method for pattern matching is amplitude-, scale- and translation-invariant and, since the pattern search and pattern constraint relaxation happen at the symbolic level, is very efficient permitting its use in a real-time/online system. We demonstrate the effectiveness of our method in a case study on stock market data although it is applicable to any numeric time series data.},
	booktitle = {2016 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {Muthumanickam, Prithiviraj K. and Vrotsou, Katerina and Cooper, Matthew and Johansson, Jimmy},
	month = oct,
	year = {2016},
	note = {ISSN: null},
	keywords = {WHEN - Real-Time Applications, HOW - Pattern Analysis, Type of Work: Tool/Software, WHY - Explain ML Model / Debug Algorithm / Query Plan, HOW: sequence (string) based representation, HOW: sketch-based querying, HOW: symbolic approximation using shape grammar, Maybe related. Temporal pattern searching, WHY: find temporal patterns, WHY - Model Steering / Active Learning, ENCODING - Grammar},
	pages = {121--130},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\78PMGV8R\\7883518.html:text/html}
}

@inproceedings{nguyen_sensemap:_2016,
	title = {{SenseMap}: {Supporting} browser-based online sensemaking through analytic provenance},
	shorttitle = {{SenseMap}},
	doi = {10.1109/VAST.2016.7883515},
	abstract = {Sensemaking is described as the process in which people collect, organize and create representations of information, all centered around some problem they need to understand. People often get lost when solving complicated tasks using big datasets over long periods of exploration and analysis. They may forget what they have done, are unaware of where they are in the context of the overall task, and are unsure where to continue. In this paper, we introduce a tool, SenseMap, to address these issues in the context of browser-based online sensemaking. We conducted a semi-structured interview with nine participants to explore their behaviors in online sensemaking with existing browser functionality. A simplified sensemaking model based on Pirolli and Card's model is derived to better represent the behaviors we found: users iteratively collect information sources relevant to the task, curate them in a way that makes sense, and finally communicate their findings to others. SenseMap automatically captures provenance of user sensemaking actions and provides multi-linked views to visualize the collected information and enable users to curate and communicate their findings. To explore how SenseMap is used, we conducted a user study in a naturalistic work setting with five participants completing the same sensemaking task related to their daily work activities. All participants found the visual representation and interaction of the tool intuitive to use. Three of them engaged with the tool and produced successful outcomes. It helped them to organize information sources, to quickly find and navigate to the sources they wanted, and to effectively communicate their findings.},
	booktitle = {2016 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {Nguyen, Phong H. and Xu, Kai and Bardill, Andy and Salman, Betul and Herd, Kate and Wong, B.L. William},
	month = oct,
	year = {2016},
	note = {ISSN: null},
	keywords = {Type of Work: Tool/Software, HOW: visual analytics tool, WHY: visualize analysis (sensemaking) history},
	pages = {91--100},
	file = {Accepted Version:C\:\\Users\\conny\\Zotero\\storage\\6N6P7ZXJ\\Nguyen et al. - 2016 - SenseMap Supporting browser-based online sensemak.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\C9I53AU5\\7883515.html:text/html}
}

@article{liu_patterns_2017,
	title = {Patterns and {Sequences}: {Interactive} {Exploration} of {Clickstreams} to {Understand} {Common} {Visitor} {Paths}},
	volume = {23},
	issn = {2160-9306},
	shorttitle = {Patterns and {Sequences}},
	doi = {10.1109/TVCG.2016.2598797},
	abstract = {Modern web clickstream data consists of long, high-dimensional sequences of multivariate events, making it difficult to analyze. Following the overarching principle that the visual interface should provide information about the dataset at multiple levels of granularity and allow users to easily navigate across these levels, we identify four levels of granularity in clickstream analysis: patterns, segments, sequences and events. We present an analytic pipeline consisting of three stages: pattern mining, pattern pruning and coordinated exploration between patterns and sequences. Based on this approach, we discuss properties of maximal sequential patterns, propose methods to reduce the number of patterns and describe design considerations for visualizing the extracted sequential patterns and the corresponding raw sequences. We demonstrate the viability of our approach through an analysis scenario and discuss the strengths and limitations of the methods based on user feedback.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Liu, Zhicheng and Wang, Yang and Dontcheva, Mira and Hoffman, Matthew and Walker, Seth and Wilson, Alan},
	month = jan,
	year = {2017},
	keywords = {WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, HOW - Pattern Analysis, Type of Work: Tool/Software, HOW: clustering, WHY: analyze clickstream data, HOW - Classification Models, WHY - Real-time or post-hoc Quantification and Re-Application, HOW: visualization, HOW: sequence (string) based representation, HOW: sequential pattern mining, ENCODING - Sequence},
	pages = {321--330},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\S6MZKHRN\\7539341.html:text/html}
}

@article{ceneda_characterizing_2017,
	title = {Characterizing {Guidance} in {Visual} {Analytics}},
	volume = {23},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2016.2598468},
	abstract = {Visual analytics (VA) is typically applied in scenarios where complex data has to be analyzed. Unfortunately, there is a natural correlation between the complexity of the data and the complexity of the tools to study them. An adverse effect of complicated tools is that analytical goals are more difficult to reach. Therefore, it makes sense to consider methods that guide or assist users in the visual analysis process. Several such methods already exist in the literature, yet we are lacking a general model that facilitates in-depth reasoning about guidance. We establish such a model by extending van Wijk's model of visualization with the fundamental components of guidance. Guidance is defined as a process that gradually narrows the gap that hinders effective continuation of the data analysis. We describe diverse inputs based on which guidance can be generated and discuss different degrees of guidance and means to incorporate guidance into VA tools. We use existing guidance approaches from the literature to illustrate the various aspects of our model. As a conclusion, we identify research challenges and suggest directions for future studies. With our work we take a necessary step to pave the way to a systematic development of guidance techniques that effectively support users in the context of VA.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Ceneda, Davide and Gschwandtner, Theresia and May, Thorsten and Miksch, Silvia and Schulz, Hans-Jörg and Streit, Marc and Tominski, Christian},
	month = jan,
	year = {2017},
	keywords = {Type of Work: Model/Framework/Theory, HOW: ???, WHY - Adaptive Systems / Guidance, WHY: use of past user interactions for recommendation / guidance},
	pages = {111--120},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\4EW4FRF9\\7534883.html:text/html}
}

@article{crouser_toward_2017,
	title = {Toward {Theoretical} {Techniques} for {Measuring} the {Use} of {Human} {Effort} in {Visual} {Analytic} {Systems}},
	volume = {23},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2016.2598460},
	abstract = {Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Crouser, R. Jordan and Franklin, Lyndsey and Endert, Alex and Cook, Kris},
	month = jan,
	year = {2017},
	pages = {121--130},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\9M9TWY5J\\7534744.html:text/html}
}

@article{kwon_axisketcher:_2017,
	title = {{AxiSketcher}: {Interactive} {Nonlinear} {Axis} {Mapping} of {Visualizations} through {User} {Drawings}},
	volume = {23},
	issn = {2160-9306},
	shorttitle = {{AxiSketcher}},
	doi = {10.1109/TVCG.2016.2598446},
	abstract = {Visual analytics techniques help users explore high-dimensional data. However, it is often challenging for users to express their domain knowledge in order to steer the underlying data model, especially when they have little attribute-level knowledge. Furthermore, users' complex, high-level domain knowledge, compared to low-level attributes, posits even greater challenges. To overcome these challenges, we introduce a technique to interpret a user's drawings with an interactive, nonlinear axis mapping approach called AxiSketcher. This technique enables users to impose their domain knowledge on a visualization by allowing interaction with data entries rather than with data attributes. The proposed interaction is performed through directly sketching lines over the visualization. Using this technique, users can draw lines over selected data points, and the system forms the axes that represent a nonlinear, weighted combination of multidimensional attributes. In this paper, we describe our techniques in three areas: 1) the design space of sketching methods for eliciting users' nonlinear domain knowledge; 2) the underlying model that translates users' input, extracts patterns behind the selected data points, and results in nonlinear axes reflecting users' complex intent; and 3) the interactive visualization for viewing, assessing, and reconstructing the newly formed, nonlinear axes.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Kwon, Bum Chul and Kim, Hannah and Wall, Emily and Choo, Jaegul and Park, Haesun and Endert, Alex},
	month = jan,
	year = {2017},
	keywords = {WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, HOW - Pattern Analysis, Type of Work: Tool/Software, HOW: semantic interaction, WHY - Report Generation / Storytelling, Maybe related. Arbitrary generation of axis for scatterplots, WHY: model steering (of a scatterplot), WHY - Model Steering / Active Learning, ENCODING - Vector},
	pages = {221--230},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\T43X9ZCJ\\7534876.html:text/html}
}

@article{feng_hindsight:_2017,
	title = {{HindSight}: {Encouraging} {Exploration} through {Direct} {Encoding} of {Personal} {Interaction} {History}},
	volume = {23},
	issn = {2160-9306},
	shorttitle = {{HindSight}},
	doi = {10.1109/TVCG.2016.2599058},
	abstract = {Physical and digital objects often leave markers of our use. Website links turn purple after we visit them, for example, showing us information we have yet to explore. These “footprints” of interaction offer substantial benefits in information saturated environments - they enable us to easily revisit old information, systematically explore new information, and quickly resume tasks after interruption. While applying these design principles have been successful in HCI contexts, direct encodings of personal interaction history have received scarce attention in data visualization. One reason is that there is little guidance for integrating history into visualizations where many visual channels are already occupied by data. More importantly, there is not firm evidence that making users aware of their interaction history results in benefits with regards to exploration or insights. Following these observations, we propose HindSight - an umbrella term for the design space of representing interaction history directly in existing data visualizations. In this paper, we examine the value of HindSight principles by augmenting existing visualizations with visual indicators of user interaction history (e.g. How the Recession Shaped the Economy in 255 Charts, NYTimes). In controlled experiments of over 400 participants, we found that HindSight designs generally encouraged people to visit more data and recall different insights after interaction. The results of our experiments suggest that simple additions to visualizations can make users aware of their interaction history, and that these additions significantly impact users' exploration and insights.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Feng, Mi and Deng, Cheng and Peck, Evan M. and Harrison, Lane},
	month = jan,
	year = {2017},
	keywords = {Type of Work: User Study, HOW: highlight data elements that a user has previously explored, WHY: encourages users to explore the visualization and data more, WHY: history visualization},
	pages = {351--360},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\A23RMQGX\\7539616.html:text/html;Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\VDI7YDJT\\Feng et al. - 2017 - HindSight Encouraging Exploration through Direct .pdf:application/pdf}
}

@article{satyanarayan_vega-lite:_2017,
	title = {Vega-{Lite}: {A} {Grammar} of {Interactive} {Graphics}},
	volume = {23},
	issn = {2160-9306},
	shorttitle = {Vega-{Lite}},
	doi = {10.1109/TVCG.2016.2599030},
	abstract = {We present Vega-Lite, a high-level grammar that enables rapid specification of interactive data visualizations. Vega-Lite combines a traditional grammar of graphics, providing visual encoding rules and a composition algebra for layered and multi-view displays, with a novel grammar of interaction. Users specify interactive semantics by composing selections. In Vega-Lite, a selection is an abstraction that defines input event processing, points of interest, and a predicate function for inclusion testing. Selections parameterize visual encodings by serving as input data, defining scale extents, or by driving conditional logic. The Vega-Lite compiler automatically synthesizes requisite data flow and event handling logic, which users can override for further customization. In contrast to existing reactive specifications, Vega-Lite selections decompose an interaction design into concise, enumerable semantic units. We evaluate Vega-Lite through a range of examples, demonstrating succinct specification of both customized interaction methods and common techniques such as panning, zooming, and linked selection.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Satyanarayan, Arvind and Moritz, Dominik and Wongsuphasawat, Kanit and Heer, Jeffrey},
	month = jan,
	year = {2017},
	pages = {341--350},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\MFAVKYW4\\7539624.html:text/html;Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\YL236449\\Satyanarayan et al. - 2017 - Vega-Lite A Grammar of Interactive Graphics.pdf:application/pdf}
}

@article{saket_visualization_2017,
	title = {Visualization by {Demonstration}: {An} {Interaction} {Paradigm} for {Visual} {Data} {Exploration}},
	volume = {23},
	issn = {2160-9306},
	shorttitle = {Visualization by {Demonstration}},
	doi = {10.1109/TVCG.2016.2598839},
	abstract = {Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Saket, Bahador and Kim, Hannah and Brown, Eli T. and Endert, Alex},
	month = jan,
	year = {2017},
	keywords = {WHY - Evaluation of Tools and Systems, Type of Work: Tool/Software, HOW: ???, Maybe related. Generate (infovis) visualization via interaction, WHY: create visual designs that reflect a user's interests},
	pages = {331--340},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\PTUPXNNN\\7539327.html:text/html}
}

@inproceedings{north_collaborative_2015,
	title = {Collaborative visual analysis with {RCloud}},
	doi = {10.1109/VAST.2015.7347627},
	abstract = {Consider the emerging role of data science teams embedded in larger organizations. Individual analysts work on loosely related problems, and must share their findings with each other and the organization at large, moving results from exploratory data analyses (EDA) into automated visualizations, diagnostics and reports deployed for wider consumption. There are two problems with the current practice. First, there are gaps in this workflow: EDA is performed with one set of tools, and automated reports and deployments with another. Second, these environments often assume a single-developer perspective, while data scientist teams could get much benefit from easier sharing of scripts and data feeds, experiments, annotations, and automated recommendations, which are well beyond what traditional version control systems provide. We contribute and justify the following three requirements for systems built to support current data science teams and users: discoverability, technology transfer, and coexistence. In addition, we contribute the design and implementation of RCloud, a system that supports the requirements of collaborative data analysis, visualization and web deployment. About 100 people used RCloud for two years. We report on interviews with some of these users, and discuss design decisions, tradeoffs and limitations in comparison to other approaches.},
	booktitle = {2015 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {North, Stephen and Scheidegger, Carlos and Urbanek, Simon and Woodhull, Gordon},
	month = oct,
	year = {2015},
	note = {ISSN: null},
	keywords = {Type of Work: Tool/Software, HOW: ???, Maybe related. RCloud allows distributed (collaborative) analysis in R, WHY: collaborative analysis (using R)},
	pages = {25--32},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\FHBBCXVC\\7347627.html:text/html}
}

@inproceedings{cook_mixed-initiative_2015,
	title = {Mixed-initiative visual analytics using task-driven recommendations},
	doi = {10.1109/VAST.2015.7347625},
	abstract = {Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.},
	booktitle = {2015 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	author = {Cook, Kristin and Cramer, Nick and Israel, David and Wolverton, Michael and Bruce, Joe and Burtner, Russ and Endert, Alex},
	month = oct,
	year = {2015},
	note = {ISSN: null},
	keywords = {WHEN - Real-Time Applications, Type of Work: Tool/Software, HOW - Probabilistic Models / Prediction, HOW: semantic interaction (ForceSPIRE), Type of Work: Design Guideline, WHY: mixed-initiative system, WHY: semi-automated foraging of information, WHY - Model Steering / Active Learning, ENCODING - Sequence},
	pages = {9--16},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\VWAGTF4K\\7347625.html:text/html}
}

@article{blascheck_va2:_2016,
	title = {{VA2}: {A} {Visual} {Analytics} {Approach} for {Evaluating} {Visual} {Analytics} {Applications}},
	volume = {22},
	issn = {2160-9306},
	shorttitle = {{VA2}},
	doi = {10.1109/TVCG.2015.2467871},
	abstract = {Evaluation has become a fundamental part of visualization research and researchers have employed many approaches from the field of human-computer interaction like measures of task performance, thinking aloud protocols, and analysis of interaction logs. Recently, eye tracking has also become popular to analyze visual strategies of users in this context. This has added another modality and more data, which requires special visualization techniques to analyze this data. However, only few approaches exist that aim at an integrated analysis of multiple concurrent evaluation procedures. The variety, complexity, and sheer amount of such coupled multi-source data streams require a visual analytics approach. Our approach provides a highly interactive visualization environment to display and analyze thinking aloud, interaction, and eye movement data in close relation. Automatic pattern finding algorithms allow an efficient exploratory search and support the reasoning process to derive common eye-interaction-thinking patterns between participants. In addition, our tool equips researchers with mechanisms for searching and verifying expected usage patterns. We apply our approach to a user study involving a visual analytics application and we discuss insights gained from this joint analysis. We anticipate our approach to be applicable to other combinations of evaluation techniques and a broad class of visualization applications.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Blascheck, Tanja and John, Markus and Kurzhals, Kuno and Koch, Steffen and Ertl, Thomas},
	month = jan,
	year = {2016},
	keywords = {WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, HOW - Pattern Analysis, Type of Work: Tool/Software, HOW: visual analytics tool, HOW: sequence (pattern) analysis, WHY: evaluating VA tool by analyzing user interactions (and eye tracking data) captured in those tools, ENCODING - Sequence},
	pages = {61--70},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\BZTUDL8T\\7192649.html:text/html}
}

@article{krause_supporting_2016,
	title = {Supporting {Iterative} {Cohort} {Construction} with {Visual} {Temporal} {Queries}},
	volume = {22},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2015.2467622},
	abstract = {Many researchers across diverse disciplines aim to analyze the behavior of cohorts whose behaviors are recorded in large event databases. However, extracting cohorts from databases is a difficult yet important step, often overlooked in many analytical solutions. This is especially true when researchers wish to restrict their cohorts to exhibit a particular temporal pattern of interest. In order to fill this gap, we designed COQUITO, a visual interface that assists users defining cohorts with temporal constraints. COQUITO was designed to be comprehensible to domain experts with no preknowledge of database queries and also to encourage exploration. We then demonstrate the utility of COQUITO via two case studies, involving medical and social media researchers.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Krause, Josua and Perer, Adam and Stavropoulos, Harry},
	month = jan,
	year = {2016},
	keywords = {HOW - Pattern Analysis, Type of Work: Tool/Software, HOW: interactive query construction, Maybe related. Mostly a tool to perform filtering / selection of similar data items, WHY: selecting similar entities that includes temporal constraints},
	pages = {91--100},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\H6A5IT8Q\\7192665.html:text/html}
}

@article{kim_interaxis:_2016,
	title = {{InterAxis}: {Steering} {Scatterplot} {Axes} via {Observation}-{Level} {Interaction}},
	volume = {22},
	issn = {2160-9306},
	shorttitle = {{InterAxis}},
	doi = {10.1109/TVCG.2015.2467615},
	abstract = {Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Kim, Hannah and Choo, Jaegul and Park, Haesun and Endert, Alex},
	month = jan,
	year = {2016},
	keywords = {Type of Work: Tool/Software, HOW: semantic interaction, Maybe related. Arbitrary generation of axis for scatterplots, WHY: model steering (of a scatterplot)},
	pages = {131--140},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\YYWSVPCZ\\7192671.html:text/html}
}

@article{nguyen_sensepath:_2016,
	title = {{SensePath}: {Understanding} the {Sensemaking} {Process} {Through} {Analytic} {Provenance}},
	volume = {22},
	issn = {2160-9306},
	shorttitle = {{SensePath}},
	doi = {10.1109/TVCG.2015.2467611},
	abstract = {Sensemaking is described as the process of comprehension, finding meaning and gaining insight from information, producing new knowledge and informing further action. Understanding the sensemaking process allows building effective visual analytics tools to make sense of large and complex datasets. Currently, it is often a manual and time-consuming undertaking to comprehend this: researchers collect observation data, transcribe screen capture videos and think-aloud recordings, identify recurring patterns, and eventually abstract the sensemaking process into a general model. In this paper, we propose a general approach to facilitate such a qualitative analysis process, and introduce a prototype, SensePath, to demonstrate the application of this approach with a focus on browser-based online sensemaking. The approach is based on a study of a number of qualitative research sessions including observations of users performing sensemaking tasks and post hoc analyses to uncover their sensemaking processes. Based on the study results and a follow-up participatory design session with HCI researchers, we decided to focus on the transcription and coding stages of thematic analysis. SensePath automatically captures user's sensemaking actions, i.e., analytic provenance, and provides multi-linked views to support their further analysis. A number of other requirements elicited from the design session are also implemented in SensePath, such as easy integration with existing qualitative analysis workflow and non-intrusive for participants. The tool was used by an experienced HCI researcher to analyze two sensemaking sessions. The researcher found the tool intuitive and considerably reduced analysis time, allowing better understanding of the sensemaking process.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Nguyen, Phong H. and Xu, Kai and Wheat, Ashley and Wong, B.L. William and Attfield, Simon and Fields, Bob},
	month = jan,
	year = {2016},
	keywords = {Type of Work: Tool/Software, HOW: visual analytics tool, WHY: understand sensemaking process},
	pages = {41--50},
	file = {Accepted Version:C\:\\Users\\conny\\Zotero\\storage\\LPN6KWDG\\Nguyen et al. - 2016 - SensePath Understanding the Sensemaking Process T.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\ZD752X68\\7194834.html:text/html}
}

@article{sacha_role_2016,
	title = {The {Role} of {Uncertainty}, {Awareness}, and {Trust} in {Visual} {Analytics}},
	volume = {22},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2015.2467591},
	abstract = {Visual analytics supports humans in generating knowledge from large and often complex datasets. Evidence is collected, collated and cross-linked with our existing knowledge. In the process, a myriad of analytical and visualisation techniques are employed to generate a visual representation of the data. These often introduce their own uncertainties, in addition to the ones inherent in the data, and these propagated and compounded uncertainties can result in impaired decision making. The user's confidence or trust in the results depends on the extent of user's awareness of the underlying uncertainties generated on the system side. This paper unpacks the uncertainties that propagate through visual analytics systems, illustrates how human's perceptual and cognitive biases influence the user's awareness of such uncertainties, and how this affects the user's trust building. The knowledge generation model for visual analytics is used to provide a terminology and framework to discuss the consequences of these aspects in knowledge construction and though examples, machine uncertainty is compared to human trust measures with provenance. Furthermore, guidelines for the design of uncertainty-aware systems are presented that can aid the user in better decision making.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Sacha, Dominik and Senaratne, Hansi and Kwon, Bum Chul and Ellis, Geoffrey and Keim, Daniel A.},
	month = jan,
	year = {2016},
	pages = {240--249},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\EYGANTZF\\7192716.html:text/html;Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\293P3EHX\\Sacha et al. - 2016 - The Role of Uncertainty, Awareness, and Trust in V.pdf:application/pdf}
}

@inproceedings{kandel_wrangler:_2011,
	address = {Vancouver, BC, Canada},
	title = {Wrangler: interactive visual specification of data transformation scripts},
	isbn = {978-1-4503-0228-9},
	shorttitle = {Wrangler},
	url = {http://dl.acm.org/citation.cfm?doid=1978942.1979444},
	doi = {10.1145/1978942.1979444},
	abstract = {Though data analysis tools continue to improve, analysts still expend an inordinate amount of time and effort manipulating data and assessing data quality issues. Such “data wrangling” regularly involves reformatting data values or layout, correcting erroneous or missing values, and integrating multiple data sources. These transforms are often difﬁcult to specify and difﬁcult to reuse across analysis tasks, teams, and tools. In response, we introduce Wrangler, an interactive system for creating data transformations. Wrangler combines direct manipulation of visualized data with automatic inference of relevant transforms, enabling analysts to iteratively explore the space of applicable operations and preview their effects. Wrangler leverages semantic data types (e.g., geographic locations, dates, classiﬁcation codes) to aid validation and type conversion. Interactive histories support review, reﬁnement, and annotation of transformation scripts. User study results show that Wrangler signiﬁcantly reduces speciﬁcation time and promotes the use of robust, auditable transforms instead of manual editing.},
	language = {en},
	urldate = {2019-12-19},
	booktitle = {Proceedings of the 2011 annual conference on {Human} factors in computing systems - {CHI} '11},
	publisher = {ACM Press},
	author = {Kandel, Sean and Paepcke, Andreas and Hellerstein, Joseph and Heer, Jeffrey},
	year = {2011},
	keywords = {ENCODING - Grammar, HOW - Program Synthesis, WHY - Data WranglingType of Work: System, WHEN - Hybrid Approaches, WHY - Data Wrangling, WHY - Real-time or post-hoc Quantification and Re-Application, Type of Work: System, ENCODING - Grammar},
	pages = {3363},
	file = {Kandel et al. - 2011 - Wrangler interactive visual specification of data.pdf:C\:\\Users\\conny\\Zotero\\storage\\ANVQSDD8\\Kandel et al. - 2011 - Wrangler interactive visual specification of data.pdf:application/pdf}
}

@inproceedings{chuang_interpretation_2012,
	address = {Austin, Texas, USA},
	title = {Interpretation and trust: designing model-driven visualizations for text analysis},
	isbn = {978-1-4503-1015-4},
	shorttitle = {Interpretation and trust},
	url = {http://dl.acm.org/citation.cfm?doid=2207676.2207738},
	doi = {10.1145/2207676.2207738},
	abstract = {Statistical topic models can help analysts discover patterns in large text corpora by identifying recurring sets of words and enabling exploration by topical concepts. However, understanding and validating the output of these models can itself be a challenging analysis task. In this paper, we offer two design considerations — interpretation and trust — for designing visualizations based on data-driven models. Interpretation refers to the facility with which an analyst makes inferences about the data through the lens of a model abstraction. Trust refers to the actual and perceived accuracy of an analyst’s inferences. These considerations derive from our experiences developing the Stanford Dissertation Browser, a tool for exploring over 9,000 Ph.D. theses by topical similarity, and a subsequent review of existing literature. We contribute a novel similarity measure for text collections based on a notion of “word-borrowing” that arose from an iterative design process. Based on our experiences and a literature review, we distill a set of design recommendations and describe how they promote interpretable and trustworthy visual analysis tools.},
	language = {en},
	urldate = {2019-12-20},
	booktitle = {Proceedings of the 2012 {ACM} annual conference on {Human} {Factors} in {Computing} {Systems} - {CHI} '12},
	publisher = {ACM Press},
	author = {Chuang, Jason and Ramage, Daniel and Manning, Christopher and Heer, Jeffrey},
	year = {2012},
	pages = {443},
	file = {Chuang et al. - 2012 - Interpretation and trust designing model-driven v.pdf:C\:\\Users\\conny\\Zotero\\storage\\DXI48YX8\\Chuang et al. - 2012 - Interpretation and trust designing model-driven v.pdf:application/pdf}
}

@inproceedings{lee_jigsawmap:_2012,
	address = {Austin, Texas, USA},
	title = {{JigsawMap}: connecting the past to the future by mapping historical textual cadasters},
	isbn = {978-1-4503-1015-4},
	shorttitle = {{JigsawMap}},
	url = {http://dl.acm.org/citation.cfm?doid=2207676.2207740},
	doi = {10.1145/2207676.2207740},
	abstract = {In this paper, we present an interactive visualization tool, JigsawMap, for visualizing and mapping historical textual cadasters. A cadaster is an official register that records land properties (e.g., location, ownership, value and size) for land valuation and taxation. Such mapping of old and new cadasters can help historians understand the social/economic background of changes in land uses or ownership. With JigsawMap, historians can continue mapping older or newer cadasters. In this way, JigsawMap can connect the past land survey results to today and to the future. We conducted usability studies and long term case studies to evaluate JigsawMap, and received positive responses. As well as summarizing the evaluation results, we also present design guidelines for participatory design projects with historians.},
	language = {en},
	urldate = {2019-12-20},
	booktitle = {Proceedings of the 2012 {ACM} annual conference on {Human} {Factors} in {Computing} {Systems} - {CHI} '12},
	publisher = {ACM Press},
	author = {Lee, Hyungmin and Lee, Sooyun and Kim, Namwook and Seo, Jinwook},
	year = {2012},
	pages = {463},
	file = {Lee et al. - 2012 - JigsawMap connecting the past to the future by ma.pdf:C\:\\Users\\conny\\Zotero\\storage\\DSXRJU2C\\Lee et al. - 2012 - JigsawMap connecting the past to the future by ma.pdf:application/pdf}
}

@article{elias_annotating_2012,
	title = {Annotating {BI} visualization dashboards: needs \&\#38; challenges},
	abstract = {Annotations have been identiﬁed as an important aid in analysis record-keeping and recently data discovery. In this paper we discuss the use of annotations on visualization dashboards, with a special focus on business intelligence (BI) analysis. In-depth interviews with experts lead to new annotation needs for multi-chart visualization systems, on which we based the design of a dashboard prototype that supports data and context aware annotations. We focus particularly on novel annotation aspects, such as multi-target annotations, annotation transparency across charts and data dimension levels, as well as annotation properties such as lifetime and validity. Moreover, our prototype is built on a data layer shared among different data-sources and BI applications, allowing cross application annotations. We discuss challenges in supporting context aware annotations in dashboards and other visualizations, such as dealing with changing annotated data, and provide design solutions. Finally we report reactions and recommendations from a different set of expert users.},
	language = {en},
	author = {Elias, Micheline and Bezerianos, Anastasia},
	year = {2012},
	pages = {10},
	file = {Elias and Bezerianos - 2012 - Annotating BI visualization dashboards needs &#38.pdf:C\:\\Users\\conny\\Zotero\\storage\\U3PE98SU\\Elias and Bezerianos - 2012 - Annotating BI visualization dashboards needs &#38.pdf:application/pdf}
}

@inproceedings{dunne_graphtrail:_2012,
	address = {Austin, Texas, USA},
	title = {{GraphTrail}: analyzing large multivariate, heterogeneous networks while supporting exploration history},
	isbn = {978-1-4503-1015-4},
	shorttitle = {{GraphTrail}},
	url = {http://dl.acm.org/citation.cfm?doid=2207676.2208293},
	doi = {10.1145/2207676.2208293},
	abstract = {Exploring large network datasets, such as scientific collaboration networks, is challenging because they often contain a large number of nodes and edges in several types and with multiple attributes. Analyses of such networks are often long and complex, and may require several sessions by multiple users. Therefore, it is often difficult for users to recall their own exploration history or share it with others. We introduce GraphTrail, an interactive visualization for analyzing networks through exploration of node and edge aggregates that captures users’ interactions and integrates this history directly in the exploration workspace. To facilitate large network analysis, GraphTrail integrates aggregation with familiar charts, drag-and-drop interaction on a canvas, and a novel pivoting mechanism for transitioning between aggregates. Through a three-month field study with a team of archeologists and a qualitative lab study with ten users, we demonstrate the effectiveness of our design and the benefits of integrated exploration history, including analysis comprehension, insight discovery, and exploration recall.},
	language = {en},
	urldate = {2019-12-20},
	booktitle = {Proceedings of the 2012 {ACM} annual conference on {Human} {Factors} in {Computing} {Systems} - {CHI} '12},
	publisher = {ACM Press},
	author = {Dunne, Cody and Henry Riche, Nathalie and Lee, Bongshin and Metoyer, Ronald and Robertson, George},
	year = {2012},
	keywords = {WHEN - Retrospective Analysis, Type of Work: User Study, WHY - Evaluation of Tools and Systems, WHY - Re-Application, HOW - Pattern Analysis, Type of Work: Tool/Software, HOW - Other},
	pages = {1663},
	file = {Dunne et al. - 2012 - GraphTrail analyzing large multivariate, heteroge.pdf:C\:\\Users\\conny\\Zotero\\storage\\8HP3YIHV\\Dunne et al. - 2012 - GraphTrail analyzing large multivariate, heteroge.pdf:application/pdf}
}

@inproceedings{gomez_modeling_2012,
	address = {Austin, Texas, USA},
	title = {Modeling task performance for a crowd of users from interaction histories},
	isbn = {978-1-4503-1015-4},
	url = {http://dl.acm.org/citation.cfm?doid=2207676.2208412},
	doi = {10.1145/2207676.2208412},
	abstract = {We present TOME, a novel framework that helps developers quantitatively evaluate user interfaces and design iterations by using histories from crowds of end users. TOME collects user-interaction histories via an interface instrumentation library as end users complete tasks; these histories are compiled using the Keystroke-Level Model (KLM) into task completion-time predictions using CogTool. With many histories, TOME can model prevailing strategies for tasks without needing an HCI specialist to describe users’ interaction steps. An unimplemented design change can be evaluated by perturbing a TOME task model in CogTool to reﬂect the change, giving a new performance prediction. We found that predictions for quick (5–60s) query tasks in an instrumented brain-map interface averaged within 10\% of measured expert times. Finally, we modiﬁed a TOME model to predict closely the speed-up yielded by a proposed interaction before implementing it.},
	language = {en},
	urldate = {2019-12-20},
	booktitle = {Proceedings of the 2012 {ACM} annual conference on {Human} {Factors} in {Computing} {Systems} - {CHI} '12},
	publisher = {ACM Press},
	author = {Gomez, Steven and Laidlaw, David},
	year = {2012},
	keywords = {WHEN - Retrospective Analysis, Type of Work: User Study, WHY - Evaluation of Tools and Systems, HOW - Probabilistic Models / PredictionENCODING - Model, HOW - Probabilistic Models / Prediction, Type of Work: Theory \& Model, Type of Work: User Study, WHEN - Retrospective Analysis, WHY - Evaluation of Tools and Systems, WHY - Report Generation / Storytelling, Type of Work: Theory \& Model, WHY - User Behavior / User Characteristics / User Modelling, ENCODING - Model},
	pages = {2465},
	file = {Gomez and Laidlaw - 2012 - Modeling task performance for a crowd of users fro.pdf:C\:\\Users\\conny\\Zotero\\storage\\GQML2XSE\\Gomez and Laidlaw - 2012 - Modeling task performance for a crowd of users fro.pdf:application/pdf}
}

@inproceedings{albers_task-driven_2014,
	address = {Toronto, Ontario, Canada},
	title = {Task-driven evaluation of aggregation in time series visualization},
	isbn = {978-1-4503-2473-1},
	url = {http://dl.acm.org/citation.cfm?doid=2556288.2557200},
	doi = {10.1145/2556288.2557200},
	abstract = {Many visualization tasks require the viewer to make judgments about aggregate properties of data. Recent work has shown that viewers can perform such tasks effectively, for example to efﬁciently compare the maximums or means over ranges of data. However, this work also shows that such effectiveness depends on the designs of the displays. In this paper, we explore this relationship between aggregation task and visualization design to provide guidance on matching tasks with designs. We combine prior results from perceptual science and graphical perception to suggest a set of design variables that inﬂuence performance on various aggregate comparison tasks. We describe how choices in these variables can lead to designs that are matched to particular tasks. We use these variables to assess a set of eight different designs, predicting how they will support a set of six aggregate time series comparison tasks. A crowd-sourced evaluation conﬁrms these predictions. These results not only provide evidence for how the speciﬁc visualizations support various tasks, but also suggest using the identiﬁed design variables as a tool for designing visualizations well suited for various types of tasks.},
	language = {en},
	urldate = {2019-12-20},
	booktitle = {Proceedings of the 32nd annual {ACM} conference on {Human} factors in computing systems - {CHI} '14},
	publisher = {ACM Press},
	author = {Albers, Danielle and Correll, Michael and Gleicher, Michael},
	year = {2014},
	pages = {551--560},
	file = {Albers et al. - 2014 - Task-driven evaluation of aggregation in time seri.pdf:C\:\\Users\\conny\\Zotero\\storage\\NJQACJGG\\Albers et al. - 2014 - Task-driven evaluation of aggregation in time seri.pdf:application/pdf}
}

@inproceedings{ragan_evaluating_2015,
	address = {Seoul, Republic of Korea},
	title = {Evaluating {How} {Level} of {Detail} of {Visual} {History} {Affects} {Process} {Memory}},
	isbn = {978-1-4503-3145-6},
	url = {http://dl.acm.org/citation.cfm?doid=2702123.2702376},
	doi = {10.1145/2702123.2702376},
	abstract = {Visual history tools provide visual representations of the workflow during data analysis tasks. While there is an established need for reviewing analytic processes, and many visual history tools provide visualizations to do so, it is not well known how helpful the tools actually are for process recall. Through a controlled experiment, we evaluated how the presence of a visual history aid and varying levels of visual detail affect process memory. Participants conducted an analysis task using a visual textdocument analysis tool. We evaluated their memories of the process both immediately after the analysis and then again one week later. Results showed that even visual history views with reduced data-resolution were effective for aiding process memory. Further, even without inclusion of any data in the visual history aids, the visual cues alone from the final workspace were enough to improve memory of the main themes of analyses.},
	language = {en},
	urldate = {2019-12-20},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems} - {CHI} '15},
	publisher = {ACM Press},
	author = {Ragan, Eric D. and Goodall, John R. and Tung, Albert},
	year = {2015},
	keywords = {WHEN - Retrospective Analysis, Type of Work: User Study, HOW - OTHERSencode - grammar, HOW - OTHERS, NOT SURE, Type of Work: User Study, WHEN - Retrospective Analysis, WHY - Report Generation and Storytelling, NOT SURE, WHY - User Behavior / User Characteristics / User Modelling, encode - grammar},
	pages = {2711--2720},
	file = {Ragan et al. - 2015 - Evaluating How Level of Detail of Visual History A.pdf:C\:\\Users\\conny\\Zotero\\storage\\48A273DC\\Ragan et al. - 2015 - Evaluating How Level of Detail of Visual History A.pdf:application/pdf}
}

@inproceedings{zgraggen_s|queries:_2015,
	address = {Seoul, Republic of Korea},
	title = {(s{\textbar}qu)eries: {Visual} {Regular} {Expressions} for {Querying} and {Exploring} {Event} {Sequences}},
	isbn = {978-1-4503-3145-6},
	shorttitle = {(s{\textbar}qu)eries},
	url = {http://dl.acm.org/citation.cfm?doid=2702123.2702262},
	doi = {10.1145/2702123.2702262},
	abstract = {Many different domains collect event sequence data and rely on ﬁnding and analyzing patterns within it to gain meaningful insights. Current systems that support such queries either provide limited expressiveness, hinder exploratory workﬂows or present interaction and visualization models which do not scale well to large and multi-faceted data sets. In this paper we present (s{\textbar}qu)eries (pronounced “Squeries”), a visual query interface for creating queries on sequences (series) of data, based on regular expressions. (s{\textbar}qu)eries is a touchbased system that exposes the full expressive power of regular expressions in an approachable way and interleaves query speciﬁcation with result visualizations. Being able to visually investigate the results of different query-parts supports debugging and encourages iterative query-building as well as exploratory work-ﬂows. We validate our design and implementation through a set of informal interviews with data scientists that analyze event sequences on a daily basis.},
	language = {en},
	urldate = {2019-12-20},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems} - {CHI} '15},
	publisher = {ACM Press},
	author = {Zgraggen, Emanuel and Drucker, Steven M. and Fisher, Danyel and DeLine, Robert},
	year = {2015},
	keywords = {encoding - graph, encoding - signal, HOW - Program Synthesis, Type of Work: System, WHEN - Real-Time Applications, WHY - Real-time or post-hoc Quantification and Re-Application, Type of Work: System, encoding - signal, encoding - graph},
	pages = {2683--2692},
	file = {Zgraggen et al. - 2015 - (squ)eries Visual Regular Expressions for Queryi.pdf:C\:\\Users\\conny\\Zotero\\storage\\6WL5KDMB\\Zgraggen et al. - 2015 - (squ)eries Visual Regular Expressions for Queryi.pdf:application/pdf}
}

@inproceedings{wang_docuviz:_2015,
	address = {Seoul, Republic of Korea},
	title = {{DocuViz}: {Visualizing} {Collaborative} {Writing}},
	isbn = {978-1-4503-3145-6},
	shorttitle = {{DocuViz}},
	url = {http://dl.acm.org/citation.cfm?doid=2702123.2702517},
	doi = {10.1145/2702123.2702517},
	abstract = {Collaborative writing is on the increase. In order to write well together, authors often need to be aware of who has done what recently. We offer a new tool, DocuViz, that displays the entire revision history of Google Docs, showing more than the one-step-at–a-time view now shown in revision history and tracking changes in Word. We introduce the tool and present cases in which the tool has the potential to be useful: To authors themselves to see recent “seismic activity,” indicating where in particular a co-author might want to pay attention, to instructors to see who has contributed what and which changes were made to comments from them, and to researchers interested in the new patterns of collaboration made possible by simultaneous editing capabilities.},
	language = {en},
	urldate = {2019-12-20},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems} - {CHI} '15},
	publisher = {ACM Press},
	author = {Wang, Dakuo and Olson, Judith S. and Zhang, Jingwen and Nguyen, Trung and Olson, Gary M.},
	year = {2015},
	keywords = {Out of Scope, Visualizing Provenance},
	pages = {1865--1874},
	file = {Wang et al. - 2015 - DocuViz Visualizing Collaborative Writing.pdf:C\:\\Users\\conny\\Zotero\\storage\\GUMI2UFR\\Wang et al. - 2015 - DocuViz Visualizing Collaborative Writing.pdf:application/pdf}
}

@inproceedings{krause_interacting_2016,
	address = {Santa Clara, California, USA},
	title = {Interacting with {Predictions}: {Visual} {Inspection} of {Black}-box {Machine} {Learning} {Models}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {Interacting with {Predictions}},
	url = {http://dl.acm.org/citation.cfm?doid=2858036.2858529},
	doi = {10.1145/2858036.2858529},
	abstract = {Understanding predictive models, in terms of interpreting and identifying actionable insights, is a challenging task. Often the importance of a feature in a model is only a rough estimate condensed into one number. However, our research goes beyond these na¨ıve estimates through the design and implementation of an interactive visual analytics system, Prospector. By providing interactive partial dependence diagnostics, data scientists can understand how features affect the prediction overall. In addition, our support for localized inspection allows data scientists to understand how and why speciﬁc datapoints are predicted as they are, as well as support for tweaking feature values and seeing how the prediction responds. Our system is then evaluated using a case study involving a team of data scientists improving predictive models for detecting the onset of diabetes from electronic medical records.},
	language = {en},
	urldate = {2019-12-20},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems} - {CHI} '16},
	publisher = {ACM Press},
	author = {Krause, Josua and Perer, Adam and Ng, Kenney},
	year = {2016},
	keywords = {WHY - Evaluation of Tools and Systems, WHY - Explain ML model},
	pages = {5686--5697},
	file = {Krause et al. - 2016 - Interacting with Predictions Visual Inspection of.pdf:C\:\\Users\\conny\\Zotero\\storage\\ISWATUYS\\Krause et al. - 2016 - Interacting with Predictions Visual Inspection of.pdf:application/pdf}
}

@inproceedings{kralj_tagrefinery:_2017,
	address = {Denver, Colorado, USA},
	title = {{TagRefinery}: {A} {Visual} {Tool} for {Tag} {Wrangling}},
	isbn = {978-1-4503-4655-9},
	shorttitle = {{TagRefinery}},
	url = {http://dl.acm.org/citation.cfm?doid=3025453.3025868},
	doi = {10.1145/3025453.3025868},
	abstract = {We present TagReﬁnery, an interactive visual application aiding the cleaning and processing of open tag spaces, such as those in Last.fm or YouTube. Our pre-design analysis showed a need to support a spectrum of user expertise from novice to advanced, which resulted in two distinct interface modes. Summative evaluations of TagReﬁnery showed that it could effectively guide the novice users through the workﬂow by giving them brief but helpful explanations on why each step was required, and providing visual and statistical aids to help them in making important decisions. This is while our more expert users greatly appreciated the amount of control and granularity over the workﬂow that our more advanced interface mode offered. Both the underlying tag cleaning workﬂow and the interface were designed iteratively in a participatory design process in collaboration with research on a music recommendation interface based on Last.fm tags.},
	language = {en},
	urldate = {2019-12-20},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems} - {CHI} '17},
	publisher = {ACM Press},
	author = {Kralj, Christoph and Kamalzadeh, Mohsen and Möller, Torsten},
	year = {2017},
	keywords = {WHEN - Real-Time Applications, WHY - Recommender Systems},
	pages = {2928--2939},
	file = {Kralj et al. - 2017 - TagRefinery A Visual Tool for Tag Wrangling.pdf:C\:\\Users\\conny\\Zotero\\storage\\3I2AAND9\\Kralj et al. - 2017 - TagRefinery A Visual Tool for Tag Wrangling.pdf:application/pdf}
}

@article{andrienko_identifying_2011,
	title = {Identifying {Place} {Histories} from {Activity} {Traces} with an {Eye} to {Parameter} {Impact}},
	volume = {18},
	abstract = {Events that happened in the past are important for understanding the ongoing processes, predicting future developments, and making informed decisions. Important and/or interesting events tend to attract many people. Some people leave traces of their attendance in the form of computer-processable data, such as records in the databases of mobile phone operators or photos on photo sharing web sites. We developed a suite of visual analytics methods for reconstructing past events from these activity traces. Our tools combine geocomputations, interactive geovisualizations, and statistical methods to enable integrated analysis of the spatial, temporal, and thematic components of the data, including numeric attributes and texts.We also support interactive investigation of the sensitivity of the analysis results to the parameters used in the computations. For this purpose, statistical summaries of computation results obtained with different combinations of parameter values are visualized in a way facilitating comparisons. We demonstrate the utility of our approach on two large real data sets, mobile phone calls in Milano during 9 days and flickr photos made on British Isles during 5 years.},
	number = {5},
	journal = {TVCG},
	author = {Andrienko, Gennady and Andrienko, Natalia and Mladenov, Martin and Mock, Michael and Poelitz, Christian},
	year = {2011},
	keywords = {WHEN - Hybrid Approaches, HOW - Other, WHY - Real-time or post-hoc Quantification and Re-Application, Type of Work: System},
	pages = {675--688}
}

@article{healey_interest_2012,
	title = {Interest {Driven} {Navigation} in {Visualization}},
	volume = {18},
	abstract = {This paper describes a new method to explore and discover within a large data set. We apply techniques from preference elicitation to automatically identify data elements that are of potential interest to the viewer. These “elements of interest (EOI)” are bundled into spatially local clusters, and connected together to form a graph. The graph is used to build camera paths that allow viewers to “tour” areas of interest (AOI) within their data. It is also visualized to provide wayfinding cues. Our preference model uses Bayesian classification to tag elements in a data set as interesting or not interesting to the viewer. The model responds in real time, updating the elements of interest based on a viewer’s actions. This allows us to track a viewer’s interests as they change during exploration and analysis. Viewers can also interact directly with interest rules the preference model defines. We demonstrate our theoretical results by visualizing historical climatology data collected at locations throughout the world.},
	number = {10},
	journal = {TVCG},
	author = {Healey, Christopher and Bennis, Brent},
	year = {2012},
	keywords = {WHEN - Real-Time Applications, WHY - Adaptive Systems / Guidance, HOW - Probabilistic Models / Prediction, HOW - Classification Models, Type of Work: Theory \& Model, WHY - User Behavior / User Characteristics / User Modelling},
	pages = {1744--1756},
	file = {Citeseer - Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\3Y6RNN8A\\Healey et al. - This article has been accepted for publication in .pdf:application/pdf}
}

@article{schlinder_multiverse_2013,
	title = {Multiverse {Data}-{Flow} {Control}},
	volume = {19},
	abstract = {In this paper, we present a data-flow system which supports comparative analysis of time-dependent data and interactive simulation steering. The system creates data on-the-fly to allow for the exploration of different parameters and the investigation of multiple scenarios. Existing data-flow architectures provide no generic approach to handle modules that perform complex temporal processing such as particle tracing or statistical analysis over time. Moreover, there is no solution to create and manage module data, which is associated with alternative scenarios. Our solution is based on generic data-flow algorithms to automate this process, enabling elaborate data-flow procedures, such as simulation, temporal integration or data aggregation over many time steps in many worlds. To hide the complexity from the user, we extend the World Lines interaction techniques to control the novel data-flow architecture. The concept of multiple, special-purpose cursors is introduced to let users intuitively navigate through time and alternative scenarios. Users specify only what they want to see, the decision which data are required is handled automatically. The concepts are explained by taking the example of the simulation and analysis of material transport in levee-breach scenarios. To strengthen the general applicability, we demonstrate the investigation of vortices in an offline-simulated dam-break data set.},
	number = {6},
	journal = {TVCG},
	author = {Schlinder, Benjamin and Waser, Juergen and Ribicic, Hrvoje and Fuchs, Raphael and Peikert, Ronald},
	year = {2013},
	keywords = {WHEN - Real-Time Applications, WHY - Evaluation of Tools and Systems, Type of Work: Empirical Study, Type of Work: Technique \& Algorithm, HOW - Other, WHY - Model Steering / Active Learning},
	pages = {1006--1018},
}

@ARTICLE{ribicic_visual_2013,  author={H. {Ribičić} and J. {Waser} and R. {Fuchs} and G. {Blöschl} and E. {Gröller}},  journal={IEEE Transactions on Visualization and Computer Graphics},  title={Visual Analysis and Steering of Flooding Simulations},   year={2013},  volume={19},  number={6},  pages={1062-1075}}

@ARTICLE{pezzotti_approximated_2015,  author={H. {Ribičić} and J. {Waser} and R. {Fuchs} and G. {Blöschl} and E. {Gröller}},  journal={IEEE Transactions on Visualization and Computer Graphics},  title={Visual Analysis and Steering of Flooding Simulations},   year={2013},  volume={19},  number={6},  pages={1062-1075},}

@article{blascheck_exploration_2018,
	title = {Exploration {Strategies} for {Discovery} of {Interactivity} in {Visualization}},
	volume = {25},
	abstract = {We investigate how people discover the functionality of an interactive visualization that was designed for the general public. While interactive visualizations are increasingly available for public use, we still know little about how the general public discovers what they can do with these visualizations and what interactions are available. Developing a better understanding of this discovery process can help inform the design of visualizations for the general public, which in turn can help make data more accessible. To unpack this problem, we conducted a lab study in which participants were free to use their own methods to discover the functionality of a connected set of interactive visualizations of public energy data. We collected eye movement data and interaction logs as well as video and audio recordings. By analyzing this combined data, we extract exploration strategies that the participants employed to discover the functionality in these interactive visualizations. These exploration strategies illuminate possible design directions for improving the discoverability of a visualization’s functionality.},
	number = {2},
	journal = {TVCG},
	author = {Blascheck, Tanja and Vermeulen, Lindsay and Vermeulen, Jo and Perin, Charles and Willett, Wesley and Ertl, Thomas and Carpendale, Sheelagh},
	year = {2018},
	keywords = {HOW - Pattern Analysis, Type of Work: Empirical Study, HOW - Classification Models, WHEN - Retrospective Analyses, WHY - User Behavior / User Characteristics / User Modelling},
	pages = {1407--1420}
}

@inproceedings{le_bras_improving_2018,
	address = {Montreal QC, Canada},
	title = {Improving {User} {Confidence} in {Concept} {Maps}: {Exploring} {Data} {Driven} {Explanations}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {Improving {User} {Confidence} in {Concept} {Maps}},
	url = {http://dl.acm.org/citation.cfm?doid=3173574.3173978},
	doi = {10.1145/3173574.3173978},
	abstract = {Automated tools are increasingly being used to generate highly engaging concept maps as an aid to strategic planning and other decision-making tasks. Unless stakeholders can understand the principles of the underlying layout process, however, we have found that they lack conﬁdence and are therefore reluctant to use these maps. In this paper, we present a qualitative study exploring the effect on users’ conﬁdence of using data-driven explanation mechanisms, by conducting in-depth scenario-based interviews with ten participants. To provide diversity in stimulus and approach we use two explanation mechanisms based on projection and agglomerative layout methods. The themes exposed in our results indicate that the data-driven explanations improved user conﬁdence in several ways, and that process clarity and layout density also affected users’ views of the credibility of the concept maps. We discuss how these factors can increase uptake of automated tools and affect user conﬁdence.},
	language = {en},
	urldate = {2019-12-20},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '18},
	publisher = {ACM Press},
	author = {Le Bras, Pierre and Robb, David A. and Methven, Thomas S. and Padilla, Stefano and Chantler, Mike J.},
	year = {2018},
	pages = {1--13},
	annote = {Excluded because there is no vis/interaction provenance data.},
	file = {Le Bras et al. - 2018 - Improving User Confidence in Concept Maps Explori.pdf:C\:\\Users\\conny\\Zotero\\storage\\4ULT78KX\\Le Bras et al. - 2018 - Improving User Confidence in Concept Maps Explori.pdf:application/pdf}
}

@article{nguyen_understanding_2018,
	title = {Understanding {User} {Behaviour} through {Action} {Sequences}:  {From} the {Usual} to the {Unusual}},
	volume = {25},
	abstract = {Action sequences, where atomic user actions are represented in a labelled, timestamped form, are becoming a fundamental data asset in the inspection and monitoring of user behaviour in digital systems. Although the analysis of such sequences is highly critical to the investigation of activities in cyber security applications, existing solutions fail to provide a comprehensive understanding due to the complex semantic and temporal characteristics of these data. This paper presents a visual analytics approach that aims to facilitate a user-involved, multi-faceted decision making process during the identification and the investigation of “unusual” action sequences. We first report the results of the task analysis and domain characterisation process. Then we describe the components of our multi-level analysis approach that comprises of constraint-based sequential pattern mining and semantic distance based clustering, and multi-scalar visualisations of users and their sequences. Finally, we demonstrate the applicability of our approach through a case study that involves tasks requiring effective decision-making by a group of domain experts. Although our solution here is tightly informed by a user-centred, domain-focused design process, we present findings and techniques that are transferable to other applications where the analysis of such sequences is of interest.},
	number = {9},
	journal = {TVCG},
	author = {Nguyen, Phong and Turkay, Cagatay and Andrienko, Gennady and Andrienko, Natalia and Thonnard, Olivier and Zouaoui, Jihane},
	year = {2018},
	keywords = {HOW - Pattern Analysis, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, WHEN - Retrospective Analyses, WHY - User Behavior / User Characteristics / User Modelling},
	pages = {2838--2852}
}

@article{nguyen_vasabi:_2020,
	title = {{VASABI}: {Hierarchical} {User} {Profiles} for {Interactive} {Visual} {User} {Behaviour} {Analytics}},
	volume = {26},
	issn = {2160-9306},
	shorttitle = {{VASABI}},
	doi = {10.1109/TVCG.2019.2934609},
	abstract = {User behaviour analytics (UBA) systems offer sophisticated models that capture users' behaviour over time with an aim to identify fraudulent activities that do not match their profiles. Motivated by the challenges in the interpretation of UBA models, this paper presents a visual analytics approach to help analysts gain a comprehensive understanding of user behaviour at multiple levels, namely individual and group level. We take a user-centred approach to design a visual analytics framework supporting the analysis of collections of users and the numerous sessions of activities they conduct within digital applications. The framework is centred around the concept of hierarchical user profiles that are built based on features derived from sessions, as well as on user tasks extracted using a topic modelling approach to summarise and stratify user behaviour. We externalise a series of analysis goals and tasks, and evaluate our methods through use cases conducted with experts. We observe that with the aid of interactive visual hierarchical user profiles, analysts are able to conduct exploratory and investigative analysis effectively, and able to understand the characteristics of user behaviour to make informed decisions whilst evaluating suspicious users and activities.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Nguyen, Phong H. and Henkin, Rafael and Chen, Siming and Andrienko, Natalia and Andrienko, Gennady and Thonnard, Olivier and Turkay, Cagatay},
	month = jan,
	year = {2020},
	keywords = {Data visualization, Visual analytics, Analytical models, Task analysis, Buildings, Feature extraction, cybersecurity, hierarchical user profiles, user behaviour analytics, Type of Work: Tool/Software, Type of Work: Empirical Study, HOW - Classification Models, Myabe related. Visual analytics of user behavior analysis, WHY: compare user groups, WHY: identify common analysis patterns, WHY - User Behavior / User Characteristics / User Modelling, ENCODING - Vector},
	pages = {77--86},
	file = {Accepted Version:C\:\\Users\\conny\\Zotero\\storage\\XWPEMTRR\\Nguyen et al. - 2020 - VASABI Hierarchical User Profiles for Interactive.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\conny\\Zotero\\storage\\42Z6SXY8\\8807354.html:text/html}
}

@article{oliveira_provenance_2018,
	title = {Provenance {Analytics} for {Workflow}-{Based} {Computational} {Experiments}: {A} {Survey}},
	volume = {51},
	url = {https://dl.acm.org/doi/abs/10.1145/3184900},
	abstract = {Until not long ago, manually capturing and storing provenance from scientific experiments were constant concerns for scientists. With the advent of computational experiments (modeled as scientific ...},
	language = {EN},
	number = {3},
	urldate = {2020-01-08},
	journal = {ACM Computing Surveys (CSUR)},
	author = {Oliveira, Wellington and De Oliveira, Daniel and Braganholo, Vanessa},
	month = may,
	year = {2018}
}

@article{freire_provenance_2008,
	title = {Provenance for {Computational} {Tasks}: {A} {Survey}},
	volume = {10},
	issn = {1558-366X},
	shorttitle = {Provenance for {Computational} {Tasks}},
	doi = {10.1109/MCSE.2008.79},
	abstract = {The problem of systematically capturing and managing provenance for computational tasks has recently received significant attention because of its relevance to a wide range of domains and applications. The authors give an overview of important concepts related to provenance management, so that potential users can make informed decisions when selecting or designing a provenance solution.},
	number = {3},
	journal = {Computing in Science Engineering},
	author = {Freire, Juliana and Koop, David and Santos, Emanuele and Silva, Cláudio T.},
	month = may,
	year = {2008},
	pages = {11--21},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\N5QL7A7M\\Freire et al. - 2008 - Provenance for Computational Tasks A Survey.pdf:application/pdf}
}

@misc{noauthor_challenges_nodate,
	title = {Challenges in irreproducible research},
	url = {https://www.nature.com/collections/prbfkwmwvz},
	urldate = {2020-01-08}
}

@book{dix_human-computer_2003,
	address = {Harlow, England ; New York},
	edition = {3 edition},
	title = {Human-{Computer} {Interaction}},
	isbn = {978-0-13-046109-4},
	abstract = {The second edition of Human-Computer Interaction established itself as one of the classic textbooks in the area, with its broad coverage and rigorous approach, this new edition builds on the existing strengths of the book, but giving the text a more student-friendly slant and improving the coverage in certain areas. The revised structure, separating out the introductory and more advanced material will make it easier to use the book on a variety of courses. This new edition now includes chapters on Interaction Design, Universal Access and Rich Interaction, as well as covering the latest developments in ubiquitous computing and Web technologies, making it the ideal text to provide a grounding in HCI theory and practice.},
	language = {English},
	publisher = {Prentice Hall},
	author = {Dix, Alan and Finlay, Janet and Abowd, Gregory D. and Beale, Russell},
	month = sep,
	year = {2003}
}

@book{preece_interaction_2015,
	address = {Chichester},
	edition = {4th edition},
	title = {Interaction {Design}: {Beyond} {Human}-{Computer} {Interaction}},
	isbn = {978-1-119-02075-2},
	shorttitle = {Interaction {Design}},
	abstract = {A new edition of the \#1 text in the Human Computer Interaction field! Hugely popular with students and professionals alike, Interaction Design is an ideal resource for learning the interdisciplinary skills needed for interaction design, human computer interaction, information design, web design and ubiquitous computing. This text offers a cross-disciplinary, practical and process-oriented introduction to the field, showing not just what principles ought to apply to interaction design, but crucially how they can be applied. An accompanying website contains extensive additional teaching and learning material including slides for each chapter, comments on chapter activities and a number of in-depth case studies written by researchers and designers.},
	language = {English},
	publisher = {John Wiley},
	author = {Preece, Jenny and Sharp, Helen and Rogers, Yvonne},
	month = feb,
	year = {2015}
}

@article{bunemanpeter_data_2019,
	title = {Data {Provenance}: {What} next?},
	volume = {47},
	doi = {https://doi.org/10.1145/3316416.3316418},
	abstract = {Research into data provenance has been active for almost twenty years. What has it delivered and where will it go next? What practical impact has it had and what might it have? We provide speculati...},
	language = {EN},
	number = {3},
	urldate = {2020-01-10},
	journal = {ACM SIGMOD Record},
	author = {BunemanPeter and TanWang-Chiew},
	month = feb,
	year = {2019},
	pages = {5--16}
}

@incollection{chirigati_provenance_2017,
	address = {New York, NY},
	title = {Provenance and {Reproducibility}},
	isbn = {978-1-4899-7993-3},
	url = {https://doi.org/10.1007/978-1-4899-7993-3_80747-1},
	language = {en},
	urldate = {2020-01-10},
	booktitle = {Encyclopedia of {Database} {Systems}},
	publisher = {Springer},
	author = {Chirigati, Fernando and Freire, Juliana},
	editor = {Liu, Ling and Özsu, M. Tamer},
	year = {2017},
	doi = {10.1007/978-1-4899-7993-3_80747-1},
	pages = {1--5}
}

@article{ivie_reproducibility_2018,
	title = {Reproducibility in {Scientific} {Computing}},
	volume = {51},
	url = {https://dl.acm.org/doi/abs/10.1145/3186266},
	abstract = {Reproducibility is widely considered to be an essential requirement of the scientific process. However, a number of serious concerns have been raised recently, questioning whether today’s computati...},
	language = {EN},
	number = {3},
	urldate = {2020-01-10},
	journal = {ACM Computing Surveys (CSUR)},
	author = {Ivie, Peter and Thain, Douglas},
	month = jul,
	year = {2018}
}

@article{boserajendra_lineage_2005,
	title = {Lineage retrieval for scientific data processing: a survey},
	volume = {37},
	shorttitle = {Lineage retrieval for scientific data processing},
	url = {https://dl.acm.org/doi/abs/10.1145/1057977.1057978},
	abstract = {Scientific research relies as much on the dissemination and exchange of data sets as on the publication of conclusions. Accurately tracking the lineage (origin and subsequent processing history) of...},
	language = {EN},
	number = {1},
	urldate = {2020-01-10},
	journal = {ACM Computing Surveys (CSUR)},
	author = {BoseRajendra and FrewJames},
	month = mar,
	year = {2005}
}

@article{l_survey_2005,
	title = {A survey of data provenance in e-science},
	volume = {34},
	url = {https://dl.acm.org/doi/abs/10.1145/1084805.1084812},
	abstract = {Data management is growing in complexity as large-scale applications take advantage of the loosely coupled resources brought together by grid middleware and by abundant storage capacity. Metadata d...},
	language = {EN},
	number = {3},
	urldate = {2020-01-10},
	journal = {ACM SIGMOD Record},
	author = {L, SimmhanYogesh and PlaleBeth and GannonDennis},
	month = sep,
	year = {2005}
}

@article{herschel_survey_2017,
	title = {A survey on provenance: {What} for? {What} form? {What} from?},
	volume = {26},
	issn = {0949-877X},
	shorttitle = {A survey on provenance},
	url = {https://doi.org/10.1007/s00778-017-0486-1},
	doi = {10.1007/s00778-017-0486-1},
	abstract = {Provenance refers to any information describing the production process of an end product, which can be anything from a piece of digital data to a physical object. While this survey focuses on the former type of end product, this definition still leaves room for many different interpretations of and approaches to provenance. These are typically motivated by different application domains for provenance (e.g., accountability, reproducibility, process debugging) and varying technical requirements such as runtime, scalability, or privacy. As a result, we observe a wide variety of provenance types and provenance-generating methods. This survey provides an overview of the research field of provenance, focusing on what provenance is used for (what for?), what types of provenance have been defined and captured for the different applications (what form?), and which resources and system requirements impact the choice of deploying a particular provenance solution (what from?). For each of these three key questions, we provide a classification and review the state of the art for each class. We conclude with a summary and possible future research challenges.},
	language = {en},
	number = {6},
	urldate = {2020-01-10},
	journal = {The VLDB Journal},
	author = {Herschel, Melanie and Diestelkämper, Ralf and Ben Lahmar, Houssem},
	month = dec,
	year = {2017},
	pages = {881--906}
}

@misc{noauthor_7_nodate,
	title = {(7) {Provenance} {Analytics} for {Workflow}-{Based} {Computational} {Experiments}: {A} {Survey} {\textbar} {Request} {PDF}},
	shorttitle = {(7) {Provenance} {Analytics} for {Workflow}-{Based} {Computational} {Experiments}},
	url = {https://www.researchgate.net/publication/325325494_Provenance_Analytics_for_Workflow-Based_Computational_Experiments_A_Survey},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	language = {en},
	urldate = {2020-01-10},
	journal = {ResearchGate}
}

@article{silva_provenance_2007,
	title = {Provenance for {Visualizations}: {Reproducibility} and {Beyond}},
	volume = {9},
	issn = {1558-366X},
	shorttitle = {Provenance for {Visualizations}},
	doi = {10.1109/MCSE.2007.106},
	abstract = {The demand for the construction of complex visualizations is growing in many disciplines of science, as users are faced with ever increasing volumes of data to analyze. In this paper, the authors present VisTrails, an open source provenance-management system that provides infrastructure for data exploration and visualization. VisTrails transparently records detailed provenance of exploratory computational tasks and leverages this information beyond just the ability to reproduce and share results. In particular, it uses this information to simplify the process of exploring data through visualization.},
	number = {5},
	journal = {Computing in Science Engineering},
	author = {Silva, Claudio T. and Freire, Juliana and Callahan, Steven P.},
	month = sep,
	year = {2007},
	pages = {82--89},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\conny\\Zotero\\storage\\HQYWMXW3\\Silva et al. - 2007 - Provenance for Visualizations Reproducibility and.pdf:application/pdf}
}

@inproceedings{callahan2006vistrails,
  title={VisTrails: visualization meets data management},
  author={Callahan, Steven P and Freire, Juliana and Santos, Emanuele and Scheidegger, Carlos E and Silva, Cl{\'a}udio T and Vo, Huy T},
  booktitle={Proceedings of the 2006 ACM SIGMOD international conference on Management of data},
  pages={745--747},
  year={2006}
}

@article{walch2019lightguider,
  title={LightGuider: Guiding Interactive Lighting Design using Suggestions, Provenance, and Quality Visualization},
  author={Walch, Andreas and Schw{\"a}rzler, Michael and Luksch, Christian and Eisemann, Elmar and Gschwandtner, Theresia},
  journal={IEEE transactions on visualization and computer graphics},
  volume={26},
  number={1},
  pages={569--578},
  year={2019},
  publisher={IEEE}
}

@inproceedings{north_analytic_2011,
	title = {Analytic provenance: process+interaction+insight},
	url = {https://dl.acm.org/doi/abs/10.1145/1979742.1979570},
	language = {EN},
	urldate = {2020-01-10},
	booktitle = {{CHI} '11 {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	author = {North, Chris and Chang, Remco and Endert, Alex and Dou, Wenwen and May, Richard and Pike, Bill and Fink, Glenn},
	year = {2011},
	pages = {33--36}
}

@inproceedings{noauthor_notitle_nodate,
	doi = {10.1145/3332165.3347866},
	keywords = {WHEN - Real-Time Applications, WHY - Adaptive Systems / Guidance, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, WHEN - Real-Time Applications, WHY - Adaptive Systems / Guidance},
	file = {Synner-UIST2019.pdf:C\:\\Users\\conny\\Zotero\\storage\\WFJNSC3X\\Synner-UIST2019.pdf:application/pdf}
}

@article{fan_fast_2018,
	title = {Fast and {Accurate} {CNN}-based {Brushing} in {Scatterplots}},
	volume = {37},
	issn = {01677055},
	url = {http://doi.wiley.com/10.1111/cgf.13405},
	doi = {10.1111/cgf.13405},
	abstract = {Brushing plays a central role in most modern visual analytics solutions and effective and efﬁcient techniques for data selection are key to establishing a successful human-computer dialogue. With this paper, we address the need for brushing techniques that are both fast, enabling a ﬂuid interaction in visual data exploration and analysis, and also accurate, i.e., enabling the user to effectively select speciﬁc data subsets, even when their geometric delimination is non-trivial. We present a new solution for a near-perfect sketch-based brushing technique, where we exploit a convolutional neural network (CNN) for estimating the intended data selection from a fast and simple click-and-drag interaction and from the data distribution in the visualization. Our key contributions include a drastically reduced error rate—now below 3\%, i.e., less than half of the so far best accuracy—and an extension to a larger variety of selected data subsets, going beyond previous limitations due to linear estimation models.},
	language = {en},
	number = {3},
	urldate = {2020-01-21},
	journal = {Computer Graphics Forum},
	author = {Fan, Chaoran and Hauser, Helwig},
	month = jun,
	year = {2018},
	pages = {111--120},
	file = {Fan and Hauser - 2018 - Fast and Accurate CNN-based Brushing in Scatterplo.pdf:C\:\\Users\\conny\\Zotero\\storage\\CV3PSQXD\\Fan and Hauser - 2018 - Fast and Accurate CNN-based Brushing in Scatterplo.pdf:application/pdf}
}

@inproceedings{beck_state_2014,
	title = {The {State} of the {Art} in {Visualizing} {Dynamic} {Graphs}},
	abstract = {Dynamic graph visualization focuses on the challenge of representing the evolution of relationships between entities in readable, scalable, and effective diagrams. This work surveys the growing number of approaches in this discipline. We derive a hierarchical taxonomy of techniques by systematically categorizing and tagging publications. While static graph visualizations are often divided into node-link and matrix representations, we identify the representation of time as the major distinguishing feature for dynamic graph visualizations: either graphs are represented as animated diagrams or as static charts based on a timeline. Evaluations of animated approaches focus on dynamic stability for preserving the viewer’s mental map or, in general, compare animated diagrams to timeline-based ones. Finally, we identify and discuss challenges for future research.},
	language = {en},
	booktitle = {Eurographics {Conference} on {Visualization} ({EuroVis})},
	author = {Beck, Fabian and Burch, Michael and Diehl, Stephan and Weiskopf, Daniel},
	year = {2014},
	pages = {21},
	file = {Beck et al. - 2014 - The State of the Art in Visualizing Dynamic Graphs.pdf:C\:\\Users\\conny\\Zotero\\storage\\5GCR8VM9\\Beck et al. - 2014 - The State of the Art in Visualizing Dynamic Graphs.pdf:application/pdf}
}

@article{ottley_follow_2019,
	title = {Follow {The} {Clicks}: {Learning} and {Anticipating} {Mouse} {Interactions} {During} {Exploratory} {Data} {Analysis}},
	volume = {38},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13670},
	doi = {10.1111/cgf.13670},
	abstract = {Abstract The goal of visual analytics is to create a symbiosis between human and computer by leveraging their unique strengths. While this model has demonstrated immense success, we are yet to realize the full potential of such a human-computer partnership. In a perfect collaborative mixed-initiative system, the computer must possess skills for learning and anticipating the users' needs. Addressing this gap, we propose a framework for inferring attention from passive observations of the user's click, thereby allowing accurate predictions of future events. We demonstrate this technique with a crime map and found that users' clicks can appear in our prediction set 92\% - 97\% of the time. Further analysis shows that we can achieve high prediction accuracy typically after three clicks. Altogether, we show that passive observations of interaction data can reveal valuable information that will allow the system to learn and anticipate future events.},
	number = {3},
	journal = {Computer Graphics Forum},
	author = {Ottley, Alvitta and Garnett, Roman and Wan, Ran},
	year = {2019},
	keywords = {• Human-centered computing → Visual analytics, CCS Concepts, concepts and paradigms, Visualization theory, WHEN - Real-Time Applications, WHY - Adaptive Systems / Guidance, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, Type of Work: Technique \& Algorithm, WHY - User Behavior / User Characteristics / User Modelling},
	pages = {41--52},
	file = {Ottley et al. - 2019 - Follow The Clicks Learning and Anticipating Mouse.pdf:C\:\\Users\\conny\\Zotero\\storage\\IL6G8JCV\\Ottley et al. - 2019 - Follow The Clicks Learning and Anticipating Mouse.pdf:application/pdf}
}

@inproceedings{ottley_personality_2015,
	address = {Seoul, Republic of Korea},
	title = {Personality as a {Predictor} of {User} {Strategy}: {How} {Locus} of {Control} {Affects} {Search} {Strategies} on {Tree} {Visualizations}},
	isbn = {978-1-4503-3145-6},
	shorttitle = {Personality as a {Predictor} of {User} {Strategy}},
	url = {http://dl.acm.org/citation.cfm?doid=2702123.2702590},
	doi = {10.1145/2702123.2702590},
	language = {en},
	urldate = {2020-01-23},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems} - {CHI} '15},
	publisher = {ACM Press},
	author = {Ottley, Alvitta and Yang, Huahai and Chang, Remco},
	year = {2015},
	pages = {3251--3254}
}

@article{aigner_evalbench_2013,
	title = {{EvalBench}: {A} {Software} {Library} for {Visualization} {Evaluation}},
	volume = {32},
	issn = {01677055},
	shorttitle = {{EvalBench}},
	url = {http://doi.wiley.com/10.1111/cgf.12091},
	doi = {10.1111/cgf.12091},
	language = {en},
	number = {3pt1},
	urldate = {2020-01-23},
	journal = {Computer Graphics Forum},
	author = {Aigner, W. and Hoffmann, S. and Rind, A.},
	month = jun,
	year = {2013},
	pages = {41--50},
	file = {Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\VKRGD6S6\\Aigner et al. - 2013 - EvalBench A Software Library for Visualization Ev.pdf:application/pdf}
}

@inproceedings{battle_dynamic_2016,
	address = {San Francisco, California, USA},
	title = {Dynamic {Prefetching} of {Data} {Tiles} for {Interactive} {Visualization}},
	isbn = {978-1-4503-3531-7},
	url = {http://dl.acm.org/citation.cfm?doid=2882903.2882919},
	doi = {10.1145/2882903.2882919},
	language = {en},
	urldate = {2020-01-23},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data} - {SIGMOD} '16},
	publisher = {ACM Press},
	author = {Battle, Leilani and Chang, Remco and Stonebraker, Michael},
	year = {2016},
	pages = {1363--1375},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\VRY7TU69\\Battle et al. - 2016 - Dynamic Prefetching of Data Tiles for Interactive .pdf:application/pdf}
}

@article{battle_role_2019,
	title = {The {Role} of {Latency} and {Task} {Complexity} in {Predicting} {Visual} {Search} {Behavior}},
	issn = {1077-2626, 1941-0506, 2160-9306},
	url = {https://ieeexplore.ieee.org/document/8809742/},
	doi = {10.1109/TVCG.2019.2934556},
	urldate = {2020-01-23},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Battle, Leilani and Crouser, R. Jordan and Nakeshimana, Audace and Montoly, Ananda and Chang, Remco and Stonebraker, Michael},
	year = {2019},
	pages = {1--1}
}

@article{bryan_temporal_2017,
	title = {Temporal {Summary} {Images}: {An} {Approach} to {Narrative} {Visualization} via {Interactive} {Annotation} {Generation} and {Placement}},
	volume = {23},
	issn = {1077-2626},
	shorttitle = {Temporal {Summary} {Images}},
	url = {http://ieeexplore.ieee.org/document/7539294/},
	doi = {10.1109/TVCG.2016.2598876},
	number = {1},
	urldate = {2020-01-23},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Bryan, Chris and Ma, Kwan-Liu and Woodring, Jonathan},
	month = jan,
	year = {2017},
	pages = {511--520}
}

@article{chinchor_science_2009,
	title = {The {Science} of {Analytic} {Reporting}},
	volume = {8},
	issn = {1473-8716, 1473-8724},
	url = {http://journals.sagepub.com/doi/10.1057/ivs.2009.21},
	doi = {10.1057/ivs.2009.21},
	language = {en},
	number = {4},
	urldate = {2020-01-23},
	journal = {Information Visualization},
	author = {Chinchor, Nancy and Pike, William A.},
	month = jan,
	year = {2009},
	pages = {286--293}
}

@article{collins_guidance_2018,
	title = {Guidance in the human–machine analytics process},
	volume = {2},
	issn = {2468502X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2468502X1830041X},
	doi = {10.1016/j.visinf.2018.09.003},
	language = {en},
	number = {3},
	urldate = {2020-01-23},
	journal = {Visual Informatics},
	author = {Collins, Christopher and Andrienko, Natalia and Schreck, Tobias and Yang, Jing and Choo, Jaegul and Engelke, Ulrich and Jena, Amit and Dwyer, Tim},
	month = sep,
	year = {2018},
	pages = {166--180},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\UIW3NH5D\\Collins et al. - 2018 - Guidance in the human–machine analytics process.pdf:application/pdf}
}

@inproceedings{steichen_user-adaptive_2013,
	address = {Santa Monica, California, USA},
	title = {User-adaptive information visualization: using eye gaze data to infer visualization tasks and user cognitive abilities},
	isbn = {978-1-4503-1965-2},
	shorttitle = {User-adaptive information visualization},
	url = {http://dl.acm.org/citation.cfm?doid=2449396.2449439},
	doi = {10.1145/2449396.2449439},
	language = {en},
	urldate = {2020-01-23},
	booktitle = {Proceedings of the 2013 international conference on {Intelligent} user interfaces - {IUI} '13},
	publisher = {ACM Press},
	author = {Steichen, Ben and Carenini, Giuseppe and Conati, Cristina},
	year = {2013},
	pages = {317}
}

@article{gajos_automatically_2010,
	title = {Automatically generating personalized user interfaces with {Supple}},
	volume = {174},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370210000822},
	doi = {10.1016/j.artint.2010.05.005},
	language = {en},
	number = {12-13},
	urldate = {2020-01-23},
	journal = {Artificial Intelligence},
	author = {Gajos, Krzysztof Z. and Weld, Daniel S. and Wobbrock, Jacob O.},
	month = aug,
	year = {2010},
	pages = {910--950},
	file = {Full Text:C\:\\Users\\conny\\Zotero\\storage\\WLIZYY5D\\Gajos et al. - 2010 - Automatically generating personalized user interfa.pdf:application/pdf}
}

@article{kodagoda_using_2017,
	title = {Using {Machine} {Learning} to {Infer} {Reasoning} {Provenance} {From} {User} {Interaction} {Log} {Data}: {Based} on the {Data}/{Frame} {Theory} of {Sensemaking}},
	volume = {11},
	issn = {1555-3434},
	shorttitle = {Using {Machine} {Learning} to {Infer} {Reasoning} {Provenance} {From} {User} {Interaction} {Log} {Data}},
	url = {http://journals.sagepub.com/doi/10.1177/1555343416672782},
	doi = {10.1177/1555343416672782},
	language = {en},
	number = {1},
	urldate = {2020-01-23},
	journal = {Journal of Cognitive Engineering and Decision Making},
	author = {Kodagoda, Neesha and Pontis, Sheila and Simmie, Donal and Attfield, Simon and Wong, B. L. William and Blandford, Ann and Hankin, Chris},
	month = mar,
	year = {2017},
	pages = {23--41},
	file = {Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\QFB5RNWW\\Kodagoda et al. - 2017 - Using Machine Learning to Infer Reasoning Provenan.pdf:application/pdf}
}

@article{pohl_analysing_2012,
	title = {Analysing {Interactivity} in {Information} {Visualisation}},
	volume = {26},
	issn = {0933-1875, 1610-1987},
	url = {http://link.springer.com/10.1007/s13218-012-0167-6},
	doi = {10.1007/s13218-012-0167-6},
	language = {en},
	number = {2},
	urldate = {2020-01-23},
	journal = {KI - Künstliche Intelligenz},
	author = {Pohl, Margit and Wiltner, Sylvia and Miksch, Silvia and Aigner, Wolfgang and Rind, Alexander},
	month = may,
	year = {2012},
	pages = {151--159}
}

@article{pohl_using_2016,
	title = {Using lag-sequential analysis for understanding interaction sequences in visualizations},
	volume = {96},
	issn = {10715819},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581916300829},
	doi = {10.1016/j.ijhcs.2016.07.006},
	language = {en},
	urldate = {2020-01-23},
	journal = {International Journal of Human-Computer Studies},
	author = {Pohl, Margit and Wallner, Günter and Kriglstein, Simone},
	month = dec,
	year = {2016},
	pages = {54--66}
}

@inproceedings{wall_warning_2017,
	address = {Phoenix, AZ},
	title = {Warning, {Bias} {May} {Occur}: {A} {Proposed} {Approach} to {Detecting} {Cognitive} {Bias} in {Interactive} {Visual} {Analytics}},
	isbn = {978-1-5386-3163-8},
	shorttitle = {Warning, {Bias} {May} {Occur}},
	url = {https://ieeexplore.ieee.org/document/8585669/},
	doi = {10.1109/VAST.2017.8585669},
	urldate = {2020-01-23},
	booktitle = {2017 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	publisher = {IEEE},
	author = {Wall, Emily and Blaha, Leslie M. and Franklin, Lyndsey and Endert, Alex},
	month = oct,
	year = {2017},
	pages = {104--115}
}

@article{willett_scented_2007,
	title = {Scented {Widgets}: {Improving} {Navigation} {Cues} with {Embedded} {Visualizations}},
	volume = {13},
	issn = {1077-2626},
	shorttitle = {Scented {Widgets}},
	url = {http://ieeexplore.ieee.org/document/4376132/},
	doi = {10.1109/TVCG.2007.70589},
	number = {6},
	urldate = {2020-01-23},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Willett, Wesley and Heer, Jeffrey and Agrawala, Maneesh},
	month = nov,
	year = {2007},
	pages = {1129--1136}
}

@inproceedings{xiao_enhancing_2006,
	address = {Baltimore, MD, USA},
	title = {Enhancing {Visual} {Analysis} of {Network} {Traffic} {Using} a {Knowledge} {Representation}},
	isbn = {978-1-4244-0591-6 978-1-4244-0592-3},
	url = {http://ieeexplore.ieee.org/document/4035754/},
	doi = {10.1109/VAST.2006.261436},
	urldate = {2020-01-23},
	booktitle = {2006 {IEEE} {Symposium} {On} {Visual} {Analytics} {And} {Technology}},
	publisher = {IEEE},
	author = {Xiao, Ling and Gerth, John and Hanrahan, Pat},
	month = oct,
	year = {2006},
	pages = {107--114}
}

@article{xu_analytic_2015,
	title = {Analytic {Provenance} for {Sensemaking}: {A} {Research} {Agenda}},
	volume = {35},
	issn = {0272-1716},
	shorttitle = {Analytic {Provenance} for {Sensemaking}},
	url = {http://ieeexplore.ieee.org/document/7111922/},
	doi = {10.1109/MCG.2015.50},
	number = {3},
	urldate = {2020-01-23},
	journal = {IEEE Computer Graphics and Applications},
	author = {Xu, Kai and Attfield, Simon and Jankun-Kelly, T.J. and Wheat, Ashley and Nguyen, Phong H. and Selvaraj, Nallini},
	month = may,
	year = {2015},
	pages = {56--64},
	file = {Accepted Version:C\:\\Users\\conny\\Zotero\\storage\\WPADEYX3\\Xu et al. - 2015 - Analytic Provenance for Sensemaking A Research Ag.pdf:application/pdf}
}

@book{snook_handbook_2012,
	address = {Thousand Oaks},
	title = {The handbook for teaching leadership: knowing, doing, and being},
	isbn = {978-1-4129-9094-3},
	shorttitle = {The handbook for teaching leadership},
	publisher = {SAGE Publications},
	editor = {Snook, Scott A. and Nohria, Nitin and Khurana, Rakesh},
	year = {2012},
	keywords = {Leadership, Study and teaching},
	annote = {Sensemaking : framing and acting in the unknown / Deborah Ancona -- Cases in leadership education : implications of human cognition / Michael Mumford ... [et al.] -- Becoming leadership literate : a core cirriculum / Barbara Kellerman -- Educating contemporary princes and princesses for power / Jose Luis Alvarez -- Teaching global leadership / Mansour Javidan -- The spirit of leadership : new directions in leadership education / Ken Starkey \& Carol Hall -- Learning to lead at Harvard Business School / Tom DeLong \& Linda Hill -- The leadership template / Michael Useem -- Mastering the art of leadership : an experiential approach from the performing arts / Belle Halpern, Richard Richards -- Teaching executives to be themselves "more" with skill : a sociological perspective on a personal question / Robert Goffee, Gareth Jones -- High performance leadership / Andrew Meikle -- Leadership effectiveness and development : building self-awareness and insight skills / Stacey Kole, Jeffrey Anderson -- Developing naturally : from management to organization to society to selves / Henry Mintzberg -- Being a leader : mental strength for leadership / Louis Csoka -- Developing leaders of consequence / Joseph LeBoeuf ... [et al.] -- Creating leaders : an ontological/phenomenological model / Werner Erhard, Michael Jensen, Kari Granger -- Transformational leadership development programs : creating long-term sustainable change / Manfred Kets de Vries, Konstantin Korotov -- An approach to teaching values-based leadership / James O'Toole -- Identity workspaces for leadership development / Gianpiero Petriglieri -- Authentic leadership development / Bill George -- Forging consciousness and (occasionally) conscience : a model based approach to leadership development / Mihnea Moldoveanu -- Learning to lead : pedagogy of practice / Marshall Ganz \& Emily S. Lin -- Teaching leadership with the brain and mind / David Rock, Al Ringleb -- The company command forum : teaching leadership outside the formal organizational structure / Anthony Burgess -- City year : developing idealistic leaders through national service / Max Klau -- Project GLOBE : global leadership and organizational behavior education / Marcus Dickson ... [et al.] -- Leadership acceleration at Goldman Sachs / Shoma Chatterjee, Cary Friedman, Keith Yardley -- Developing interdependent leadership / Charles Palus, John McGuire, Chris Ernst -- Developing business innovators who integrate profitability and social value / Nancy McGaw -- Re-developing leaders : the Harvard advanced leadership experiment in even higher education / Rosabeth Moss Kanter}
}

@article{brehmer_multi-level_2013,
	title = {A {Multi}-{Level} {Typology} of {Abstract} {Visualization} {Tasks}},
	volume = {19},
	issn = {1077-2626},
	url = {http://ieeexplore.ieee.org/document/6634168/},
	doi = {10.1109/TVCG.2013.124},
	number = {12},
	urldate = {2020-01-23},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Brehmer, Matthew and Munzner, Tamara},
	month = dec,
	year = {2013},
	pages = {2376--2385},
	file = {Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\VEEFHY2R\\Brehmer and Munzner - 2013 - A Multi-Level Typology of Abstract Visualization T.pdf:application/pdf}
}

@article{lu_survey_2007,
	title = {A survey of image classification methods and techniques for improving classification performance},
	volume = {28},
	issn = {0143-1161, 1366-5901},
	url = {https://www.tandfonline.com/doi/full/10.1080/01431160600746456},
	doi = {10.1080/01431160600746456},
	language = {en},
	number = {5},
	urldate = {2020-01-23},
	journal = {International Journal of Remote Sensing},
	author = {Lu, D. and Weng, Q.},
	month = mar,
	year = {2007},
	pages = {823--870}
}

@inproceedings{nath_survey_2014,
	address = {Kanyakumari District, India},
	title = {A survey of image classification methods and techniques},
	isbn = {978-1-4799-4190-2 978-1-4799-4191-9 978-1-4799-4193-3 978-1-4799-4192-6},
	url = {http://ieeexplore.ieee.org/document/6993023/},
	doi = {10.1109/ICCICCT.2014.6993023},
	urldate = {2020-01-23},
	booktitle = {2014 {International} {Conference} on {Control}, {Instrumentation}, {Communication} and {Computational} {Technologies} ({ICCICCT})},
	publisher = {IEEE},
	author = {Nath, Siddhartha Sankar and Mishra, Girish and Kar, Jajnyaseni and Chakraborty, Sayan and Dey, Nilanjan},
	month = jul,
	year = {2014},
	pages = {554--557}
}

@incollection{aggarwal_survey_2012,
	address = {Boston, MA},
	title = {A {Survey} of {Text} {Classification} {Algorithms}},
	isbn = {978-1-4614-3222-7 978-1-4614-3223-4},
	url = {http://link.springer.com/10.1007/978-1-4614-3223-4_6},
	language = {en},
	urldate = {2020-01-23},
	booktitle = {Mining {Text} {Data}},
	publisher = {Springer US},
	author = {Aggarwal, Charu C. and Zhai, ChengXiang},
	editor = {Aggarwal, Charu C. and Zhai, ChengXiang},
	year = {2012},
	doi = {10.1007/978-1-4614-3223-4_6},
	pages = {163--222},
	file = {Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\TTD9PHJH\\Aggarwal and Zhai - 2012 - A Survey of Text Classification Algorithms.pdf:application/pdf}
}

@article{song_short_2014,
	title = {Short {Text} {Classification}: {A} {Survey}},
	volume = {9},
	issn = {1796-2048},
	shorttitle = {Short {Text} {Classification}},
	url = {http://ojs.academypublisher.com/index.php/jmm/article/view/12635},
	doi = {10.4304/jmm.9.5.635-643},
	number = {5},
	urldate = {2020-01-23},
	journal = {Journal of Multimedia},
	author = {Song, Ge and Ye, Yunming and Du, Xiaolin and Huang, Xiaohui and Bie, Shifu},
	month = may,
	year = {2014},
	pages = {635--643}
}

@article{lloyd_least_1982,
	title = {Least squares quantization in {PCM}},
	volume = {28},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1056489/},
	doi = {10.1109/TIT.1982.1056489},
	language = {en},
	number = {2},
	urldate = {2020-01-23},
	journal = {IEEE Transactions on Information Theory},
	author = {Lloyd, S.},
	month = mar,
	year = {1982},
	pages = {129--137},
	file = {Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\68GK278A\\Lloyd - 1982 - Least squares quantization in PCM.pdf:application/pdf}
}

@inproceedings{mannino_is_2019,
	address = {New Orleans LA USA},
	title = {Is this {Real}?: {Generating} {Synthetic} {Data} that {Looks} {Real}},
	isbn = {978-1-4503-6816-2},
	shorttitle = {Is this {Real}?},
	url = {http://dl.acm.org/doi/10.1145/3332165.3347866},
	doi = {10.1145/3332165.3347866},
	language = {en},
	urldate = {2020-01-23},
	booktitle = {Proceedings of the 32nd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Mannino, Miro and Abouzied, Azza},
	month = oct,
	year = {2019},
	keywords = {WHEN - Real-Time Applications, WHY - Adaptive Systems / Guidance, HOW - Probabilistic Models / Prediction, Type of Work: Empirical Study, WHEN - Real-Time Applications, WHY - Adaptive Systems / Guidance},
	pages = {549--561}
}

@inproceedings{garg_model-driven_2008,
	address = {Columbus, OH, USA},
	title = {Model-driven {Visual} {Analytics}},
	isbn = {978-1-4244-2935-6},
	url = {http://ieeexplore.ieee.org/document/4677352/},
	doi = {10.1109/VAST.2008.4677352},
	urldate = {2020-01-23},
	booktitle = {2008 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	publisher = {IEEE},
	author = {Garg, Supriya and Nam, Julia Eunju and Ramakrishnan, I.V. and Mueller, Klaus},
	month = oct,
	year = {2008},
	pages = {19--26},
	file = {Submitted Version:C\:\\Users\\conny\\Zotero\\storage\\X2FWDD6X\\Garg et al. - 2008 - Model-driven Visual Analytics.pdf:application/pdf}
}

@techreport{perry_supporting_2009,
	title = {Supporting {Cognitive} {Models} of {Sensemaking} in {Analytics} {Systems}},
	language = {en},
	author = {Perry, Jason and Janneck, Christopher D},
	year = {2009},
	pages = {20},
	number = {2009-12},
	institution = {DIMACS},
}

@book{snook_handbook_2012-1,
	address = {Thousand Oaks},
	title = {The handbook for teaching leadership: knowing, doing, and being},
	isbn = {978-1-4129-9094-3},
	shorttitle = {The handbook for teaching leadership},
	publisher = {SAGE Publications},
	editor = {Snook, Scott A. and Nohria, Nitin and Khurana, Rakesh},
	year = {2012},
	keywords = {Leadership, Study and teaching},
	annote = {Sensemaking : framing and acting in the unknown / Deborah Ancona -- Cases in leadership education : implications of human cognition / Michael Mumford ... [et al.] -- Becoming leadership literate : a core cirriculum / Barbara Kellerman -- Educating contemporary princes and princesses for power / Jose Luis Alvarez -- Teaching global leadership / Mansour Javidan -- The spirit of leadership : new directions in leadership education / Ken Starkey \& Carol Hall -- Learning to lead at Harvard Business School / Tom DeLong \& Linda Hill -- The leadership template / Michael Useem -- Mastering the art of leadership : an experiential approach from the performing arts / Belle Halpern, Richard Richards -- Teaching executives to be themselves "more" with skill : a sociological perspective on a personal question / Robert Goffee, Gareth Jones -- High performance leadership / Andrew Meikle -- Leadership effectiveness and development : building self-awareness and insight skills / Stacey Kole, Jeffrey Anderson -- Developing naturally : from management to organization to society to selves / Henry Mintzberg -- Being a leader : mental strength for leadership / Louis Csoka -- Developing leaders of consequence / Joseph LeBoeuf ... [et al.] -- Creating leaders : an ontological/phenomenological model / Werner Erhard, Michael Jensen, Kari Granger -- Transformational leadership development programs : creating long-term sustainable change / Manfred Kets de Vries, Konstantin Korotov -- An approach to teaching values-based leadership / James O'Toole -- Identity workspaces for leadership development / Gianpiero Petriglieri -- Authentic leadership development / Bill George -- Forging consciousness and (occasionally) conscience : a model based approach to leadership development / Mihnea Moldoveanu -- Learning to lead : pedagogy of practice / Marshall Ganz \& Emily S. Lin -- Teaching leadership with the brain and mind / David Rock, Al Ringleb -- The company command forum : teaching leadership outside the formal organizational structure / Anthony Burgess -- City year : developing idealistic leaders through national service / Max Klau -- Project GLOBE : global leadership and organizational behavior education / Marcus Dickson ... [et al.] -- Leadership acceleration at Goldman Sachs / Shoma Chatterjee, Cary Friedman, Keith Yardley -- Developing interdependent leadership / Charles Palus, John McGuire, Chris Ernst -- Developing business innovators who integrate profitability and social value / Nancy McGaw -- Re-developing leaders : the Harvard advanced leadership experiment in even higher education / Rosabeth Moss Kanter}
}

@inproceedings{garg_model-driven_2008-1,
	address = {Columbus, OH, USA},
	title = {Model-driven {Visual} {Analytics}},
	isbn = {978-1-4244-2935-6},
	url = {http://ieeexplore.ieee.org/document/4677352/},
	doi = {10.1109/VAST.2008.4677352},
	booktitle = {2008 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	publisher = {IEEE},
	author = {Garg, Supriya and Nam, Julia Eunju and Ramakrishnan, I.V. and Mueller, Klaus},
	month = oct,
	year = {2008},
	pages = {19--26}
}

@article{lloyd_least_1982-1,
	title = {Least squares quantization in {PCM}},
	volume = {28},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1056489/},
	doi = {10.1109/TIT.1982.1056489},
	language = {en},
	number = {2},
	journal = {IEEE Transactions on Information Theory},
	author = {Lloyd, S.},
	month = mar,
	year = {1982},
	pages = {129--137}
}

@incollection{aggarwal_survey_2012-1,
	address = {Boston, MA},
	title = {A {Survey} of {Text} {Classification} {Algorithms}},
	isbn = {978-1-4614-3222-7 978-1-4614-3223-4},
	url = {http://link.springer.com/10.1007/978-1-4614-3223-4_6},
	language = {en},
	booktitle = {Mining {Text} {Data}},
	publisher = {Springer US},
	author = {Aggarwal, Charu C. and Zhai, ChengXiang},
	editor = {Aggarwal, Charu C. and Zhai, ChengXiang},
	year = {2012},
	doi = {10.1007/978-1-4614-3223-4_6},
	pages = {163--222}
}

@inproceedings{nath_survey_2014-1,
	address = {Kanyakumari District, India},
	title = {A survey of image classification methods and techniques},
	isbn = {978-1-4799-4190-2 978-1-4799-4191-9 978-1-4799-4193-3 978-1-4799-4192-6},
	url = {http://ieeexplore.ieee.org/document/6993023/},
	doi = {10.1109/ICCICCT.2014.6993023},
	booktitle = {2014 {International} {Conference} on {Control}, {Instrumentation}, {Communication} and {Computational} {Technologies} ({ICCICCT})},
	publisher = {IEEE},
	author = {Nath, Siddhartha Sankar and Mishra, Girish and Kar, Jajnyaseni and Chakraborty, Sayan and Dey, Nilanjan},
	month = jul,
	year = {2014},
	pages = {554--557}
}

@article{lu_survey_2007-1,
	title = {A survey of image classification methods and techniques for improving classification performance},
	volume = {28},
	issn = {0143-1161, 1366-5901},
	url = {https://www.tandfonline.com/doi/full/10.1080/01431160600746456},
	doi = {10.1080/01431160600746456},
	language = {en},
	number = {5},
	journal = {International Journal of Remote Sensing},
	author = {Lu, D. and Weng, Q.},
	month = mar,
	year = {2007},
	pages = {823--870}
}

@article{brehmer_multi-level_2013-1,
	title = {A {Multi}-{Level} {Typology} of {Abstract} {Visualization} {Tasks}},
	volume = {19},
	issn = {1077-2626},
	url = {http://ieeexplore.ieee.org/document/6634168/},
	doi = {10.1109/TVCG.2013.124},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Brehmer, Matthew and Munzner, Tamara},
	month = dec,
	year = {2013},
	pages = {2376--2385}
}

@article{xu_analytic_2015-1,
	title = {Analytic {Provenance} for {Sensemaking}: {A} {Research} {Agenda}},
	volume = {35},
	issn = {0272-1716},
	shorttitle = {Analytic {Provenance} for {Sensemaking}},
	url = {http://ieeexplore.ieee.org/document/7111922/},
	doi = {10.1109/MCG.2015.50},
	number = {3},
	journal = {IEEE Computer Graphics and Applications},
	author = {Xu, Kai and Attfield, Simon and Jankun-Kelly, T.J. and Wheat, Ashley and Nguyen, Phong H. and Selvaraj, Nallini},
	month = may,
	year = {2015},
	pages = {56--64}
}

@inproceedings{xiao_enhancing_2006-1,
	address = {Baltimore, MD, USA},
	title = {Enhancing {Visual} {Analysis} of {Network} {Traffic} {Using} a {Knowledge} {Representation}},
	isbn = {978-1-4244-0591-6 978-1-4244-0592-3},
	url = {http://ieeexplore.ieee.org/document/4035754/},
	doi = {10.1109/VAST.2006.261436},
	booktitle = {2006 {IEEE} {Symposium} {On} {Visual} {Analytics} {And} {Technology}},
	publisher = {IEEE},
	author = {Xiao, Ling and Gerth, John and Hanrahan, Pat},
	month = oct,
	year = {2006},
	pages = {107--114}
}

@article{pohl_analysing_2012-1,
	title = {Analysing {Interactivity} in {Information} {Visualisation}},
	volume = {26},
	issn = {0933-1875, 1610-1987},
	url = {http://link.springer.com/10.1007/s13218-012-0167-6},
	doi = {10.1007/s13218-012-0167-6},
	language = {en},
	number = {2},
	journal = {KI - Künstliche Intelligenz},
	author = {Pohl, Margit and Wiltner, Sylvia and Miksch, Silvia and Aigner, Wolfgang and Rind, Alexander},
	month = may,
	year = {2012},
	pages = {151--159}
}

@article{gajos_automatically_2010-1,
	title = {Automatically generating personalized user interfaces with {Supple}},
	volume = {174},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370210000822},
	doi = {10.1016/j.artint.2010.05.005},
	language = {en},
	number = {12-13},
	journal = {Artificial Intelligence},
	author = {Gajos, Krzysztof Z. and Weld, Daniel S. and Wobbrock, Jacob O.},
	month = aug,
	year = {2010},
	pages = {910--950}
}

@article{collins_guidance_2018-1,
	title = {Guidance in the human–machine analytics process},
	volume = {2},
	issn = {2468502X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2468502X1830041X},
	doi = {10.1016/j.visinf.2018.09.003},
	language = {en},
	number = {3},
	journal = {Visual Informatics},
	author = {Collins, Christopher and Andrienko, Natalia and Schreck, Tobias and Yang, Jing and Choo, Jaegul and Engelke, Ulrich and Jena, Amit and Dwyer, Tim},
	month = sep,
	year = {2018},
	pages = {166--180}
}

@inproceedings{battle_dynamic_2016-1,
	address = {San Francisco, California, USA},
	title = {Dynamic {Prefetching} of {Data} {Tiles} for {Interactive} {Visualization}},
	isbn = {978-1-4503-3531-7},
	url = {http://dl.acm.org/citation.cfm?doid=2882903.2882919},
	doi = {10.1145/2882903.2882919},
	language = {en},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data} - {SIGMOD} '16},
	publisher = {ACM Press},
	author = {Battle, Leilani and Chang, Remco and Stonebraker, Michael},
	year = {2016},
	pages = {1363--1375}
}

@article{aigner_evalbench_2013-1,
	title = {{EvalBench}: {A} {Software} {Library} for {Visualization} {Evaluation}},
	volume = {32},
	issn = {01677055},
	shorttitle = {{EvalBench}},
	url = {http://doi.wiley.com/10.1111/cgf.12091},
	doi = {10.1111/cgf.12091},
	language = {en},
	number = {3pt1},
	journal = {Computer Graphics Forum},
	author = {Aigner, W. and Hoffmann, S. and Rind, A.},
	month = jun,
	year = {2013},
	pages = {41--50}
}

@inproceedings{north_analytic_2011-1,
	title = {Analytic provenance: process+interaction+insight},
	url = {https://dl.acm.org/doi/abs/10.1145/1979742.1979570},
	language = {EN},
	booktitle = {{CHI} '11 {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	author = {North, Chris and Chang, Remco and Endert, Alex and Dou, Wenwen and May, Richard and Pike, Bill and Fink, Glenn},
	year = {2011},
	pages = {33--36}
}



@article{boserajendra_lineage_2005-1,
	title = {Lineage retrieval for scientific data processing: a survey},
	volume = {37},
	shorttitle = {Lineage retrieval for scientific data processing},
	url = {https://dl.acm.org/doi/abs/10.1145/1057977.1057978},
	abstract = {Scientific research relies as much on the dissemination and exchange of data sets as on the publication of conclusions. Accurately tracking the lineage (origin and subsequent processing history) of...},
	language = {EN},
	number = {1},
	journal = {ACM Computing Surveys (CSUR)},
	author = {{BoseRajendra} and {FrewJames}},
	month = mar,
	year = {2005}
}

@article{bunemanpeter_data_2019-1,
	title = {Data {Provenance}: {What} next?},
	volume = {47},
	doi = {https://doi.org/10.1145/3316416.3316418},
	abstract = {Research into data provenance has been active for almost twenty years. What has it delivered and where will it go next? What practical impact has it had and what might it have? We provide speculati...},
	language = {EN},
	number = {3},
	journal = {ACM SIGMOD Record},
	author = {{BunemanPeter} and {TanWang-Chiew}},
	month = feb,
	year = {2019},
	pages = {5--16}
}

@book{preece_interaction_2015-1,
	address = {Chichester},
	edition = {4th edition},
	title = {Interaction {Design}: {Beyond} {Human}-{Computer} {Interaction}},
	isbn = {978-1-119-02075-2},
	shorttitle = {Interaction {Design}},
	abstract = {A new edition of the \#1 text in the Human Computer Interaction field! Hugely popular with students and professionals alike, Interaction Design is an ideal resource for learning the interdisciplinary skills needed for interaction design, human computer interaction, information design, web design and ubiquitous computing. This text offers a cross-disciplinary, practical and process-oriented introduction to the field, showing not just what principles ought to apply to interaction design, but crucially how they can be applied. An accompanying website contains extensive additional teaching and learning material including slides for each chapter, comments on chapter activities and a number of in-depth case studies written by researchers and designers.},
	language = {English},
	publisher = {John Wiley},
	author = {Preece, Jenny and Sharp, Helen and Rogers, Yvonne},
	month = feb,
	year = {2015}
}

@book{dix_human-computer_2003-1,
	address = {Harlow, England ; New York},
	edition = {3 edition},
	title = {Human-{Computer} {Interaction}},
	isbn = {978-0-13-046109-4},
	abstract = {The second edition of Human-Computer Interaction established itself as one of the classic textbooks in the area, with its broad coverage and rigorous approach, this new edition builds on the existing strengths of the book, but giving the text a more student-friendly slant and improving the coverage in certain areas. The revised structure, separating out the introductory and more advanced material will make it easier to use the book on a variety of courses. This new edition now includes chapters on Interaction Design, Universal Access and Rich Interaction, as well as covering the latest developments in ubiquitous computing and Web technologies, making it the ideal text to provide a grounding in HCI theory and practice.},
	language = {English},
	publisher = {Prentice Hall},
	author = {Dix, Alan and Finlay, Janet and Abowd, Gregory D. and Beale, Russell},
	month = sep,
	year = {2003}
}

@book{noauthor_challenges_nodate-1,
	title = {Challenges in irreproducible research},
	url = {https://www.nature.com/collections/prbfkwmwvz}
}

@article{chen_employing_2014-1,
	title = {Employing a {Parametric} {Model} for {Analytic} {Provenance}},
	volume = {4},
	issn = {2160-6455},
	url = {http://doi.acm.org/10.1145/2591510},
	doi = {10.1145/2591510},
	abstract = {We introduce a propagation-based parametric symbolic model approach to supporting analytic provenance. This approach combines a script language to capture and encode the analytic process and a parametrically controlled symbolic model to represent and reuse the logic of the analysis process. Our approach first appeared in a visual analytics system called CZSaw. Using a script to capture the analyst’s interactions at a meaningful system action level allows the creation of a parametrically controlled symbolic model in the form of a Directed Acyclic Graph (DAG). Using the DAG allows propagating changes. Graph nodes correspond to variables in CZSaw scripts, which are results (data and data visualizations) generated from user interactions. The user interacts with variables representing entities or relations to create the next step’s results. Graph edges represent dependency relationships among nodes. Any change to a variable triggers the propagation mechanism to update downstream dependent variables and in turn updates data views to reflect the change. The analyst can reuse parts of the analysis process by assigning new values to a node in the graph. We evaluated this symbolic model approach by solving three IEEE VAST Challenge contest problems (from IEEE VAST 2008, 2009, and 2010). In each of these challenges, the analyst first created a symbolic model to explore, understand, analyze, and solve a particular subproblem and then reused the model via its dependency graph propagation mechanism to solve similar subproblems. With the script and model, CZSaw supports the analytic provenance by capturing, encoding, and reusing the analysis process. The analyst can recall the chronological states of the analysis process with the CZSaw script and may interpret the underlying rationale of the analysis with the symbolic model.},
	number = {1},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Chen, Yingjie Victor and Qian, Zhenyu Cheryl and Woodbury, Robert and Dill, John and Shaw, Chris D.},
	month = apr,
	year = {2014},
	keywords = {HOW - Program Synthesis, WHEN - Hybrid Approaches, WHY - Adaptive Systems / Guidance, Type of Work: Empirical Study, Type of Work: Theory \& ModelType of Work: Empirical Study, Type of Work: Theory \& Model, WHEN - Hybrid Approaches, WHY - Adaptive Systems / Guidance},
	pages = {6:1--6:32}
}

@article{feng_patterns_2019-1,
	title = {Patterns and {Pace}: {Quantifying} {Diverse} {Exploration} {Behavior} with {Visualizations} on the {Web}},
	volume = {25},
	issn = {2160-9306},
	shorttitle = {Patterns and {Pace}},
	doi = {10.1109/TVCG.2018.2865117},
	abstract = {The diverse and vibrant ecosystem of interactive visualizations on the web presents an opportunity for researchers and practitioners to observe and analyze how everyday people interact with data visualizations. However, existing metrics of visualization interaction behavior used in research do not fully reveal the breadth of peoples' open-ended explorations with visualizations. One possible way to address this challenge is to determine high-level goals for visualization interaction metrics, and infer corresponding features from user interaction data that characterize different aspects of peoples' explorations of visualizations. In this paper, we identify needs for visualization behavior measurement, and develop corresponding candidate features that can be inferred from users' interaction data. We then propose metrics that capture novel aspects of peoples' open-ended explorations, including exploration uniqueness and exploration pacing. We evaluate these metrics along with four other metrics recently proposed in visualization literature by applying them to interaction data from prior visualization studies. The results of these evaluations suggest that these new metrics 1) reveal new characteristics of peoples' use of visualizations, 2) can be used to evaluate statistical differences between visualization designs, and 3) are statistically independent of prior metrics used in visualization research. We discuss implications of these results for future studies, including the potential for applying these metrics in visualization interaction analysis, as well as emerging challenges in developing and selecting metrics depicting visualization explorations.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Feng, Mi and Peck, Evan and Harrison, Lane},
	month = jan,
	year = {2019},
	keywords = {Type of Work: Case Study, HOW - Probabilistic Models / Prediction, WHEN - Retrospective Analyses, HOW: TFIDF, HOW: wavelet transform, WHY: characterizing exploration behaviorType of Work: Case Study, WHEN - Retrospective Analyses, WHY - User Behavior / User Characteristics / User Modelling, WHY: characterizing exploration behaviors},
	pages = {501--511}
}

@article{liu_mental_2010-1,
	title = {Mental {Models}, {Visual} {Reasoning} and {Interaction} in {Information} {Visualization}: {A} {Top}-down {Perspective}},
	volume = {16},
	issn = {2160-9306},
	shorttitle = {Mental {Models}, {Visual} {Reasoning} and {Interaction} in {Information} {Visualization}},
	doi = {10.1109/TVCG.2010.177},
	abstract = {Although previous research has suggested that examining the interplay between internal and external representations can benefit our understanding of the role of information visualization (InfoVis) in human cognitive activities, there has been little work detailing the nature of internal representations, the relationship between internal and external representations and how interaction is related to these representations. In this paper, we identify and illustrate a specific kind of internal representation, mental models, and outline the high-level relationships between mental models and external visualizations. We present a top-down perspective of reasoning as model construction and simulation, and discuss the role of visualization in model based reasoning. From this perspective, interaction can be understood as active modeling for three primary purposes: external anchoring, information foraging, and cognitive offloading. Finally we discuss the implications of our approach for design, evaluation and theory development.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Liu, Zhicheng and Stasko, John},
	month = nov,
	year = {2010},
	keywords = {Maybe related. This paper lists "types of interactions"},
	pages = {999--1008}
}

@article{battle_characterizing_2019-1,
	title = {Characterizing {Exploratory} {Visual} {Analysis}: {A} {Literature} {Review} and {Evaluation} of {Analytic} {Provenance} in {Tableau}},
	volume = {38},
	copyright = {© 2019 The Author(s) Computer Graphics Forum © 2019 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {Characterizing {Exploratory} {Visual} {Analysis}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13678},
	doi = {10.1111/cgf.13678},
	abstract = {Supporting exploratory visual analysis (EVA) is a central goal of visualization research, and yet our understanding of the process is arguably vague and piecemeal. We contribute a consistent definition of EVA through review of the relevant literature, and an empirical evaluation of existing assumptions regarding how analysts perform EVA using Tableau, a popular visual analysis tool. We present the results of a study where 27 Tableau users answered various analysis questions across 3 datasets. We measure task performance, identify recurring patterns across participants' analyses, and assess variance from task specificity and dataset. We find striking differences between existing assumptions and the collected data. Participants successfully completed a variety of tasks, with over 80\% accuracy across focused tasks with measurably correct answers. The observed cadence of analyses is surprisingly slow compared to popular assumptions from the database community. We find significant overlap in analyses across participants, showing that EVA behaviors can be predictable. Furthermore, we find few structural differences between behavior graphs for open-ended and more focused exploration tasks.},
	language = {en},
	number = {3},
	journal = {Computer Graphics Forum},
	author = {Battle, Leilani and Heer, Jeffrey},
	year = {2019},
	keywords = {HOW - Pattern Analysis, Type of Work: Technique, Type of Work: Survey, HOW - Probabilistic Models / Prediction, WHEN - Retrospective Analyses, Type of Work: Theory \& Model, WHY - User Behavior / User Characteristics / User Modelling},
	pages = {145--159}
}

@inproceedings{garg_model-driven_2008-2,
	address = {Columbus, OH, USA},
	title = {Model-driven {Visual} {Analytics}},
	isbn = {978-1-4244-2935-6},
	url = {http://ieeexplore.ieee.org/document/4677352/},
	doi = {10.1109/VAST.2008.4677352},
	booktitle = {2008 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	publisher = {IEEE},
	author = {Garg, Supriya and Nam, Julia Eunju and Ramakrishnan, I.V. and Mueller, Klaus},
	month = oct,
	year = {2008},
	pages = {19--26}
}

@article{lloyd_least_1982-2,
	title = {Least squares quantization in {PCM}},
	volume = {28},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1056489/},
	doi = {10.1109/TIT.1982.1056489},
	language = {en},
	number = {2},
	journal = {IEEE Transactions on Information Theory},
	author = {Lloyd, S.},
	month = mar,
	year = {1982},
	pages = {129--137}
}

@incollection{aggarwal_survey_2012-2,
	address = {Boston, MA},
	title = {A {Survey} of {Text} {Classification} {Algorithms}},
	isbn = {978-1-4614-3222-7 978-1-4614-3223-4},
	url = {http://link.springer.com/10.1007/978-1-4614-3223-4_6},
	language = {en},
	booktitle = {Mining {Text} {Data}},
	publisher = {Springer US},
	author = {Aggarwal, Charu C. and Zhai, ChengXiang},
	editor = {Aggarwal, Charu C. and Zhai, ChengXiang},
	year = {2012},
	doi = {10.1007/978-1-4614-3223-4_6},
	pages = {163--222}
}

@inproceedings{nath_survey_2014-2,
	address = {Kanyakumari District, India},
	title = {A survey of image classification methods and techniques},
	isbn = {978-1-4799-4190-2 978-1-4799-4191-9 978-1-4799-4193-3 978-1-4799-4192-6},
	url = {http://ieeexplore.ieee.org/document/6993023/},
	doi = {10.1109/ICCICCT.2014.6993023},
	booktitle = {2014 {International} {Conference} on {Control}, {Instrumentation}, {Communication} and {Computational} {Technologies} ({ICCICCT})},
	publisher = {IEEE},
	author = {Nath, Siddhartha Sankar and Mishra, Girish and Kar, Jajnyaseni and Chakraborty, Sayan and Dey, Nilanjan},
	month = jul,
	year = {2014},
	pages = {554--557}
}

@article{lu_survey_2007-2,
	title = {A survey of image classification methods and techniques for improving classification performance},
	volume = {28},
	issn = {0143-1161, 1366-5901},
	url = {https://www.tandfonline.com/doi/full/10.1080/01431160600746456},
	doi = {10.1080/01431160600746456},
	language = {en},
	number = {5},
	journal = {International Journal of Remote Sensing},
	author = {Lu, D. and Weng, Q.},
	month = mar,
	year = {2007},
	pages = {823--870}
}

@article{brehmer_multi-level_2013-2,
	title = {A {Multi}-{Level} {Typology} of {Abstract} {Visualization} {Tasks}},
	volume = {19},
	issn = {1077-2626},
	url = {http://ieeexplore.ieee.org/document/6634168/},
	doi = {10.1109/TVCG.2013.124},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Brehmer, Matthew and Munzner, Tamara},
	month = dec,
	year = {2013},
	pages = {2376--2385}
}

@article{xu_analytic_2015-2,
	title = {Analytic {Provenance} for {Sensemaking}: {A} {Research} {Agenda}},
	volume = {35},
	issn = {0272-1716},
	shorttitle = {Analytic {Provenance} for {Sensemaking}},
	url = {http://ieeexplore.ieee.org/document/7111922/},
	doi = {10.1109/MCG.2015.50},
	number = {3},
	journal = {IEEE Computer Graphics and Applications},
	author = {Xu, Kai and Attfield, Simon and Jankun-Kelly, T.J. and Wheat, Ashley and Nguyen, Phong H. and Selvaraj, Nallini},
	month = may,
	year = {2015},
	pages = {56--64}
}

@inproceedings{xiao_enhancing_2006-2,
	address = {Baltimore, MD, USA},
	title = {Enhancing {Visual} {Analysis} of {Network} {Traffic} {Using} a {Knowledge} {Representation}},
	isbn = {978-1-4244-0591-6 978-1-4244-0592-3},
	url = {http://ieeexplore.ieee.org/document/4035754/},
	doi = {10.1109/VAST.2006.261436},
	booktitle = {2006 {IEEE} {Symposium} {On} {Visual} {Analytics} {And} {Technology}},
	publisher = {IEEE},
	author = {Xiao, Ling and Gerth, John and Hanrahan, Pat},
	month = oct,
	year = {2006},
	pages = {107--114}
}

@article{pohl_analysing_2012-2,
	title = {Analysing {Interactivity} in {Information} {Visualisation}},
	volume = {26},
	issn = {0933-1875, 1610-1987},
	url = {http://link.springer.com/10.1007/s13218-012-0167-6},
	doi = {10.1007/s13218-012-0167-6},
	language = {en},
	number = {2},
	journal = {KI - Künstliche Intelligenz},
	author = {Pohl, Margit and Wiltner, Sylvia and Miksch, Silvia and Aigner, Wolfgang and Rind, Alexander},
	month = may,
	year = {2012},
	pages = {151--159}
}

@article{gajos_automatically_2010-2,
	title = {Automatically generating personalized user interfaces with {Supple}},
	volume = {174},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370210000822},
	doi = {10.1016/j.artint.2010.05.005},
	language = {en},
	number = {12-13},
	journal = {Artificial Intelligence},
	author = {Gajos, Krzysztof Z. and Weld, Daniel S. and Wobbrock, Jacob O.},
	month = aug,
	year = {2010},
	pages = {910--950}
}

@article{collins_guidance_2018-2,
	title = {Guidance in the human–machine analytics process},
	volume = {2},
	issn = {2468502X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2468502X1830041X},
	doi = {10.1016/j.visinf.2018.09.003},
	language = {en},
	number = {3},
	journal = {Visual Informatics},
	author = {Collins, Christopher and Andrienko, Natalia and Schreck, Tobias and Yang, Jing and Choo, Jaegul and Engelke, Ulrich and Jena, Amit and Dwyer, Tim},
	month = sep,
	year = {2018},
	pages = {166--180}
}

@inproceedings{battle_dynamic_2016-2,
	address = {San Francisco, California, USA},
	title = {Dynamic {Prefetching} of {Data} {Tiles} for {Interactive} {Visualization}},
	isbn = {978-1-4503-3531-7},
	url = {http://dl.acm.org/citation.cfm?doid=2882903.2882919},
	doi = {10.1145/2882903.2882919},
	language = {en},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data} - {SIGMOD} '16},
	publisher = {ACM Press},
	author = {Battle, Leilani and Chang, Remco and Stonebraker, Michael},
	year = {2016},
	pages = {1363--1375}
}

@article{aigner_evalbench_2013-2,
	title = {{EvalBench}: {A} {Software} {Library} for {Visualization} {Evaluation}},
	volume = {32},
	issn = {01677055},
	shorttitle = {{EvalBench}},
	url = {http://doi.wiley.com/10.1111/cgf.12091},
	doi = {10.1111/cgf.12091},
	language = {en},
	number = {3pt1},
	journal = {Computer Graphics Forum},
	author = {Aigner, W. and Hoffmann, S. and Rind, A.},
	month = jun,
	year = {2013},
	pages = {41--50}
}

@inproceedings{north_analytic_2011-2,
	title = {Analytic provenance: process+interaction+insight},
	url = {https://dl.acm.org/doi/abs/10.1145/1979742.1979570},
	language = {EN},
	booktitle = {{CHI} '11 {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	author = {North, Chris and Chang, Remco and Endert, Alex and Dou, Wenwen and May, Richard and Pike, Bill and Fink, Glenn},
	year = {2011},
	pages = {33--36}
}



@article{boserajendra_lineage_2005-2,
	title = {Lineage retrieval for scientific data processing: a survey},
	volume = {37},
	shorttitle = {Lineage retrieval for scientific data processing},
	url = {https://dl.acm.org/doi/abs/10.1145/1057977.1057978},
	abstract = {Scientific research relies as much on the dissemination and exchange of data sets as on the publication of conclusions. Accurately tracking the lineage (origin and subsequent processing history) of...},
	language = {EN},
	number = {1},
	journal = {ACM Computing Surveys (CSUR)},
	author = {{BoseRajendra} and {FrewJames}},
	month = mar,
	year = {2005}
}

@article{bunemanpeter_data_2019-2,
	title = {Data {Provenance}: {What} next?},
	volume = {47},
	doi = {https://doi.org/10.1145/3316416.3316418},
	abstract = {Research into data provenance has been active for almost twenty years. What has it delivered and where will it go next? What practical impact has it had and what might it have? We provide speculati...},
	language = {EN},
	number = {3},
	journal = {ACM SIGMOD Record},
	author = {{BunemanPeter} and {TanWang-Chiew}},
	month = feb,
	year = {2019},
	pages = {5--16}
}

@book{preece_interaction_2015-2,
	address = {Chichester},
	edition = {4th edition},
	title = {Interaction {Design}: {Beyond} {Human}-{Computer} {Interaction}},
	isbn = {978-1-119-02075-2},
	shorttitle = {Interaction {Design}},
	abstract = {A new edition of the \#1 text in the Human Computer Interaction field! Hugely popular with students and professionals alike, Interaction Design is an ideal resource for learning the interdisciplinary skills needed for interaction design, human computer interaction, information design, web design and ubiquitous computing. This text offers a cross-disciplinary, practical and process-oriented introduction to the field, showing not just what principles ought to apply to interaction design, but crucially how they can be applied. An accompanying website contains extensive additional teaching and learning material including slides for each chapter, comments on chapter activities and a number of in-depth case studies written by researchers and designers.},
	language = {English},
	publisher = {John Wiley},
	author = {Preece, Jenny and Sharp, Helen and Rogers, Yvonne},
	month = feb,
	year = {2015}
}

@book{dix_human-computer_2003-2,
	address = {Harlow, England ; New York},
	edition = {3 edition},
	title = {Human-{Computer} {Interaction}},
	isbn = {978-0-13-046109-4},
	abstract = {The second edition of Human-Computer Interaction established itself as one of the classic textbooks in the area, with its broad coverage and rigorous approach, this new edition builds on the existing strengths of the book, but giving the text a more student-friendly slant and improving the coverage in certain areas. The revised structure, separating out the introductory and more advanced material will make it easier to use the book on a variety of courses. This new edition now includes chapters on Interaction Design, Universal Access and Rich Interaction, as well as covering the latest developments in ubiquitous computing and Web technologies, making it the ideal text to provide a grounding in HCI theory and practice.},
	language = {English},
	publisher = {Prentice Hall},
	author = {Dix, Alan and Finlay, Janet and Abowd, Gregory D. and Beale, Russell},
	month = sep,
	year = {2003}
}

@book{noauthor_challenges_nodate-2,
	title = {Challenges in irreproducible research},
	url = {https://www.nature.com/collections/prbfkwmwvz}
}

@article{chen_employing_2014-2,
	title = {Employing a {Parametric} {Model} for {Analytic} {Provenance}},
	volume = {4},
	issn = {2160-6455},
	url = {http://doi.acm.org/10.1145/2591510},
	doi = {10.1145/2591510},
	abstract = {We introduce a propagation-based parametric symbolic model approach to supporting analytic provenance. This approach combines a script language to capture and encode the analytic process and a parametrically controlled symbolic model to represent and reuse the logic of the analysis process. Our approach first appeared in a visual analytics system called CZSaw. Using a script to capture the analyst’s interactions at a meaningful system action level allows the creation of a parametrically controlled symbolic model in the form of a Directed Acyclic Graph (DAG). Using the DAG allows propagating changes. Graph nodes correspond to variables in CZSaw scripts, which are results (data and data visualizations) generated from user interactions. The user interacts with variables representing entities or relations to create the next step’s results. Graph edges represent dependency relationships among nodes. Any change to a variable triggers the propagation mechanism to update downstream dependent variables and in turn updates data views to reflect the change. The analyst can reuse parts of the analysis process by assigning new values to a node in the graph. We evaluated this symbolic model approach by solving three IEEE VAST Challenge contest problems (from IEEE VAST 2008, 2009, and 2010). In each of these challenges, the analyst first created a symbolic model to explore, understand, analyze, and solve a particular subproblem and then reused the model via its dependency graph propagation mechanism to solve similar subproblems. With the script and model, CZSaw supports the analytic provenance by capturing, encoding, and reusing the analysis process. The analyst can recall the chronological states of the analysis process with the CZSaw script and may interpret the underlying rationale of the analysis with the symbolic model.},
	number = {1},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Chen, Yingjie Victor and Qian, Zhenyu Cheryl and Woodbury, Robert and Dill, John and Shaw, Chris D.},
	month = apr,
	year = {2014},
	keywords = {HOW - Program Synthesis, WHEN - Hybrid Approaches, WHY - Adaptive Systems / Guidance, Type of Work: Empirical Study, Type of Work: Theory \& Model},
	pages = {6:1--6:32}
}

@article{feng_patterns_2019-2,
	title = {Patterns and {Pace}: {Quantifying} {Diverse} {Exploration} {Behavior} with {Visualizations} on the {Web}},
	volume = {25},
	issn = {2160-9306},
	shorttitle = {Patterns and {Pace}},
	doi = {10.1109/TVCG.2018.2865117},
	abstract = {The diverse and vibrant ecosystem of interactive visualizations on the web presents an opportunity for researchers and practitioners to observe and analyze how everyday people interact with data visualizations. However, existing metrics of visualization interaction behavior used in research do not fully reveal the breadth of peoples' open-ended explorations with visualizations. One possible way to address this challenge is to determine high-level goals for visualization interaction metrics, and infer corresponding features from user interaction data that characterize different aspects of peoples' explorations of visualizations. In this paper, we identify needs for visualization behavior measurement, and develop corresponding candidate features that can be inferred from users' interaction data. We then propose metrics that capture novel aspects of peoples' open-ended explorations, including exploration uniqueness and exploration pacing. We evaluate these metrics along with four other metrics recently proposed in visualization literature by applying them to interaction data from prior visualization studies. The results of these evaluations suggest that these new metrics 1) reveal new characteristics of peoples' use of visualizations, 2) can be used to evaluate statistical differences between visualization designs, and 3) are statistically independent of prior metrics used in visualization research. We discuss implications of these results for future studies, including the potential for applying these metrics in visualization interaction analysis, as well as emerging challenges in developing and selecting metrics depicting visualization explorations.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Feng, Mi and Peck, Evan and Harrison, Lane},
	month = jan,
	year = {2019},
	keywords = {Type of Work: Case Study, HOW - Probabilistic Models / Prediction, WHEN - Retrospective Analyses, HOW: TFIDF, HOW: wavelet transform, WHY: characterizing exploration behaviors, WHY - User Behavior / User Characteristics / User Modelling},
	pages = {501--511}
}

@article{liu_mental_2010-2,
	title = {Mental {Models}, {Visual} {Reasoning} and {Interaction} in {Information} {Visualization}: {A} {Top}-down {Perspective}},
	volume = {16},
	issn = {2160-9306},
	shorttitle = {Mental {Models}, {Visual} {Reasoning} and {Interaction} in {Information} {Visualization}},
	doi = {10.1109/TVCG.2010.177},
	abstract = {Although previous research has suggested that examining the interplay between internal and external representations can benefit our understanding of the role of information visualization (InfoVis) in human cognitive activities, there has been little work detailing the nature of internal representations, the relationship between internal and external representations and how interaction is related to these representations. In this paper, we identify and illustrate a specific kind of internal representation, mental models, and outline the high-level relationships between mental models and external visualizations. We present a top-down perspective of reasoning as model construction and simulation, and discuss the role of visualization in model based reasoning. From this perspective, interaction can be understood as active modeling for three primary purposes: external anchoring, information foraging, and cognitive offloading. Finally we discuss the implications of our approach for design, evaluation and theory development.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Liu, Zhicheng and Stasko, John},
	month = nov,
	year = {2010},
	keywords = {Maybe related. This paper lists "types of interactions"},
	pages = {999--1008}
}

@article{battle_characterizing_2019-2,
	title = {Characterizing {Exploratory} {Visual} {Analysis}: {A} {Literature} {Review} and {Evaluation} of {Analytic} {Provenance} in {Tableau}},
	volume = {38},
	copyright = {© 2019 The Author(s) Computer Graphics Forum © 2019 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {Characterizing {Exploratory} {Visual} {Analysis}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13678},
	doi = {10.1111/cgf.13678},
	abstract = {Supporting exploratory visual analysis (EVA) is a central goal of visualization research, and yet our understanding of the process is arguably vague and piecemeal. We contribute a consistent definition of EVA through review of the relevant literature, and an empirical evaluation of existing assumptions regarding how analysts perform EVA using Tableau, a popular visual analysis tool. We present the results of a study where 27 Tableau users answered various analysis questions across 3 datasets. We measure task performance, identify recurring patterns across participants' analyses, and assess variance from task specificity and dataset. We find striking differences between existing assumptions and the collected data. Participants successfully completed a variety of tasks, with over 80\% accuracy across focused tasks with measurably correct answers. The observed cadence of analyses is surprisingly slow compared to popular assumptions from the database community. We find significant overlap in analyses across participants, showing that EVA behaviors can be predictable. Furthermore, we find few structural differences between behavior graphs for open-ended and more focused exploration tasks.},
	language = {en},
	number = {3},
	journal = {Computer Graphics Forum},
	author = {Battle, Leilani and Heer, Jeffrey},
	year = {2019},
	keywords = {HOW - Pattern Analysis, Type of Work: Technique, Type of Work: Survey, HOW - Probabilistic Models / Prediction, WHEN - Retrospective Analyses, Type of Work: Theory \& Model, WHY - User Behavior / User Characteristics / User Modelling},
	pages = {145--159}
}

@article{xu_chart_2018-1,
	title = {Chart {Constellations}: {Effective} {Chart} {Summarization} for {Collaborative} and {Multi}-{User} {Analyses}},
	volume = {37},
	issn = {01677055},
	shorttitle = {Chart {Constellations}},
	url = {http://doi.wiley.com/10.1111/cgf.13402},
	doi = {10.1111/cgf.13402},
	abstract = {Many data problems in the real world are complex and require multiple analysts working together to uncover embedded insights by creating chart-driven data stories. How, as a subsequent analysis step, do we interpret and learn from these collections of charts? We present Chart Constellations, a system to interactively support a single analyst in the review and analysis of data stories created by other collaborative analysts. Instead of iterating through the individual charts for each data story, the analyst can project, cluster, ﬁlter, and connect results from all users in a meta-visualization approach. Constellations supports deriving summary insights about prior investigations and supports the exploration of new, unexplored regions in the dataset. To evaluate our system, we conduct a user study comparing it against data science notebooks. Results suggest that Constellations promotes the discovery of both broad and high-level insights, including theme and trend analysis, subjective evaluation, and hypothesis generation.},
	language = {en},
	number = {3},
	urldate = {2020-03-05},
	journal = {Computer Graphics Forum},
	author = {Xu, Shenyu and Bryan, Chris and Li, Jianping Kelvin and Zhao, Jian and Ma, Kwan-Liu},
	month = jun,
	year = {2018},
	pages = {75--86},
	file = {Xu et al. - 2018 - Chart Constellations Effective Chart Summarizatio.pdf:C\:\\Users\\conny\\Zotero\\storage\\3ZZ3G8QN\\Xu et al. - 2018 - Chart Constellations Effective Chart Summarizatio.pdf:application/pdf}
}

@article{cheney_provenance_2007,
	title = {Provenance in {Databases}: {Why}, {How}, and {Where}},
	volume = {1},
	issn = {1931-7883, 1931-7891},
	shorttitle = {Provenance in {Databases}},
	url = {http://www.nowpublishers.com/article/Details/DBS-006},
	doi = {10.1561/1900000006},
	language = {en},
	number = {4},
	urldate = {2020-04-10},
	journal = {Foundations and Trends in Databases},
	author = {Cheney, James and Chiticariu, Laura and Tan, Wang-Chiew},
	year = {2007},
	pages = {145--159}
}

@book{oxford_dictionary,
   edition = {2nd},
   title = {{Oxford English Dictionary}},
   url = {https://www.lexico.com/en/definition/provenance},
   booktitle = {{Oxford English Dictionary}},
   publisher = {Oxford University Press},
   urldate = {2020-01-25},
   year = {1989}
}

@book{merriam_webster_dictionary,
   edition = {12th},
   title = {{Merriam-Webster Dictionary}},
   url = {https://www.merriam-webster.com/dictionary/provenance},
   booktitle = {{Merriam-Webster Dictionary}},
   publisher = {Merriam-Webster, Incorporated},
   urldate = {2020-01-25},
   year = {2019}
}
